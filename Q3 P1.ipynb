{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# load `.mat` file\n",
    "data = scipy.io.loadmat('face.mat')\n",
    "\n",
    "# Images\n",
    "# N: number of images\n",
    "# D: number of pixels\n",
    "X = data['X']  # shape: [D x N]\n",
    "y = data['l']  # shape: [1 x N]\n",
    "\n",
    "assert(X.shape[1] == y.shape[1])\n",
    "# Number of images\n",
    "D, N = X.shape\n",
    "\n",
    "# Fix the random seed\n",
    "np.random.seed(13)\n",
    "\n",
    "# Cardinality of labels\n",
    "_card = len(set(y.ravel()))\n",
    "\n",
    "# Step splitting of dataset\n",
    "_step = int(N / _card)\n",
    "\n",
    "# Shape boundaries\n",
    "_bounds = np.arange(0, N+1, _step)\n",
    "\n",
    "# Shapes\n",
    "shapes = list(zip(_bounds[:-1], _bounds[1:]))\n",
    "\n",
    "# Training Mask\n",
    "_mask = []\n",
    "\n",
    "for _shape in shapes:\n",
    "    _idx = np.random.choice(\n",
    "        np.arange(*_shape), int(0.8 * _step), replace=False)\n",
    "    _mask.append(_idx)\n",
    "\n",
    "mask_train = np.array(_mask).ravel()\n",
    "\n",
    "mask_test = np.array(list(set(np.arange(0, N)) - set(mask_train)))\n",
    "\n",
    "# Partition dataset to train and test sets\n",
    "X_train, X_test = X[:, mask_train], X[:, mask_test]\n",
    "y_train, y_test = y[:, mask_train], y[:, mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA(object):\n",
    "    \"\"\"Principle Component Analysis.\"\"\"\n",
    "\n",
    "    def __init__(self, n_comps=5, standard=True):\n",
    "        \"\"\"Contructor.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_comps: int\n",
    "            Number of principle components\n",
    "        \"\"\"\n",
    "        self._fitted = False\n",
    "        self.n_comps = n_comps\n",
    "        self.standard = standard\n",
    "        self.mean = None\n",
    "        self.U = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit PCA according to `X.cov()`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Features matrix\n",
    "        Returns\n",
    "        -------\n",
    "        array: numpy.ndarray\n",
    "            Transformed features matrix\n",
    "        \"\"\"\n",
    "        self.D, N = X.shape\n",
    "        self.mean = X.mean(axis=1).reshape(-1, 1)\n",
    "        # center data\n",
    "        A = X - self.mean\n",
    "        # covariance matrix\n",
    "        S = (1 / N) * np.dot(A.T, A)\n",
    "        \n",
    "        _l, _v = np.linalg.eig(S)\n",
    "\n",
    "        _indexes = np.argsort(_l)[::-1]\n",
    "\n",
    "        # Sorted eigenvalues and eigenvectors\n",
    "        l, v = _l[_indexes], _v[:, _indexes]\n",
    "\n",
    "        V = v[:, :self.n_comps]\n",
    "\n",
    "        _U = np.dot(A, V)\n",
    "\n",
    "        self.U = _U / np.apply_along_axis(np.linalg.norm, 0, _U)\n",
    "\n",
    "        W = np.dot(self.U.T, A)\n",
    "\n",
    "        if self.standard:\n",
    "            self.W_mean = np.mean(W, axis=1)\n",
    "            self.W_std = np.std(W, axis=1)\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "        if self.standard:\n",
    "            return ((W.T - self.W_mean) / self.W_std).T\n",
    "        else:\n",
    "            return W\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform `X` by projecting it to PCA feature space.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Features matrix\n",
    "        Returns\n",
    "        -------\n",
    "        array: numpy.ndarray\n",
    "            Transformed features matrix\n",
    "        \"\"\"\n",
    "\n",
    "        Phi = X - self.mean\n",
    "\n",
    "        W = np.dot(self.U.T, Phi)\n",
    "\n",
    "        if self.standard:\n",
    "            return ((W.T - self.W_mean) / self.W_std).T\n",
    "        else:\n",
    "            return W\n",
    "\n",
    "    def reconstruct(self, W):\n",
    "        \"\"\"Recontruct compressed data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        W: numpy.ndarray\n",
    "            Projection coefficients matrix\n",
    "        Returns\n",
    "        -------\n",
    "        X_hat: numpy.ndarray\n",
    "            Reconstructed features matrix\n",
    "        \"\"\"\n",
    "        A_hat = np.dot(self.U, W).reshape(-1, 1)\n",
    "        A_hat = A_hat + self.mean\n",
    "        return A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  1 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  2  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  3  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  4  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  5  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  6  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  7  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  8  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  9  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  10  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  11  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  12  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  13  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  14  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  15  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  16  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  17  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  18  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  19  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  20  --->  Accuracy = 4.81%\n",
      "M_pca =  2 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  2 , M_lda =  2  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  3  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  4  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  5  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  6  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  7  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  8  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  9  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  10  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  11  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  12  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  13  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  14  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  15  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  16  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  17  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  18  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  19  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  20  --->  Accuracy = 9.62%\n",
      "M_pca =  3 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  3 , M_lda =  2  --->  Accuracy = 5.77%\n",
      "M_pca =  3 , M_lda =  3  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  4  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  5  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  6  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  7  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  8  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  9  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  10  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  11  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  12  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  13  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  14  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  15  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  16  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  17  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  18  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  19  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  20  --->  Accuracy = 17.31%\n",
      "M_pca =  4 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  4 , M_lda =  2  --->  Accuracy = 6.73%\n",
      "M_pca =  4 , M_lda =  3  --->  Accuracy = 18.27%\n",
      "M_pca =  4 , M_lda =  4  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  5  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  6  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  7  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  8  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  9  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  10  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  11  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  12  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  13  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  14  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  15  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  16  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  17  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  18  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  19  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  20  --->  Accuracy = 25.00%\n",
      "M_pca =  5 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  5 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  5 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  5 , M_lda =  4  --->  Accuracy = 25.96%\n",
      "M_pca =  5 , M_lda =  5  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  7  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  8  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  9  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  10  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  11  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  12  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  13  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  14  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  15  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  16  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  17  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  18  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  19  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  20  --->  Accuracy = 35.58%\n",
      "M_pca =  6 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  6 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  6 , M_lda =  3  --->  Accuracy = 20.19%\n",
      "M_pca =  6 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  6 , M_lda =  5  --->  Accuracy = 35.58%\n",
      "M_pca =  6 , M_lda =  6  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  7  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  8  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  9  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  10  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  11  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  12  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  13  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  14  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  15  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  16  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  17  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  18  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  19  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  20  --->  Accuracy = 41.35%\n",
      "M_pca =  7 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  7 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  7 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  7 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  7 , M_lda =  5  --->  Accuracy = 32.69%\n",
      "M_pca =  7 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  7 , M_lda =  7  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  8  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  9  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  10  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  11  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  12  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  13  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  14  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  15  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  16  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  17  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  18  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  19  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  20  --->  Accuracy = 37.50%\n",
      "M_pca =  8 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  8 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  8 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  8 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  8 , M_lda =  5  --->  Accuracy = 33.65%\n",
      "M_pca =  8 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  8 , M_lda =  7  --->  Accuracy = 44.23%\n",
      "M_pca =  8 , M_lda =  8  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  9  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  10  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  11  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  12  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  13  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  14  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  15  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  16  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  17  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  18  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  19  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  20  --->  Accuracy = 42.31%\n",
      "M_pca =  9 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  9 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  9 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  9 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  9 , M_lda =  5  --->  Accuracy = 36.54%\n",
      "M_pca =  9 , M_lda =  6  --->  Accuracy = 38.46%\n",
      "M_pca =  9 , M_lda =  7  --->  Accuracy = 40.38%\n",
      "M_pca =  9 , M_lda =  8  --->  Accuracy = 44.23%\n",
      "M_pca =  9 , M_lda =  9  --->  Accuracy = 48.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  9 , M_lda =  10  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  11  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  12  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  13  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  14  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  15  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  16  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  17  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  18  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  19  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  20  --->  Accuracy = 48.08%\n",
      "M_pca =  10 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  10 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  10 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  10 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  10 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  10 , M_lda =  6  --->  Accuracy = 42.31%\n",
      "M_pca =  10 , M_lda =  7  --->  Accuracy = 45.19%\n",
      "M_pca =  10 , M_lda =  8  --->  Accuracy = 50.96%\n",
      "M_pca =  10 , M_lda =  9  --->  Accuracy = 49.04%\n",
      "M_pca =  10 , M_lda =  10  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  11  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  12  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  13  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  14  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  15  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  16  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  17  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  18  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  19  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  20  --->  Accuracy = 52.88%\n",
      "M_pca =  11 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  11 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  11 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  11 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  11 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  11 , M_lda =  6  --->  Accuracy = 39.42%\n",
      "M_pca =  11 , M_lda =  7  --->  Accuracy = 47.12%\n",
      "M_pca =  11 , M_lda =  8  --->  Accuracy = 51.92%\n",
      "M_pca =  11 , M_lda =  9  --->  Accuracy = 50.96%\n",
      "M_pca =  11 , M_lda =  10  --->  Accuracy = 51.92%\n",
      "M_pca =  11 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  12  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  13  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  14  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  15  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  16  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  17  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  18  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  19  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  20  --->  Accuracy = 56.73%\n",
      "M_pca =  12 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  12 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  12 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  12 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  12 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  12 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  12 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  12 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  12 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  12 , M_lda =  10  --->  Accuracy = 60.58%\n",
      "M_pca =  12 , M_lda =  11  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  12  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  13  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  14  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  15  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  16  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  17  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  18  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  19  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  20  --->  Accuracy = 59.62%\n",
      "M_pca =  13 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  13 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  13 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  13 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  13 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  13 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  13 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  13 , M_lda =  8  --->  Accuracy = 53.85%\n",
      "M_pca =  13 , M_lda =  9  --->  Accuracy = 53.85%\n",
      "M_pca =  13 , M_lda =  10  --->  Accuracy = 55.77%\n",
      "M_pca =  13 , M_lda =  11  --->  Accuracy = 57.69%\n",
      "M_pca =  13 , M_lda =  12  --->  Accuracy = 58.65%\n",
      "M_pca =  13 , M_lda =  13  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  14  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  15  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  16  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  17  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  18  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  19  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  20  --->  Accuracy = 63.46%\n",
      "M_pca =  14 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  14 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  14 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  14 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  14 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  14 , M_lda =  6  --->  Accuracy = 43.27%\n",
      "M_pca =  14 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  14 , M_lda =  8  --->  Accuracy = 48.08%\n",
      "M_pca =  14 , M_lda =  9  --->  Accuracy = 51.92%\n",
      "M_pca =  14 , M_lda =  10  --->  Accuracy = 57.69%\n",
      "M_pca =  14 , M_lda =  11  --->  Accuracy = 61.54%\n",
      "M_pca =  14 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  14 , M_lda =  13  --->  Accuracy = 63.46%\n",
      "M_pca =  14 , M_lda =  14  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  15  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  16  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  17  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  18  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  19  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  20  --->  Accuracy = 64.42%\n",
      "M_pca =  15 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  15 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  15 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  15 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  15 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  15 , M_lda =  6  --->  Accuracy = 46.15%\n",
      "M_pca =  15 , M_lda =  7  --->  Accuracy = 50.96%\n",
      "M_pca =  15 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  15 , M_lda =  9  --->  Accuracy = 53.85%\n",
      "M_pca =  15 , M_lda =  10  --->  Accuracy = 55.77%\n",
      "M_pca =  15 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  15 , M_lda =  12  --->  Accuracy = 60.58%\n",
      "M_pca =  15 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  15 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  15 , M_lda =  15  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  16  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  17  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  18  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  19  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  20  --->  Accuracy = 65.38%\n",
      "M_pca =  16 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  16 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  16 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  16 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  16 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  16 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  16 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  16 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  16 , M_lda =  9  --->  Accuracy = 54.81%\n",
      "M_pca =  16 , M_lda =  10  --->  Accuracy = 54.81%\n",
      "M_pca =  16 , M_lda =  11  --->  Accuracy = 58.65%\n",
      "M_pca =  16 , M_lda =  12  --->  Accuracy = 57.69%\n",
      "M_pca =  16 , M_lda =  13  --->  Accuracy = 61.54%\n",
      "M_pca =  16 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  15  --->  Accuracy = 67.31%\n",
      "M_pca =  16 , M_lda =  16  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  17  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  18  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  19  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  20  --->  Accuracy = 66.35%\n",
      "M_pca =  17 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  17 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  17 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  17 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  17 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  17 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  17 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  17 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  17 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  17 , M_lda =  10  --->  Accuracy = 56.73%\n",
      "M_pca =  17 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  17 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  17 , M_lda =  13  --->  Accuracy = 62.50%\n",
      "M_pca =  17 , M_lda =  14  --->  Accuracy = 65.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  17 , M_lda =  15  --->  Accuracy = 66.35%\n",
      "M_pca =  17 , M_lda =  16  --->  Accuracy = 65.38%\n",
      "M_pca =  17 , M_lda =  17  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  18  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  19  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  20  --->  Accuracy = 67.31%\n",
      "M_pca =  18 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  18 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  18 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  18 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  18 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  18 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  18 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  18 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  18 , M_lda =  9  --->  Accuracy = 55.77%\n",
      "M_pca =  18 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  11  --->  Accuracy = 57.69%\n",
      "M_pca =  18 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  13  --->  Accuracy = 63.46%\n",
      "M_pca =  18 , M_lda =  14  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  15  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  16  --->  Accuracy = 67.31%\n",
      "M_pca =  18 , M_lda =  17  --->  Accuracy = 68.27%\n",
      "M_pca =  18 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  20  --->  Accuracy = 70.19%\n",
      "M_pca =  19 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  19 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  19 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  19 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  19 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  19 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  19 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  19 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  19 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  19 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  19 , M_lda =  11  --->  Accuracy = 61.54%\n",
      "M_pca =  19 , M_lda =  12  --->  Accuracy = 63.46%\n",
      "M_pca =  19 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  19 , M_lda =  14  --->  Accuracy = 65.38%\n",
      "M_pca =  19 , M_lda =  15  --->  Accuracy = 64.42%\n",
      "M_pca =  19 , M_lda =  16  --->  Accuracy = 66.35%\n",
      "M_pca =  19 , M_lda =  17  --->  Accuracy = 68.27%\n",
      "M_pca =  19 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  19 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  20 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  20 , M_lda =  2  --->  Accuracy = 10.58%\n",
      "M_pca =  20 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  20 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  20 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  20 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  20 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  20 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  20 , M_lda =  9  --->  Accuracy = 60.58%\n",
      "M_pca =  20 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  20 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  20 , M_lda =  12  --->  Accuracy = 66.35%\n",
      "M_pca =  20 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  20 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  20 , M_lda =  15  --->  Accuracy = 69.23%\n",
      "M_pca =  20 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  20 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  21 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  21 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  21 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  21 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  21 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  21 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  21 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  21 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  21 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  21 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  21 , M_lda =  14  --->  Accuracy = 64.42%\n",
      "M_pca =  21 , M_lda =  15  --->  Accuracy = 65.38%\n",
      "M_pca =  21 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  21 , M_lda =  17  --->  Accuracy = 68.27%\n",
      "M_pca =  21 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  21 , M_lda =  19  --->  Accuracy = 69.23%\n",
      "M_pca =  21 , M_lda =  20  --->  Accuracy = 70.19%\n",
      "M_pca =  22 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  22 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  22 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  22 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  22 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  22 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  22 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  22 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  22 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  22 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  22 , M_lda =  12  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  22 , M_lda =  15  --->  Accuracy = 67.31%\n",
      "M_pca =  22 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  22 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  22 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  22 , M_lda =  19  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  20  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  23 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  23 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  23 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  23 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  23 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  23 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  23 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  23 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  23 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  23 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  14  --->  Accuracy = 70.19%\n",
      "M_pca =  23 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  23 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  24 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  24 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  24 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  24 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  24 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  24 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  24 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  24 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  14  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  24 , M_lda =  16  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  17  --->  Accuracy = 69.23%\n",
      "M_pca =  24 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  20  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  25 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  25 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  25 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  25 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  25 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  25 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  25 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  25 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  25 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  25 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  25 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  25 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  18  --->  Accuracy = 69.23%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  25 , M_lda =  19  --->  Accuracy = 69.23%\n",
      "M_pca =  25 , M_lda =  20  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  26 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  26 , M_lda =  3  --->  Accuracy = 20.19%\n",
      "M_pca =  26 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  26 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  26 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  26 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  26 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  26 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  26 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  26 , M_lda =  13  --->  Accuracy = 68.27%\n",
      "M_pca =  26 , M_lda =  14  --->  Accuracy = 67.31%\n",
      "M_pca =  26 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  26 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  26 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  27 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  27 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  27 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  27 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  27 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  27 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  27 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  27 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  27 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  27 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  27 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  27 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  27 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  27 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  27 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  27 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  27 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  27 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  28 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  28 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  28 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  28 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  28 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  28 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  28 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  28 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  28 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  28 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  28 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  28 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  28 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  28 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  28 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  28 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  29 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  29 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  29 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  29 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  29 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  29 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  29 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  29 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  29 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  29 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  29 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  29 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  29 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  29 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  29 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  29 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  29 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  30 , M_lda =  1  --->  Accuracy = 13.46%\n",
      "M_pca =  30 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  30 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  30 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  30 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  30 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  30 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  30 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  30 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  30 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  30 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  30 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  30 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  30 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  30 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  30 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  30 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  30 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  30 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  31 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  31 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  31 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  31 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  31 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  31 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  31 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  31 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  31 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  31 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  31 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  32 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  32 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  32 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  32 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  32 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  32 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  32 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  32 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  32 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  32 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  32 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  32 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  32 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  32 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  32 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  32 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  33 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  33 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  33 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  33 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  33 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  33 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  33 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  33 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  33 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  33 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  33 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  1  --->  Accuracy = 6.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  34 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  34 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  34 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  34 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  34 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  34 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  34 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  34 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  34 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  34 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  34 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  34 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  34 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  35 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  35 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  35 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  35 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  35 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  35 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  35 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  35 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  35 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  35 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  35 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  35 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  36 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  36 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  36 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  36 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  36 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  36 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  36 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  36 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  36 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  36 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  36 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  36 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  36 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  36 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  37 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  37 , M_lda =  3  --->  Accuracy = 20.19%\n",
      "M_pca =  37 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  37 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  37 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  37 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  37 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  37 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  37 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  38 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  38 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  38 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  38 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  38 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  38 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  38 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  38 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  38 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  38 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  38 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  39 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  39 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  39 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  39 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  39 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  39 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  39 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  39 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  39 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  39 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  39 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  39 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  39 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  39 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  40 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  40 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  40 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  40 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  40 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  40 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  40 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  40 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  40 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  40 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  41 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  41 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  41 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  41 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  41 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  41 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  41 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  41 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  41 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  41 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  42 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  42 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  42 , M_lda =  4  --->  Accuracy = 42.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  42 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  42 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  42 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  42 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  42 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  43 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  43 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  43 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  43 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  43 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  43 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  43 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  43 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  43 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  43 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  43 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  43 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  44 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  44 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  44 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  44 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  44 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  44 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  44 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  44 , M_lda =  8  --->  Accuracy = 75.96%\n",
      "M_pca =  44 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  44 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  44 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  44 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  44 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  44 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  45 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  45 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  45 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  45 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  45 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  45 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  45 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  45 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  45 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  45 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  45 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  45 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  45 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  45 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  46 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  46 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  46 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  46 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  46 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  46 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  46 , M_lda =  8  --->  Accuracy = 75.00%\n",
      "M_pca =  46 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  46 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  46 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  46 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  46 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  47 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  47 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  47 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  47 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  47 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  47 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  47 , M_lda =  8  --->  Accuracy = 75.96%\n",
      "M_pca =  47 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  47 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  47 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  47 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  47 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  48 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  48 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  48 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  48 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  48 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  48 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  48 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  48 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  48 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  48 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  48 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  48 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  49 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  49 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  49 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  49 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  49 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  49 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  49 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  49 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  49 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  49 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  49 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  49 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  49 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  50 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  50 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  50 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  50 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  50 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  50 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  50 , M_lda =  8  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  50 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  50 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  50 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  50 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  50 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  50 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "Accuracy is maximum for M__pca =  47 , M_lda =  15  with accuracy of 84.62% .\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# KNN Classifer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "_card = 52\n",
    "D, N = X_train.shape\n",
    "\n",
    "M_pca = 1\n",
    "M_lda = 1\n",
    "\n",
    "M_pca_range = N-_card\n",
    "M_lda_range = _card-1\n",
    "\n",
    "\n",
    "acc_array = np.empty((M_pca_range, M_lda_range))\n",
    "M_pca_array = np.arange(1, M_pca_range+1)\n",
    "M_lda_array = np.arange(1, M_lda_range+1)\n",
    "\n",
    "\n",
    "standard = False\n",
    "\n",
    "M__pca_ideal = None\n",
    "M__lda_ideal = None\n",
    "acc_max = 0\n",
    "\n",
    "while M_pca <= M_pca_range:\n",
    "    M_lda = 1\n",
    "    while M_lda <= M_lda_range:\n",
    "\n",
    "        pca = PCA(n_comps=M_pca, standard=standard)\n",
    "        W_train = pca.fit(X_train)\n",
    "        \n",
    "\n",
    "        lda = LinearDiscriminantAnalysis(n_components=M_lda)\n",
    "        W_train_2 = lda.fit_transform(W_train.T, y_train.T.ravel())\n",
    "\n",
    "        nn = KNeighborsClassifier(n_neighbors=1)\n",
    "        nn.fit(W_train_2, y_train.T.ravel())\n",
    "\n",
    "        W_test = pca.transform(X_test)\n",
    "\n",
    "        W_test_2 = lda.transform(W_test.T)\n",
    "\n",
    "        acc = nn.score(W_test_2, y_test.T.ravel())\n",
    "        \n",
    "        acc_array[M_pca-1, M_lda-1] = acc\n",
    "\n",
    "        print('M_pca = ', M_pca, ', M_lda = ', M_lda,' --->  Accuracy = %.2f%%' % (acc * 100))\n",
    "        \n",
    "        if (acc > acc_max):\n",
    "            M__pca_ideal = M_pca\n",
    "            M__lda_ideal = M_lda\n",
    "            acc_max = acc\n",
    "\n",
    "        M_lda = M_lda + 1\n",
    "        \n",
    "    M_pca = M_pca + 1\n",
    "    \n",
    "print (\"Accuracy is maximum for M__pca = \", M__pca_ideal, \", M_lda = \", M__lda_ideal, \" with accuracy of %.2f%%\"% (acc_max * 100), \".\")\n",
    "\n",
    "#Ideal: M_pca =  147 , M_lda =  46  --->  Accuracy = 94.23%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 20)\n",
      "(50, 20)\n",
      "(50, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsvXmYHFd57/85p6p6meme6dk0M9JIo92yJcsyQoCwfYNzA1xDIITrBxtsx0CcS0j4hYBD4uT+CNw8xJeAASdcwi+BBLAdHJM8hnBvCNeYxQi8L5Ktzdr32Wd6pqe3qjrn/f3R0z09mzQjjcFLf/zMI3dX1Tmnqru+/dZ73vc9SkSoUaNGjRq/fPQvewA1atSoUaNETZBr1KhR40VCTZBr1KhR40VCTZBr1KhR40VCTZBr1KhR40VCTZBr1KhR40VCTZBr1KhR40VCTZBr1KhR40VCTZBr1KhR40WCu8D9a2l9NWrUqLFw1Hx2qlnINWrUqPEioSbINV507Nixg4suuuiXPYxXHJ/85Ce58cYb59y+cuVKHnzwwV/giF551AR5kXjDG95AU1MTxWLxlz2UlzxXXXUVzz///AvS9hve8AaUUuzatWvK++94xztQSvGTn/zkBel3oYgIt956Ky0tLbS0tHDttdee85iXyrnVmJuaIC8Cx44dY8eOHSil+O53v/sL7TsMw19of4vBL3vM69ev56677qq8Hhoa4tFHH6Wtre2XOKqpPPDAA9xzzz3s2rWLM2fO8IEPfGBex70Uzq3G3NQEeRG46667eN3rXsd73/tevvGNb0zZls/nufXWW+nu7qaxsZErr7ySfD4PwM9+9jNe//rXk0qlWL58OV//+teBkqXz1a9+tdLG17/+da688srKa6UUX/rSl1i3bh3r1q0D4MMf/jDLly+noaGBrVu3smPHjsr+xhhuv/121qxZQzKZZOvWrZw8eZLf//3f59Zbb50y3re97W3ceeedM87xd3/3d/mjP/qjKe/9xm/8Bp///OcB+PSnP11p/5JLLuHb3/72lPFfccUVfOQjH6G5uZmPf/zjNDc389xzz1X26e/vJx6PMzAwwE9+8hO6uroq21auXMkdd9zB5s2baWxs5LrrrqNQKFS2f+Yzn6Gzs5OlS5fy1a9+FaUUhw4dmu2jAuCGG27gvvvuwxgDwL333stv/uZvEolE5jymzCc/+UmuvfZarrvuOpLJJK961aumWKQnT57kne98J21tbbS0tPChD30IgMOHD/Orv/qrtLS00Nrayg033EA6nZ6zH9d1icfjdHR0EI1GeeMb33jOsV3ouU3n7rvvpru7m5aWFv7yL/9yyrbHH3+c7du3k0ql6Ozs5EMf+hC+7y+4jxrTEJGF/NWYhTVr1siXvvQlefLJJ8V1Xent7a1s+73f+z35lV/5FTl16pSEYSg///nPpVAoyPHjxyWRSMg3v/lN8X1fBgcH5ZlnnhERkV/5lV+Rr3zlK5U2vva1r8kVV1xReQ3Ir/3ar8nQ0JDkcjkREbn77rtlcHBQgiCQO+64Q9rb2yWfz4uIyGc+8xnZtGmT7N+/X6y1snPnThkcHJTHHntMOjs7xRgjIiIDAwMSj8enjL/MQw89JF1dXWKtFRGR4eFhicVicvr0aRER+da3viWnT58WY4z88z//s9TV1cmZM2cq43ccR/7mb/5GgiCQXC4nH/zgB+WP//iPK+3feeed8uu//usiIvLjH/9Yli1bVtnW3d0t27Ztk9OnT8vQ0JBs2LBBvvzlL4uIyH/8x39Ie3u77N69W7LZrNx4440CyMGDB2f9rMrX9o1vfKN873vfExGRbdu2ycMPPyzLli2TH//4x2f9rD/xiU+I67ryL//yL+L7vnz2s5+VlStXiu/7EoahbN68Wf7wD/9QxsfHJZ/Py44dO0RE5ODBg/LAAw9IoVCQ/v5+ueqqq+TDH/7wnP2cPn1aksmkvPe9761c83OxGOd2ww03iIjInj17pL6+Xh566CEpFArykY98RBzHkR/84AciIvLkk0/KI488IkEQyNGjR2XDhg3yhS98YV7jfIUyL42tCfIFsmPHDnFdVwYGBkRE5KKLLpLPf/7zIiJijJFYLCY7d+6ccdztt98u73jHO2Ztcz6C/MMf/vCs40qlUpV+169fL9/5zndm3W/Dhg3ywAMPiIjIF7/4Rbnmmmtm3c9aK8uXL5eHHnpIRET+/u//Xq6++uo5+7/ssssqfX7ta1+T5cuXT9n+6KOPSldXV+XHYOvWrXLfffeJyOyCfPfdd1def+xjH5MPfOADIiLyvve9T2677bbKtoMHD85LkO+++265/vrrZf/+/bJu3ToRkXmL1mtf+9rKa2OMdHR0yE9/+lN5+OGHpbW1VYIgOGsbIiLf/va3ZcuWLbNu831fNm3aJHfffbe8/e1vl/e///0VUX79618v3/3ud1+wcysL8v/4H/9Drrvuusq28fFx8TyvIsjT+cIXvjDn97mGiMxTY2suiwvkG9/4Bm9605tobW0F4D3veU/FbTE4OEihUGDNmjUzjjt58uSs78+X5cuXT3n9uc99josvvpjGxkZSqRSjo6MMDg6es6+bb76Ze+65B4B77rmHm266adb9lFJcf/313HvvvQB885vf5IYbbqhsv+uuu9iyZQupVIpUKsXu3bsr/c823te+9rXU19fz0EMPsX//fg4dOsTb3/72Oc+3o6Oj8v91dXWMj48DcObMmSltT+9nLt75znfyox/9iC9+8YtznvNcVPehtaarq4szZ85w8uRJuru7cd2Z4f39/f1cf/31LFu2jIaGBm688cYp16eaH/3oR4yOjnLjjTdy3333ceTIEW655RbGxsY4ePDgFPfVYp9bmenXtb6+npaWlsrrAwcO8Ou//ut0dHTQ0NDAn/3Zn815PjXmz0ITQ2pUkc/n+da3voUxpiIYxWKRdDrNrl27uPTSS4nFYhw+fJjLLrtsyrHLly/n8ccfn7Xd+vp6crlc5XVvb++MfZSajDPfsWMHf/VXf8UPf/hDNm7ciNaapqam0iPQRF+HDx9m06ZNM9q58cYb2bRpE7t27WLfvn284x3vmPN83/3ud/OmN72J2267jccee6ziJz5+/Di/8zu/ww9/+EO2b9+O4zhs2bKl0v/08ZYp/xh0dHRw7bXXEovF5ux7Ljo7Ozl16lTl9cmTJ+d1XF1dHddccw1f/vKXOXz48IL6rO7DWsupU6dYunQpruty4sQJwjCcIcp/+qd/ilKKZ599lpaWFr7zne9U/MvTqT4+Fovx3e9+l6uvvppt27Zx880309TU9IKdW5nOzk727dtXeZ3L5RgaGqq8/uAHP8jll1/OvffeSzKZ5M477+Rf//Vfz6uvGpPULOQL4Dvf+Q6O47B371527tzJzp072bdvH1dddRV33XUXWmve//7389GPfpQzZ85gjOGRRx6hWCxyww038OCDD/Ktb32LMAwZGhpi586dAGzZsoX777+fXC7HoUOH+Id/+IezjiOTyeC6Lm1tbYRhyF/8xV8wNjZW2X7LLbfw8Y9/nIMHDyIiPPvss5Wbq6uri23btnHTTTfxX//rfyUej8/Zz+WXX05bWxu33HILb37zm0mlUgBks1mUUpWZ/K997Wvs3r37nNfvpptu4tvf/jb33HMPv/Vbv3XO/WfjXe96F1/72tfYt28fuVyOv/iLv5j3sbfffjsPPfQQK1euXFCfTz31FPfffz9hGHLnnXcSjUZ53etex2te8xo6Ozu57bbbyGazFAoFfv7znwOlzyiRSJBKpTh9+jSf/exn52z/yiuvpFAo8Od//ufk83mstVx99dUcOHAAred3y57vuZW59tpr+T//5//ws5/9DN/3+fM//3OstZXtmUyGhoYGEokE+/fv58tf/vJ59VNjKjVBvgC+8Y1v8L73vY8VK1bQ0dFR+fvQhz7EP/3TPxGGIXfccQeXXnop27Zto7m5mT/5kz/BWsuKFSv43ve+x+c+9zmam5vZsmVLZbb+Ix/5CJFIhPb2dm6++eYproHZePOb38w111zD+vXr6e7uJhaLTXnc/OhHP8q73vUu3vSmN9HQ0MBv//ZvVyI9oGSpPvfcc/N6vH33u9/Ngw8+yHve857Ke5dccgm33nor27dvp729neeee44rrrjinG11dXXxqle9CqUUV1111Tn3n41rrrmGP/iDP+Dqq69m7dq1bN++HYBoNHrOY5cuXXrOx//Z+I3f+A3uu+8+mpqauPvuu7n//vvxPA/Hcfjf//t/c+jQIVasWEFXVxf33XcfAJ/4xCd4+umnaWxs5K1vfSvvfOc752y/sbGRBx54gEcffZSlS5eyefNmcrkcTz/9NP/4j//IV77ylRfs3Mps3LiRL33pS7znPe+hs7OTpqamKZEvd9xxB9/85jdJJpP8zu/8Dtddd91591VjElX9WDkParUsXob89Kc/5cYbb+TYsWPztsAWi/e///0sXbqUT33qU4vS3r59+9i0aRPFYnFWX241IoK1liAIcF0Xx3Fmda1U88lPfpJDhw5V/O41asyTedWyqPmQX+EEQcBf//Vfc8stt/zCxfjYsWPcf//9PPPMMxfUzre//W3e+ta3ks1m+ZM/+RPe9ra3zSrGIoIxhjAMK39QitOORCK4rovrumit0VqfU5xr1Fhsai6LVzD79u0jlUrR09PDH/7hH/5C+/74xz/Opk2b+NjHPsaqVasuqK2/+7u/o62tjTVr1uA4TsWfWbZ+C4UCmUyG0dFRxsfHyefzGGPQWles4rIAh2HINddcQzKZJJFITPm7/fbbF+PUf6lcc801M87r5XJuLwdqLosaLwvKcZzGGIIgIAxDrLWcOXMG13Vpb29HKTWr1RsEAdFodIpVXd1eT08P3d3dOI5Ts5xrnC81l0WNly9l/2+1+8FaWxHLstVbZqHumLJ4h2FIX18fXV1dlXRkx3Fq4lzjBaEmyDVeEszl/y2jtT7nJN759lst7uVx1MS5xgtBTZBrvCix1mKMoVAoVMQPSoL4i550q+6n2u1RE+cai01NkGv80pnL/ysiHD58mLa2Npqamn4pIne2OZaaONdYbGqCXOMXTtn/W3ZBBEEwI826LGTlf18IUbPWksmMMTQ0TGNjI+3t7TN8zWWXxbmoiXONxaAmyDVecKoFquz/rRbgs4muUuqsVupCCMOQsbEx0uk0YeFJ2ht/QGP9UQrRT5BOC0eOHCGZTNLR0UFra2tFnBcqojVxrnG+1AS5xqJTPQE3NDREfX19ZVtZrH4RSSi+7zM+Pk6xWOT06dMoQpZ3PMfq1geI6AOV/boa/4yw829xN7ye0dFRenp6OHDgAI2NjVOKNJ0Pc4lzf38/bW1tRKPRmjjXqFAT5BoXTLX7IQzDijUoIuzfv59t27adt+As5LhCoUA6PcjYWA/53ACeG+A6BdqaFUvbBonYf0HJ8NSxi0tRrcLm/ztu9N8r5UNFhHQ6zcmTJxkeHmb37t10dHTQ3Nx83j8m1eJ88uRJUqlU5XXZap5P+naNly81Qa6xIKb7f8sTcGWq/b7l1xcqMLNZqCJCLpcjnU4zOjqK8Q+wsvN+2uufor1+ZhtKXj1DjAO1niJpkCdAIPS/hxd9G9YOEeb+msaG9xNZswZjDMuWLaOnp4f9+/eTSqUuWJxFpCLC1ZZzEAQ1cX4FUxPkGmdltgSMclREWXx/EY/cIsL4+DjpdJp0Ok0ul6Ouro5UKsXq5UeoV38F5AjpBOmZebx5ilBfjCsHsDRSUF1YO7WGRpD/MjY8gC38I1DE+P8Xcf8XWkdoamqquC9GRkbo7e1l//79NDU1VcR5IdegerJwLrdGTZxfedQEucYUqv2/o6OjiMiUwvFlAX6hMcZU6k8MDQ1x+PDhSj3hNWvWEI/HUYD2/xYV/D1qIqtf6+VYM1OQQTAySsjlhLIXZJaCRuZ5rJn0LYvNYcI7cZwPVt5TStHc3ExzczMiwvDwML29vezbt4/m5mY6OjrmFaJnrZ31OtbE+ZVNTZBf4ZTdD+Wbvtr/Ozg4iOd5UyblXijKPwDpdBpbfJoVLd/BldU01G2hpdnSkjIgI8AoSsag4BPSAuH3UKoqYkNOYlFUl12x0oBx1pEzh0H1E0dNKSygcFATR1WOcS4lsD2IfZhEfRcws7awUoqWlhZaWlqw1jI8PMyZM2fYu3cvzc3NdHZ2TvETVzOfcLqaOL/yqAnyK4i5EjDKTPf/lv2bLwS+71fcD+XVTVqaXFa1/CsJ7/sY1U3IcerjD6P1APiT4yyPSBMl0OuwtOKoJK49iZZ+lL4YsXsRtZIxP0ao96Pk0YmDB8irpcTpQjmJkmVtn6qIsag2jG4nDHdW+kvW/zti/1+UTs55PlprWltbaW1trYjzqVOn2LNnDy0tLXR0dMwQ54UI6dnEuSzMNXF+6VMT5Jcx8y3AM9dNrLWeItjni7W2stZgOp0mk8ngeR6NjY20trayZs0aovZ7uP7fYlQ7BbsM7PNopfEcKJq1eM6BGe0qimjpJ1CrEPM0IUWUXoNSbRRYQ9HsB6ck4B7gKYXCQejB0otjy46O9Vg5Ce4m/PBZxPQQAs7ENVI6Sybze8TiH8B1L0frsz8xTBfnoaGhKeLc2dm5qKF01toppUaTyWRNnF+i1Mpvvoyo9v/mcjkymQyNjY2V7QudfOvp6SEMw3mv5Fw9jmw2y+joKIcPHyYajRKLxUilGkmlXBrq8zgMo2QIZY+i7R6sDRGeBzuMj0FNOBI0Ct9uxHOmrtFnRePrLRTNbnAupmCOknDW4NhnAR9fXULRPIel9KW1KKKqiQgRLH1AhAhmwt2hUHoDQpKC2UMgJYvdwcFR03+QNInE/yIau2ZB1wRKP0yDg4P09vbS29vLihUr6OzspKGhYVHEU0R4+OGHK8tY1SznFxW18psvd87m/83lcvT19dHc3Hze7c/XZWGtnRIBkc/nqa+vJ5VK0VCfY8vGQ1j7BNYepp5BdHFmm0q141tFgMEiKASFxiK4ejciMZQqABCoNRQoYs1jpWNtH0aGGA2HcFQrCWc1ETmNTxIhg0URUSuJOJ1Y88hEjz7ibEPZpyhN+OUp4hPK5OKwBoviIjTPV58t+fz5CbLWmiVLlrBkyRIymQzNzc0cO3aM8fFxWltb6ejouCBxrk66KbungiCouTVeQtQE+SXCQv2/juNcsP9XKTWry8IYU0lBTqfTBEFAMpkklUqxdu1a4vE42jwD4T+RjOzAD44CpXYKzuuok50z2gxtL0XxKT+EycSfAiwC5FGiCOgitAeIKY2rInhKE0ofRtWRlxxGBhkNB3FUG56zFWsewrWXEY1Eq8S43OcJXElSZA2h3QmYqYNCKEqKoLiJZLwHJUMT578X3/8xkcjVF3Rty+JsjGFwcLAizm1tbXR0dJBMJi84oaacel4T55cGNUF+kbKQAjyzsRj+37KlFQRBJQKiHArX0NBAY2MjS5cunVzhWQrgP4AUfoZvnkHkFPH41DYD8xSBXolLH1atBJWkIA6h/RnTPWJWX4ZjdyEIDgpXaWL0onRsyn6O0rSIIWsVeREKAKqFonkSL9yE1mDN41PbtvVki60EoZBIPDXj3EUcxNtGLngCURsZNWkizgaiuhFtj5PPfWmKIId2jEKwk0T0Py34OjuOQ3t7O+3t7RhjGBgY4MiRI2SzWdra2ujs7CSRSNTE+RVATZBfJFTPnOdyOcbHx0kmJ2f1F1r17EIEuTwB19vbSyaTYXi4VA0tlUrR3d2N53kzjwkeIF+4g5x0kFJHcBmao3VDkRZC1YpjBihqjbUzLWYAsX1oFC4lsTCi8fVGHBygH7GnJ1wbAIqocrEERNAE0oujO/BRuM6uSpuFYht504ITPYaK7iEWW4aImhI6F5p68nYZ8ETpDXcvnrMB3+zFnzCiI9Rjsn9DMvYOMsWHGMx+FSOjrG99CEdP+u0XiuM4dHR00NHRQRiGDA4OcujQIfL5/BTL+XypifOLm5og/5KonoCr9v8CZLNZenp6SKVS593+fAVZRMjn8xULuBwBUa7pEI/HWb9+/VnbCMJdjOf+CKsvJmueIua9ioTMFGRBo/WVGHKEMoqoIZTtmTHboVFEcHAYQFCEgKUZ6Af7DEatxCGHo6B6rsRFExOXrIQ4jBL49RSLDqh6jCzDOhrj7cf1BirHGDmNqzeDlERb6VUEKgB7uLKPUoJvG4h4V6IIQAyCYbz4c0aLD5MLJi3sodxdLEn8P+e87vPBdd0p4jwwMFAR5yVLltDR0UEikTjv9mvi/OKjJsi/IOYqwAMz3Q+u6y6Ku2G2NsoREGX/bzabJR6Pk0qlWLZsGYlEopJBlk6n6e/vP2s/xp4hk/sgQooxcxyAnM2TUOXzFgRB6ZI32NiHphyvnG1gnqyMzVWaKA4AARYjHlr5IJPjENtD3tmKtUOgClg7jMgIqJI8h6YVX5YR8RT17lG0Cgl0lLx5btZzCCWNIwrtXk4mPICQn+gohpGNZIJ+cHchhWYaXIPW/US9beTD57GSmdLWcO4uWurej3OO0LiF4rounZ2ddHZ2EoYh/f39HDhwgEKhQHt7Ox0dHdTV1Z13+zVxfnFQE+QXgIUW4JmO4zhTBPt8KAtyqQh7piLAhUKhkoK8cuVK6uvrz+qHPtvEoMg4mewHsHaEnLoIKwcByJuDGKcBJaOMIWigZMfNbMuagyjiWMmhAYOQw6JpxmG4JMZVBOpiitIP5qcUCuupi5Us2epT8NxBPKUo2Bg+Yyi7kqgTJ+6+hnywB6XqEVWHUjGECKG4KPcixsNHgDxKtSN6LZngAEb2VO4S5Q5TkG7ILcOP7UHr/LQLEgPlMpS7B9fpZjzYR9Ttojn2Rjx9/k8703Fdl6VLl7J06VKCIGBgYID9+/dTLBbxfZ9sNntB2ZWzifOZM2cAaG9vr4nzC0hNkBeB6gSMTCaDMYZoNHreBXguRJCNMRX3Qzab5amnnqoI8Pr164nFYvMex1xRFgAihkzuIxj7PKGznWL4ZNXWkIJei2+enIgBhkBvxbMzJ88gjaUbxXGUigABYLFKoSWBYhyAwKQY85uIxiat3Fgsh6AqdSwq6EsoiouviuT9S6lzz1A0RwCNcS4maw6BpKceYyHiXopW7YwUdwAzx1rvbkCTxY26iGwkKCp8OQ6qgNIFUAFKtTBQ2MF48P8hhIBmNP/vOMqjKf42UrE3o4ic9bovBM/zKuKcy+V46qmn2L9/P77vL6rlnM/ncV23Zjm/wNQE+Tw42wrI/f39aK1ZtmzZebe/kAm5IAgq1u/o6CgADQ0NpFIpYrEY27ZtO+9xnG21jlzhfxKED2HVRjLh0zO2Z83hKV+uXLiPhL4MNICDiAYp4qhxtAxM3NDB5AEywFj+IpKxQ6Tz6/BiR4nGBqf1cgqtL0PsLiCG6E3k7ADFcN9kM2otGRmnXr8G156gjjQ5ogjFiR0iOHojAVnS/vNodRxHJTETgi1YXIS4uxzFEIHtIbBHS8e6kPS2Ml58FuxFFMIshchxoOS6cVWSVGQTkMPYIU6P/SnD+XvpbvjbF0TAHMehrq6OrVu34vs+/f397N27lyAI6OjooL29/bzF2RhT+TGvuTVeOGqCPA/OloAx3fp1XXfGEvUL5WyCXCrCXhLfsbExHMehsbGR5uZmVq1ahetOfqTHjx9fcN+hPc1w9k9R1OPwWkStBC6ZOobiP1Hw70JIkbFDlGOMy7hYnAmfLoBIDMdZRw4fFeytRDRElIPlEkK7AhMGhCFYUWjXw3Vd3DrNaHELkfgTc4xWYfEw6jVk7fOYcKZV62LxVUg2fAZwqXc30+RGGAkP4uj15MwxQrOnsr+VAonIJjL+4xM9KIxESynZFPF0d0mMMFgJyJkckch2xvxHIWJxEFwMiINWaXLBpM9cEacYnuTY6MdQ7rUL+FTmhzGm4v+PRCJ0dXXR1dWF7/v09fWxZ88ejDEVyzk+PSbxLEyvTlfzOb8w1AR5GgtNwJiO4zgUCoULGkN1nYJ8Pl+xgMfHx4lEIqRSKdrb21m7di2O41xQX9WE5hT947+FkTNEnG0UzKdp6ITezMXE3auJe1cjdoRs4VMAFNQqjJ2a0qyxxJSt+HRN2IJ26glMKbQtoi4HnsZFkcl3Y9QIrjdC1I3ielMLyIcWlKsIws147rOABrUSo1op2JAxc5IwfI6kezlUZdhNGY97BNesIdDHEPHJBE+jlEvE2cpIML0EZ+mHIuM/Phm7IVFCczE9xadpimymGFS7ZkoCbNR+6hRozERCi0cscimBCcn7PgH9KF0EZYBRMI9BxyHy4Qbi7kry4XGOjd3J+tRf4ujzdy/MVdIzEomwfPlyli9fXhHn3bt3Y62tiHN1idXZMMbM+V2rifPi8YqvZVHt/x0bGyMSiUwpwLPQhS6HhoYYGRlh7dq15zWWcgry0aNHiUajlSLsqVSK+vr6BdUifuKJJ+btsgjNSfrHb8bImYl3HBxWYpgM/0LiOM5yXLUEK0LGPDqlDYWlTln0xKVy9EaKwTG0zlb2iVCSMUsC1GjlfU9vwpHpURAOxnQR2CZUpIV0uBMjWaajiJJ0GhE5NTlUFC7d+EEjSltwjiJkydsoYbl2Mg4WgxWFxYFpJTg9ZwNj4SghIxP9ODS5XfhmP56yaITS7SOIUljRKKWpfmKIu5sYC/bPes0jeimdiVs4kfkioaTpqLueNY1/Nuu+82FsbIzjx49z6aWXzmv/YrFIX18fvb29iMhZxXnPnj0sW7ZsQaGYZW0p//sKF+daLYvZOJv/99lnn2Xbtm1THvsXiuu6856Qs9ZWUpBHR0cpFoskEgkaGxuJRCK85jWv+YV8cUNzYkKMe1Ak8PRKtIphbYF0fh06OoKVLGAgPAYcI+ZcMq0VS7xKjP3iJhxvD1oL1jShVBeuM0bIaSAERqccHdjdOM42FKNYmijaPFlzHMsZ0GeIyqWzijGAUMSnAU+tRqsOHOVhbA++HARvqhWRcNsYDdMIPnYiVVorKaVh625y4fM4ugOrOxkO9lUdaYkqH2uex1PORHSGwhKgcHCIolSG6e6bfLibBu/VjAU7UcT2fcRtAAAgAElEQVSJuasJApdscZDA1nPE3D5hOUNv7j5aYr9GKvqa+X941SOcw0Kei2g0yooVK1ixYgXFYpHe3l6effZZgIo4l7Mwz2Yhz0XNcl44L3tBXoj/dzG+GGeLkKguwj46OooxppKCPN0yOXPmzKJVADtbO354mOHcHQRqPY5aj1JFjPhYySAyRMINCFUzBRlFEDSCA4TmORwVgYnJMQX4QRuuLmDNOsRVRN0rMME+RDcxTg8NajlaZvdru3oTRTtEzuYJ5ejMcZoDKGJMJEZPIwI0onQDxhwF3YnoRhy5mrHCEK72iEYclOSxMkrK3cRIsBut6tGqHiGKo+MoFacucjUD/jNYUxbjkCiGqCqVGrJEEClFglSuMSEWjzrnNRhVpHRbeTAR/xEAcW87Y/5TFIP9KFw8tRKpOzDxGSkk6ECrFg6O/B0Xt7SR8Fad5VOdnYUKcjXRaJTu7m66u7spFAr09vayc+dOlFJ0dHTg+/4FucfOJs7l9QUjkcgrXpxfVoJ8of7f8mTahXzxqgXZ9/0pAgxUUpCXL19OJHL28Kf5rCpxNspf/rnaGMx/lVF/B9nwIJY8SfdyAjN1Yky0g0cRRzXhyxgWixEhokwlUkEJKNWI9howegN5fZRQBskEghOsA+8wYMnbHPXThqJoRetl+KaUKVfvbGI0nB5NUbKC484Gcma6z7qJmLMMVwmBPU0o/WBKSSRapQh0SKB88kH1Uf1E1GrCMEmGI6AHKnWFYs5yrBTQGCIYPGXwKPnEo+5GMlOsZkBcYt5mCpJjODwGOIQyMmP8jopT567DN324pCjoDAnvNYRSJBeeIPQGgUEKFh7rvZklxdtY2X7VgmpYXIggVxOLxVi5ciUrV66siHM6nea5555j6dKltLe3n/O7ezami/OJEyfQWtPV1fWKt5xf0oJ8oQV4plOOkDhfQc7n8wwNDTE2NsYTTzyB4zikUilaWlpmRECci8X4cSi3UX2TWhswVLyH4eK/kzeHp+xftANMv52VMoRBJ4HeC0qwhfVI5CgGiFPAVQKqBXFWkQn3Ifb0xJFCRLVgvINV7R+jzr0MJbvwjYPRG4g5KSCPo9cjMkJo9pJwX8V4OLO+hVZTR+eynIwU0dQRTKvkBmAlTcxupuBM9eFav4WC9SB6kIRaCqqFrH0eYRzfHKdOQKkQF4tb9dUphrupdzeRDfehaSTibWA8PMlwMPkjkfQ2kQmmCrIiQtzpxtUptIqQC46C8siFvRTs5Pp/Ipo6dwWu10Ja38f+ow7BeJIlS5bQ2dl5zmSPxRLkasri3NfXx8aNGxkYGODpp5+u1NxYDHGuDql7pbs1XrKC/KMf/Yjjx4/zm7/5m5X3FlqAZzplQa5ULzsL1cvQl5MwotEoDQ0NuK7L1q1bL+jmWCxBNsaQDf8vo/59FMxhLFEKdqYFB+DbU9Sp1Vh1BABrPYq5Fbj1u0E8Ys4WstGSUAoQ6lcDRYp2H4STBXwUgkMMM0s9i5wNiLCKQbHU0cqoPzWkTZEgIkNE9GvRFPHDvVh8QBHKPkLRlMR+LeP4BDLGcPAsTe6VuPI8RoZQqvqp6BhiHZQ2aLMax4lSjBxAMY4mASqPq0KavI0UzWF824uokmNmlDgYjWvbqI8146ooVrnEvCtI+88y7s9cKDUT7KbBu5zQ5HHcVhwccsEucuFUy1p0hJhqxPMuRRElFJ9seJKx8BRQmpyMdN7PttT/qmTi+b5PU3sUSR3h4uaZYXMvhCCXMcZQV1fHqlWrWLVqVaXe9tNPP12pudHe3j5r4an5tF0tuq9kn/NLVpBzuRz79u3j2msXL57zbP7fuYqwNzY2smLFisqjZXlx0Au9MRzHmXdyyMncv9NT+AG+jNPiNtHsNiGERDoe5Ui2dyJjrEREtwOzCzJAoRgjEgMtbWgniak/ikML2mkmO2G1KhrRznqGwt00eVNn9JVYtHJn9fWKKFDtDISn8aUPT3Iz9yGkaPso2n48Zxu+XYXSEz5lsYACiZJXIb7tA0ATJ7Aj+GY5ocpCmERLEtepQ4wm5iTxXAj1EK5OoswqtHZxVD1GxiiYXWBD6rzNSBgSSin8ziNJQIbQ6WM06KuMMeZ04upGfFuur+FR767G0UmMBBiijEsPfrH0w9bkvYqo9iiYIxibJ5RxRIWMmRNgIOFeilIpIo6DI2MEZhBLlhF/J6eL/8rKpe9m6dKlnMw8wr6Rr5POPMfwkXqWtV5MR0cHxklT57a/oIIMTGl7ujj39vby5JNPEolE6OjoYMmSJfMW5zAMZzw9vlLF+SUryE1NTZXFMReL6qSO6iLso6Oj+L5fSUEuF2Gf7cuwWF+QsnV7Lkb8vTyX+QJJZx1D4XEGg+M0upvo0A9DJD8jTtG3h9BmKdbpm7U9IieJ6Esp2NMYewxT6ELFCvjmMCKKiLuVMXOCMCyFqI0G+0npFkI7RBGXCFEianxGs0riRNwtjJpT+BOFgnLmGFGioIrT98ZzXk1fsJe4rKQ6fUHTRKgT+OVHfRtFh4340cOgIcZl6GiIoz1CO0hRn8TgoWUVnm7AMITlBBG1ifGJRBKXJXhuO+PBVP90k7eafn8X0ymYHjzbTb33eqxKkwuP4EsBVxoYDY5gpXQ+Ioqm6OUMFp8BNK3RTRTMUzg0Y/wlROsayIZ99BQPoXDRqpm8LT1VeKqFmNPEkfGfcjq/F9/msASk2QUKvO7djOVzPNtzN4GTplvdREv4q4salz5f6urqWL16NatXryabzS5YnGcT5GpeSeL8khXkVCpFOp0+947zJAxDisUip0+f5ujRo1hrKynIU4qwz4PFWKl5PvUsrITsznwesPhSKsJT76zjlH+YxviricmOWY+L6hbyzBRk19mIL1H6wiGSziXEtDAeeZJQDK7uxifBcDhtbTuKFKSZgAig8IkQIQlMVkFTsgTtLGXMnMK3k6UvBZ+ou46i2VvVosJ1ttEXlN7Lq2NEzUq0cwykhbzxEKd3ot04CWc5KuIAHfh2mDG7B2U8klyMp7oIwha0M1EfA4MmRcxJYiVH0nsdVsBYA7g4uh1jJ6+Lb46BqNKs5QR17nqMjTFq9jPm9+D6q6mLbCUjTwKT/mCHOuq8VQwVn5l4XU/Bxog4b2DIfwyc42SqfoeEkEZvKfliSZADyRKEWbzIJvqLz1HnLGEsOAgIDpZThe8iSlXu4JPyz4ydaSYibTiOsyALdTGpr69nzZo1rFmzhvHxcfr6+njiiSeIxWIVcZ4uvgsJqXu5i/NLVpCbmpoqkQvnw/Rl6MsTgPX19WzYsOGX8mWuZj71LI7k7iMTHiHpbmAomKi0FpYeK/fkTrCRFcTiJ2YcJ84JlPUQApTEcdxNjAVD+Lb0iB1RSxBi5GScXHYzTakUw8HjyAwRL9V5CGSQcty7JYfjbMFMJI1IuBajLb4ZnNhvOtUTVQrXeTX9wd4pe4wVIjRGmwhVBHEGsVKSpTqnmYw9CGH1D6DgqSSONmTCJ8CbiDVmLcXwJI5K4DmdiIqRDQ9V3BPaxnDcdYQkiTstWDuAb45Qx3py9hSJyKUUbIZh/yiKCI3RLeTCIXKR4xSlh2jQBBOZhjHdiaBJB3uJOstw9FIG/YOki8/hqXqiNFWSTapJB/uIqEZ8GSXuLCGimxn095JwOhkPjqDwJyY29YwnH6uKBMu/x0rzpxQKhYoIdnZ20tbWdkGx9edLIpEgkUhUxLm3t5fHH3+ceDxOR0dHZVznspDn4uUozi9pQZ6vhSwilRoQ04uwl5ehdxynssryhYrxucLN5sO5LORseIpD43cjosgUx0pZxf4SRt0T5UFwKlhHl+3EcwMcFUFTsmGRLPVOK6PGJydHINiFi1Ba+F7jywBDwYQlWwfDQQJXNRJIObW5lDKsmD39aCw8REIliDgXkwlPE4hg5vBbF2154k9jzaUM2t1ECQlxMJTEx4tovOhFFMKnCETji4erkhRNX5X1WhpTKTa6n2w4tY5zKDGK6mI8OUpx1giOBlwVQztt+JLH0oh2LscYD0fqGPR3E9EtNEa2MuofYbA4mVUoyicS7aJo0kRMN1nVizYdaG8jg8FRYDIlPJAscb0aZgmNs1KkObKREM1QcQ8F04uHJm9OTSz56jE98WRyEA5Z49Hn/ZztXTexZs0aMpkMvb29HDlyhPr6+oo4L9TPvBhPfIlEgrVr17J27drKuI4ePUo8Hiefz5+7gXNwNnEuLyhbzjt4MfOSFeRYLDZnzYjpRdhzudzEMvQl90MymZz1g1mMOhSwuCFr1ZQru42kRzge+Qw25hM1qxl3SskWsbqljPmlFZKVuKSdPjJFywZNKRKiilCGiTJGUoeIlKImIqqZjB2FacFvoWTwVBdWhifKXbp4OKBmXitPLcHRyxi1dWQKJ4mRJaJcJpcsnUrBnsQzCQKpQ7t7aJiIkCiKYNG40oYmRs4Mk7fxiWoREMg4jd5miuZprAgBEeqVAabWTxbTiM8qhkzpCSKml9CgI0CIphPRHnnTR9b2g58GNA3eFtLBxErTCjR1NHjdBNZjsDjTpyyiMWJoiPwnDKUC+SOyD+zsfvq8GSZqXo1EA6zkMGQxtoCQpbewC6U0gqVUHQMmnz6EhNNB3vROac9VDYhezpA9wlh4io3Bf6bBK33Pk8kka9euZWxsjN7eXg4dOkRDQwMdHR20tLTMS6AWe7Kwelzj4+M88cQTPPHEE9TX11cs58VMQtm9ezfbt28HJtO3X6zi/JIV5Grrs7oI++joKIVCoVID4lxF2KtZSNrz2ShbtxeaYFIoFOjr66u4VcpxzbZpF4E5UsrwioRgIKJaGfAPVY5vjqyj1z9ACJwJN7PUa0erUTRRxoIMmbCXmFcSN6UAAU0fjapU0SEQTaGqbm/enqYs1AIYlSCuOxDyOGopRfEYDocYN2ngBGBJ6pAigqdKKc8KwHrYMImSFNrVGNVPqDN4KjelyHxEgaGB0KYx0kHeDE6sPj3JuBlCWwdflex15WwEWyoFKqJxna0MhkexqjoWegRfX4K1PQi7SeDj4mBxCdE4qoFAhFTkDYQyTrE4hpCjaAZwyJJyNKEtonQURRIhQlEERSM9xYMUbRoQmr1uMuGRknNBSvHcGlX6T/dhdF8poYbSTVgSD43WQihSWSlwOvFpghx3uhkzQjYsRaIYfH46+Dne2vHZiboapbYbGxtpbGxk/fr1pNNpenp6eP7556lvglzTT3h953+f8x650O/yXCilSCaTxGIxtm/fXrGcDx8+XLHoW1tbL1icy+7IassZqGQHvph4SQpyLpfj0UcfpVAocPXVV/OBD3yAyy67jFQqxbp16xZUhL2axSidCedfYL7arTI0NITrupWl4suV3YpmmJ8O3QNAg3cxQxOWnOesQMykFVywpZAyVxKc8Y9xxg+5vC7GcFgkZzs5FViGZB3W9tDmGur0KFFVJK4CHCU4ymCt4M9IFQFNDFevZcDEGQufBU4gAnFdpNkpopCSK0XqcImQmIgjFgAdoCLDiAzjC+SIgURQEkVNuB3qnCXUO+0olWDQHMN3Dsx6vfK2F0/FkYk6yqPhftq8rVg7TI46hoO9U4xyhUfCXU063EW9DjA4aCz12lJPgBFFXgLG/VEKEqfecVE6jatkYvSlxjwNljxqIrSvTgGcoUkr8ngThf33U6/BCBilS+4iVRJnYXJRVSuUihJRqqkB4CmFpZ5gltod4+GZyqKsCW8zvcVThDLzSeWx4b/ntc3/DaU0+XCIU7mfsirxFlwdpampiaamJsaDfn7Q+z6syfCTJ9bQkdxCZ2cnjY2NU+6fF0qQq1FK0dDQQENDA+vWrZti0ScSiYo4X4i7Zbrl/GJk0QX5+9//Ph/+8IcxxnDLLbdw2223Tdl+4sQJbr75ZtLpNMYYPv3pT/OWt7xl3u3//Oc/56Mf/Sive93rcByHr371q6xZs2ZRxr4YSyfNt53ZSmtGo1EaGxvp7OwkmUwiInR1dU05bm/mSwSSQURTsKWwP4cEg8GRyj5JZwXDYSm5IMoyxiZqQxz1OzhTPEqT69PoLiFreoAoGdnMsfxp1sUPMyZFmpwWktriqDTDvoc4JXGP6CVYVnO00EtRTqNQNDn1IGM0ulkcSiafsYpxWwcKQhxCUaWMvipCUWSJUao4XKqyJmhCYMwMMWZKvuU6LsJn5rp+cd2NUnmKVW6BqF7OuE1QCE8TTjtGESHhriQTPk8UHzPhAQ9xiEw4BhwlJJSPi2VcwtLNoSZbqMaIpmBdtFLoCdtdI0RVgC8eBl2xcp3qQ5UQZxkFOYOmAWPyaGeqESAYWiJr6C0+O+O882aAJm8NVjVxqrCX6tJJEdtE3G2gr7iHvuKz9Bf2EdOKTLgPIwVO5n/IlW3/k4hOkg37eKD3ZoTSZxtd9Ryt/BonTpwgk8nQ2tpa+R6+kII8mzBOt+jHxsbo6enhwIEDFXfLfMV5rrFfSALZC8miCrIxht///d/nBz/4AV1dXWzbto23v/3tXHLJZGWwT33qU7zrXe/igx/8IHv37uUtb3kLx44dm3cfV1xxBY899hgA/+W//JcLWnV3OotlIc/m+pie2Tc+Pl5ZXLSrq2vK4qJQSsP2/an+0P7io/QUfwxAo3cxgxNlHevci0j7k5EJni4tEx+zSxhkspiPbw3N7jJGzSmWRzeQMz0gmn6/l5zN4TmbKZonGTHDjBhodVcR9UElIGMaOZw/hXCs0l5MZYmIIuLkS9JTXu5HokjVdz00a3DdSXdKwbpT3CEQwSNOwMyomSKn0DaJ1RlAk3Q3IAJa5ciaPkQUde7FBALD4WGgnzbvUjJVxeo1iqTjoOwuGrRLddTzmIkT0cFE0SQhRFMQF1AULUT15Ofo6RZcZy19/kmKdpyYTuLb2SJ9FAoPhSbqJInoRrSKoJQLoin6PjHbjheLMxb2kgnzaImhAg8Xl3g0gm8dGt2NaF0qeS8orAihtYwGo+Ttc5Q8zQolDsvjWxgZT+OIJabrsSIMlJ+eVB1xZxnjRrFj8PNsbriOHQMfnlzMFegp/JxNHf+NzW2bMcYwODjIkSNHyOVyNDQ0XPCiu3NxLv90tTiLCKOjo/T29nLgwIFKUa6z+cKDIJhzkv5lL8iPP/44a9euZfXq1QBcf/31/Nu//dsUQVZKVRI6RkdHWbp06Xn3Vw59a29vv7CBT+A4zqK5LIIgYHx8nJGRkcrEYtmv3d3dfU6/9nQr2zcF9oz9demFaHK2FGGiJMJQeLqyX0Q1MOCXalRY6hFK+zW5K+j3T9IVvYhRU5oUA0h5azheKFnT+7M51sQiE6nKMBgehRiMFdcwZk5W+hCEmPKJKUPCm/pY7VtNyFSLpCAOMUDEwXE2UbAHp2wXQurcVYzOEv1gyBK1FxGL1JGzZxg3J0g4reRND/XO5WTMEIPB1Joco+ExIgJxJ0cEO+EKGCMs+3urrrsoKMrsN6yro2iJoalHeR30FA8hVcWFAgnQxLAzshKFOncVWZNH6zaG/WcrJTbr3ZUENkaGA0ih9F5LZDNnivtKBeKAMQsUoS5YyYhTvu6WqArxlEEpUMoSV+W6zoqCOY5rWxmV45iJxJRGt4vR8BSB5AnCkvgO+M8zXPgxWk0V2Khu5VDm+7yq+XdxHIf29nba29sJw5CjR48yNDTEI488sijr9FWzkJA3pVSlNriIkE6n6e3t5fnnn59TnM83pO6XxaKO9PTp0yxfvrzyuqurq2LNlvnkJz/Jm970Jr74xS+SzWZ58MEHz7u/pqYmRkbmTgNeKBfisqhOrR4YGKC3t7fy5Vm9ejV1dXUL+kWeHmXx4/QD5OxaogzQ4F3C4IQwJLxLKpEVAA1uN1l/L03eek4Xj022RwyNZiQ8RUTVMRKURNjKZMLLqMkS0VdSsD+aMpZYwWVsQixKj+aWrI0xLjGa3WxlMs5ayMvM4uYF1YujVxOSYDg8OGM7QKGq7oVDAzG3G5EImaCfYX0Mz2+kJXIRKEXeZHB0B33+MxVf7MQIqFM+MZXB0aXVrst+WWPiFEmh9Py/L9Y6jBUuIh/dA8WZlrCRAqnIJtL+pGvBVY1E3dX0F0tPLFnTR6u3EY3BJ2DIn1latGB6J5Zqnfr4nvOOkTRLCfUpnKpVWKAUh136MRgn6rRgaGQkMtXXHndSjIaTRfsVloQuopVgRZH0NqCoYyzsZzAYYDC4n7bY5Syve+3k+bguTU1NWGtZvXr1lKWgOjo65rXayNk4X3eIUqriCy+Lc3miMpVK0dHRQXNz8yvbQp7LH1TNvffey3vf+15uvfVWHnnkEW666SZ27959XiEoqVTqgpJDprOQD6gc2TEyMjKluHwqlaKtrY14PH5B1n/1j0M6GGbHyIMEEvDqxOvxbMkNIaIZNVUCI5rRsAclLplwsk5E0llCr3+Udq+bwfAISyNrGQiexyVGjz9p+dbTzL5hn2XeRUTqDlRifFNJRf+EESgIoTj4E4/1ORuh3ilZ1HnxZl1SRtsm0nZtKVlklkvskETRRMJdRd70M2aOkSkepd5ZgUcHEjbiu0OcLE5dTLXVu5TxcA9a+dSrIlEVThEtI5rQRnG0AW8l1swUw/IIKvU3J4g5y+kzMcacIySLLRCdWSgJIJTJx/6kt4Xh4BSjE2LsEKMluo7xsI+oTpIJemdtI2f66YheSk9xsiqdiKCx4J6Y4yYV6twuXFVHT/EAluEZe6SDk5QiYwwKwSMkkAQRZzmZ4CjZ4szr8cjQnSyJfoWoM+kKLIum53mVdfrKq42UC9qXxXmhUQuLYcFOF+eRkRF6e3vZv38/sVgMz/MuOC/gF8WiCnJXVxcnT07e4KdOnZohSv/wD//A97//fQC2b99OoVBgcHCQJUuWLLi/hSSHXCjVtS3S6TRBEFRSq2crLn+hPrfq4kLfH/o3AilFEgyGzeQsNDmD1Hnr6PEnJ/OaI2vp8w/TErmUk8VJqzmuW4FhXF36uMuresRsF0My6e5wpYVhXcDzBlnjvJqifRIQivY4miQGU/JlVqWEjJgE9c4wBesQMNMScaSZjK1jsLiHJd5leLIbpVzizspSNqAZJW3OgDkM4tDkXULE2Yhvi6RtAmt3oxyZdfGwcd/HUdDkZJntXtMqDmY5VkXImX0zd5igTv//7L15jGTpdeX3+94e+5Zb5FqVlVlVXdXdbKpJqiUOLUAagRzNWBJMySPQgEDLEAyMNBjLBvSHx9AGARJMCxxYlIGBFhOwLbaJsca2NAQtSOYiNmfU7L1rycqsrNz3yIx9eevnP16sGZHV1VVFqgn5AgVUVUS8LeKdd79zzz13kUYfjRLTnme9WQgbYQRo+hQeowG57D4goz9DKxAc2uE+LCVHUp+h6Kxz3G4gaXgF0sYSjt/E8ySqYmAYcUS7GUeiktV/gKq/R8s/IyBAINGkM3Rumkwg3Rl23BPiQgm/0KHzlzhBMRwkKwNUkSAAWrKB7x0TUTI0g2EQb/invFH6n/mh3D/v/t+oLLZ/2kjHM/lxnN+eNqUghCCbzZLNZpFSsrq6SrFY5JVXXiGTyZDP58lkMh9YcH6qgPzRj36UtbU1NjY2mJmZ4eWXX+ZP//RPB94zPz/PX//1X/PZz36Wu3fv0mq1GB8ff6z9fbcAueOz3DGX7yhCHtXbosMhP0l0zIV2Wpu8VQ0tKlNals3mFp50uRF7kUll8Nw9GaAR49jpLVP1IM6B/QBNGJw4GyCh0NwCFVpy8KFxKn0O3RoL0RTrrQcsRl7E8V7HF01y2iUOvWOkBFf2fjZnfpTJoEQrMIdAQRfjlH2TlgiP89TdY9r8QWruHcp+70GiEyemX6HsHXDgrqARJWCMir/JjHoVm3sD21X8JK6bo2RugjQpumPktTKaCJspVGEicZA4oK0jgggJ/YdxgwpOsNrlyDuhig4fKohoH2OlcX/g9aJ8wLiS73oXK5go7iSOa2LrAWd+g3FjlrR2E1NLIaVP1a8jlWlk0MKRDRzZoGpvk9AmcWhhBwV61HM4GlWca4mWCFLGC1TckFuPqDOoyiT7rTWkuoEAqtTQAwshNYQU6LqBIloEsooqLBQlScuv4PU567myQVKfpumEgGwoceLaHL60OHNLvFL8Bsvxn2LMnAfem1boN7Tvd34zTfM9W7efxH/8vUIIgWVZzM7OMj09zdnZGfv7+9y5c4dsNsvVq1ff02P6ex1PFZA1TeMLX/gCn/zkJ/F9n1/4hV/g5s2b/Nqv/Rof+chH+Mmf/El+7/d+j1/8xV/k85//PEIIvvjFLz720yqTyQxk5E8SnudRKpVwHIfXXw8r9J3pHrOzs+9rKfY05HMdDvkvTv5Nl1u0RIJTGVI0NT9DRirQVj3E1ClO3Q2y+vPs9i19NTtDYB6Q9GaoajtktWmqwQ6WkubY6y2hU9oEt2sh/WEpM7SCMg+aG0wGz6BbKxT9E0AQyPMN0wpHXoK4OvgAMpQJKr5Fq72U1rBIaePs22+T0mbRZQFNiWEo4xTcB1TbKhEhNTR1noIbnlfJdYi0L70MFCJcp8QDTHMdUwg8qRJTbBAKklR7Jl8TXcwhxBTVVpOWckKh9Q5p7QUO3DHy+jRxNcALNnHlGRIXVUTwxbOsngPj9p7R1cvo6gwVv8W+fUBApVuEI5AUqyckTZOC+x0kAaY6R9kbpiiq3hGGTBInj696+DLAk0WCobaXMA7su8xbP0gtaLDfWkdSxEfBEr2Vjt/XMenKFvgaKfFh0E5p+PtD21SFgZSCcfM/4tQ95MA5RPY9xAH+tvhv+cdT/yLcvu8/sp1Av/NbrVbj4OCAjY0NotHoyEYP3/e/q0U313WxLAtFURgbG2NsbIwgCDg7O/s7ccZ7r3jqV+InfuInhnTFv/Vbv9X9+40bN3jllVeeyr6eJEPutCF3uvs68hrDMLhx48YTVZGfBiCrqsoGq2y2QgXBvC571ScAACAASURBVLnEg2bI+UWVGHutE05dyfWogY+DLrOYQcBea7WLl2pgUDfCpbYVUam6ENMSVB2IqnNIeuBjKhNASF80g965HylHzMmPY8u7oezq3E9GSDhyxxjTA1ptTXAHjBvtJbEqTVL6GCVvCxAoWPg8T9F7FRg0HDKDJQrBZvffdaVAzL2EZUpsRePYXQcEceEjhGiPklJoBRNYapao8iGK3n3soAgUB7rAXVnAky47TkcKqDKmP4cWJKn4gjPvAedDkQZRZZnVxhYaAS79vgseFj66UMHc7fO3A8uPDYj4pJRtEkIFqVOliu+FmXreWKDsDfO5pkgR0RbYaO7gywpe240EwJMKhhimxSJKFpQEx94aqm2RJI+nH5HU5tCVDDW/xbFzQMHdJ6m5VLwCo/igldq3+Lj7c6T1ye5Ej/cb8Xic5eXlrn/FwcEB9+/fJ5FIkM/nyeVy33UVhOu6Q9vvgPMHrUsPvk879Trxfop6HXe3YrHYbUNOpVJD45Vu3br1Hlt673gagCyVgLeMUKESVeIc2L0GiIye50Fzk0YAQesSwtjkzN/GkJeRoq99Wltk37tPVElScEPToWZ7WkjZG/Qs3mv1/r1ntxhrJ0Smn2OnDdxSqvRnxxomQknT9M848yeJihNMZYKyr/fAGIuoTFL1j8jqz3Ho1LnTCOfHXTGXaNHjbg17iYK2ee5CCBx/nIp7lwAX2g0XAUrYiNIOR1ZwvAoVNklrV9stzINR93eJKTPU+7jTgntIgUPy5rNwLqPNGlfZb1QoBA9IKk1iSkB/8a+zsEtoCwRynIa/RzMIHzANuQcytALypYIrBYgwI3MGoBukGAa7nPEce40NSv6dkTZOQsQQsokUfcMHnAlsw6cZtLNiJUDRZmk5KmdeeM37o+KdMG1eZd8e7oSUBHyn+H/y4xP/5RP7svR34V29epVyudxVRKiqSi6X+64V3Z6GWdj3Mr6vAflhGXJ/G3KlUum6u/W3IY+Kp5XdPuk2vl35BnUlvHGz2iSbrU0AMsE4D5qb3fet+ho/ZN7kwJ7gwK9htE9LSI1ie9hnTs+z75SIKSnK3gFxdZo9uwc+aW2KW7Xeg22vVWbGSOBJm0BIfFwSSp7T4JT+bCqlL7Jnh2C9a5/wQuyHOPO2abRBXydCQuRpBSqngWDXHqSXThqSRDsZT6vX2dMGJ1LrIopOnn3lAQvqDcr+O93uN18qqGL0NW4GBVSi+AxPJMno49Tt4WLWkb1CQpum5u1jKWl0dZ77zXUsxSGn2CgCPMKEWz2HG1Vvi4yR5dipMmE+R9M/ouSeIjBQcagHFnpgEWjDxwNwaK8zZcxR8XaIqlO4QYx79Q0UAqwRresArrSZspY5dcJC4rjxLHv+fWTQ0TZfoew12HHusRB5hkZrtNyvEVSH/i+iZIhr0+w0j7hffxfP056aEc95LfG7775LuVzmlVdeIZfLMTU1RTqdfmrg/Pdah/y9jk6G3G+vWSwWqdVqGIZBOp0mn89z9erVR/5BPY1uvScF5Kpb4WtnoRIl4090wViTGk3VH1BoSSzeraZYa+zjSo8fG5un7G0zYSyza4fZZ6PdTZbRpzhyztDFOP1jnAwxRoeuCLcZFpB0RXDohHy0pqSQfRnWhPEM261eNp7TLvN6rcTlyGWgiCajKLzAamuFQBl9PSvqKZPKMopocejuD+hwI3KCljxFUe+SUyQV/+1wud8OF63b8nw+7OCMceMjVNwdbDnouCblMAABBHgoWOSMD7Ft7+G6q2TUVrtQ2PksVAOLQAosFSaNywjh4AYVqu49EtoEnoRGkKMe1IgoDo3AIkDBUyWGtPBGOORBOCE7p2dZb6zTkk0kGgoBpvRGKkgAXBl26Y2Zz4Zt1AJMkSCmL7DT6mW9B60NTBHDHuGNUXIPSHnzCENH0WKU3BKHzgm01TvbO/8DH5P/mGntk6MP4glCCIFhGMzMzJDJZDg9PWVnZ4c7d+4MtG4/CThfpEP+e9E6/b0KKSX37t3ja1/7Gru7u3z4wx/mX//rf00+nx/Zhvx+4u8CkDt2oZ2uvlf4K2yrhRKo2KrbtcCdiSyy2ujxnGktg5QKBdfBleExF+wxNGWHqh+CcMRPURLhEjaQNkhBwR30edi1h0cutfwxTr2Q64+R59Dp98qYYN/uGd9PGjd40NxAEnCnXmdBPkvBDSho60yJLI0RXhSdaAQpAlnDkz3lQ06/ykZzj0DqzGp1fKkQnOv+cwIjLOb1RdD2ETWVLEf228TUBRLuJKgOrmLTCnaoehsYIosj6yAVYtocikxj47Ft75PV4uiUSaj2ABB6UnDmxmhKC11IyoHkyD3p23uMOXOWjUanoKrj+KFWG8CXLuORaxzYtweOWQ0sEmKOjcYJrmzi992SAQppfYGyN7hy6ETVK5EzfpDd5usgwLLnaJp1iq1BCsKRLaasZzhohXScIjVSxhyaSFLx6hREGaXlU1MGC4CGMMkas6w03sasR/mx1D8ZeRxPEp0MVlEUxsfHGR8fH2rdHh8fJ5/PP5ZNwv+fIfPeBkO/8iu/wte+FnoyNBoNjo+P31dx7ktf+hJ/9md/xo/8yI8QiUT45je/+dQqpt8LyuI8ADcajW5TSXwuwtZJmHmmvQmOtRDMxvUp7jc2u9vIaBk8GZDUxtmt927Yt6tn/JPxD7PTNqaxgjgORVShUfR2SOuX2Wod9G0nz7u1QR5eSIW3q02eTXyElnwNQZQO/6ig40kDTxZRMYgyz3pzve+zcCRa1LQwE43oWRrBaEDWMCh6LjF1HiGrBPiMG8+y2bxH3bcI0HECBTli2e4jMJSJ7qBRPwApVFQlQqOdBVf8TVBBxUQjCiKLFAZZ5TJnXpmKf0gz2EcRu6HLnAjwZYGI0svUpYRGEKfgmXTAddJY4sAZ7jjct9fPZaKDGVjT7xUEY0qOqDbDg8YmJbYJ+gp2/aErGeAcIEuFCfMmu61djuxbzFjPEVVjbAavjfSvl1LQ9FpMmi9Q922O7QOKzUOgR1sZmkVeu0TTtcHVaEqbmlqi2twCAYelf4OjNvlHkz87vIMniFGSuvOt2ycnJ6yurmLbNhMTE+Tz+fdVdP8gZsIXxVMH5EcxGPr85z/f/fvv//7v8+abw+PUHxaf+cxn+MxnPgPAn/zJnzxV+crTyJDPtz0/DICvXLnSHZjqBB7/6sGXmdSvAg1225IlIRVcqRC077aslsUJPFLaGPfqgzerqRi8VY4zF83SCM5oqSE1MabPcuauIxnUXWoiRz9dATBvLXK3vs03zuBj1oucmb3pGEkuc+hthKbyfowDtY8XlpJAmtT6aIFWcMG1lIKkcYlde5OCC5et54mpPtv2XWypEaAQU1ptMB59Q2liGodjnCCcLCIAT7qYIostezyxj40f2CBVVH+KI+UNwCeueqiCgUxYSoEuUziiTEpfYqvmUz5XhBNi9G3j4zFuLrHbGt2EcuJsM2fd5KzeYJcjpB0+yBSUC0Gj5A4+LBPaNL6MslF/QD6yRMNvsNl+IOa8eRrGDkJIYmqOhDaJJ6HgHLHV2iOjT+AFDq7srSzS2iRxLYcdOBy5J4wbU2z7g94gBIK0MsFa7QF1/0v8J/l/2h4l9eTxXhmspmnk83ny+Tyu63J8fMydO3fwPI/JyUny+fxjKUA+qCD91AH5UQyG+uNLX/oSv/mbv/nY++uA39MqOmiaRqMxuvjyqCGlDJs6dnYeCsDn4/MP/m++froOEm7EZ4gqZRpqjfnIUpeqyGlZWoGLECrbreHusXFtnjfLBxy15vixiSmO25OUDWGiYnBoD+pNd1uDYKOicmiHqxUFhZWWz7R3lUh0Dcud4lDbICXmOFPqNNvGRVLKNhhqnG9BPvULmEIglEFp1aR1g43mGuP6JHE1yoH9gACfS9aHWXVXEdJjXK9xERgD+Gi0AhUVFbpcb0BCn8F2Bgt3qjOFrwrqaphRx9XL7Nl1bGmjEKDjE9fiZLVJAjfAa/q8o4+e+FHzL/bDqPvDHLWCyoS5xJnrs2/7lDgeeNCoQiG4gA8/dY8YU1L4ok7OvMlha4+cMYWpNdlqDmqmS2qBxcgPUrB3OHFOOHEGj6XoHjNuzJLRZ3ClR8E54cg55cjp/Y52mg+4HLlKySsRV3PYgcdeY48DWYRGkbXGOvvVfX5x8Z8R0SM8abwfSkHXdWZmZpiZmcFxHA4PD3nnnXeQUpLP55mcnBxo2Pqgeh4/LJ46ID+KwVAntra22NjY4Ed/9Ecfe3/JZJJKpUI6nX7sbfTH41AWUsqusVCxWKTZbHYHLT4MgPvjf939Bl8/DfnF67F53irvkxJZ/kF+go1mmIV2wLjmNcgYMzTPjQhajFzib89COmLfbnGrNM+MsYerF6n6RyS1H+DM640gymrTvNOqDGxjUp3jvh1m5pN+jj21QMmDH5AvolqCZLBM2d+iGYTL8iAIzeilGL1KcaRNhhzNvoLglHEdN3CZMac5dnYouQp56zJ24FLzHXwZoAiB324svihOnT0sJYI857ZWam7TwTsRWFjiEkX9AZ1uuKT2PCuNTUwRAekQCAUbBduzOfVCbnwueROaowG56J4QVaLYfd1vQQCOVDmR+6gYBLioaMS1CYqey+3qFnoXgweThxnrGjutwcGu/RFTlig2PA6rEtsSbPj3ht6TNxY5bp5wp/Y2M9YldK+Mi4MhLHLGDIowKblFdluHRJQKKS1Dze9991E1RVafJJAqO60TJo0pVuttLrrvp5tWc1ScJp9/+1/x4/o/5NL0pUceBTUqHtdcyDCModbtN998E1VVu+AshLhw239vMuRHMRjqxMsvv8zP/MzPPBHl0JG+PS1AfhTKYhQAx+NxMpkMS0tLRCIRXnvtNWZmZh5pn18rvMv/tvdNAAyhsdts25NKn3tnU6SsBNMRh1P3iKpfYyFylbv1zYFt5PQM7/Tx8Dk9xd+eHRJX5vlPF2bYb8a4W4nx4dwYVb/DB2egr9FBCVQOvGJ3e0ei1B7tBGcurDdKKKLCnDVNh1Nu+AagYamD7cj9oco4UCAIIKFOUvbPqHinGFjMmtc5dQtstzaZ0ufYdTYQAhypU/cNktoF25UBGmD587hBg0A2QasTKC08tUxKW0RgceocUCRcgusiBlxmpc3F27LJmD5N0dsb2nzDf1hNQ5JUJzlpN3MEATQCAx8FVTpI4RN64gUU29rmiCrwRnh9AJS8YRkegKXESGiXeLW6zZiexVVLTCkz2LLHX0eVBCk9z3ZrHRRQpIIX+MxGblD36+y1tkMeuC+aQQPhKSxGbuJIScmtcOIcc+psdt9T9SpciS5x1DpCd6LEEylO7FOO3DJQBh3+XP8K//HJT3Dv3j2y2Sz5fP6xJGtPCo4XtW53fMm/nwp7T/0oH8VgqBMvv/wyf/AHf/BE+3vajm+jMuRHAeDzP6pwjM97Uyl3q7t8/sGfd/+9FJ3jrXJ4/fKkuFU9girciM9ScOCl8Sus1Ac5SlWo2G6s2/QBMKZn2WOXSgDfOV2g5NbZaha4EZ8BLQTT9erpQPYzH73CndomIDBVC88NQemSNs22vcuCNctWa5fVRpOFSJqyX6bqRYkoERTljOACXbArJS1fxUfH9RrMWItkrHm2W/fYaIXgIqTAli5SBt1uwDM3TlIbBislUNHcMZp6HUUBTy3hi5AXVYljKSkkcU7dja7MLK7mKbgWRW+QsokoiZHzsE/dPeJqtj0jcDicpg96CMb1wGwX5cIs2RpxHTQh8WTotXE+iu4JOX2MstdbRUybV7nfKLPV3EYTKlFNcOLY7Hg7LETmObF3mFAWOHGP8NwCU+olWl6TmlrmwN7lwN4lp08QUxNU/RJCKoyZ01hKkrrf4tA+oOiusxC9zInTK7rq6ExYM+jC4tSpkDQm2XS3OKwP0mNxNY6lxfmW8g7/4of/c87Oztjd3e1K1qanp4nH49/zTLS/dfvo6Ij79+/z6quvEo1GBwao/r3JkB/FYAjg3r17FIvF7jTYx42nbTCkaVp3dHinCNdqtYjFYg8F4PPRAfaHAfKRXeK3Vr+MK8MbOKXFWKmGGZWQgkZ7tRFVdFarx7QCj/vlDGPxHCdu7+adMy/znWJPOZFQI93tmKis1faZIJwi8h9OXV6cVEhrU+yKXnasY7DTDGVcS9FLrLazyLyaY8ff42q0J7nzpI8h5qg4HhJBI2ixaCxy7A4rDwIpOaSADEwyWo6SV+Je4z5JNYUv/e4DYc5aYqO5hk+vG9BFIxrM0lD6zZKSaEoKNWpT85tU2GVSvwnSQYoWblCl5Zeo+8ektDlq7ikRMcVWq4Yjh6HXDi6eMp4zpqg1R/+2jJhG0IJaYA0MJPWkSiD9rs9EJ4SAuBKj5o8eeZ/Uxil7BZJaFpjgzcoe4VCrgDkrQ80vkdJiqCg0fIekNkUTh5nYFR40VqgGlRDr+xaonpSMG/Nk5TybzU22m8dwToK4Wd/gWvxZHOlR91oc2Ic8aPQ9tBzIBBk8wyOtZ1BFhBO7wU7jlO3GAXDAJyc2uDF2hbGxsa5k7f79+zSbzW7h7WkZ2r+f0HWdbDbL9evXu63b6+vrJBIJlpeXn9qq+mnGUwfkRzEYgrCY93M/93NP/KRKp9NPDMidDLhYLHb/7OzskE6nHxmAz0cHkC9q26x7Nr9+72VKXk+sP2WMcdIKl8/X47O8WQpB9kpskjdL+yRUk3vVYz5ijtGhDOatWV4r7tOf6o7JDCdBCK55GeOUFvtt5cOpH5DRllCFQb+6Ysa6xJ3aJkk1wXZbFhdTIjRlg5xIs35u2bvWOGgX8drn4w/STp4f+iMbwid8JgnO+pbmFb/MUmSRPWcVA5M9e7M9aWTwOre8OBjhMFBdjNNUakzoKmfuMQoaSe1ZvlM5ZExPUO5yokmiikUziKK5GXbFNhfFqXeAIjUCMUxTucFo8ASouWXqQQw5pDMTCD8NaqWv0BhGQjWp+82RJkJ24DBt3eRO9RAn2MVUXHThoykqJe+ACWOOg06nY18CfuoeMx+5wm5zEy0wyBp5Wo5PyS9x6Jc5tMtE1ShpPcOhHX6vKS1DxhjHDQIO7WPeqa5wNbbMTqvfzF4wYU4SU1Lsl4pkIzO8XR72+gD4xukb3EiGcy3PS9aOjo64c+cOvu93wblTePtuF906VMWo1u0PorEQgHifF+UDV7b8whe+gBCCn//5n3/kz/QDcKlUotlskkgkui2dt27d4mMf+9gTHdft27e5dOnSSHs/Xwb8+r2Xeb3ckxfNmGNs1csESCKKjgwsSm4ICHPWGBv1Mz6cnuWt8i4K8MPTPnZgc9o0Kfu9LE+XKkFgUQ8cNKEQReVyIs/blR4o/XAujlCPOGv7WZjCxA4sGn6TS5E5HjTDG3MpOsN+8xBLmFRkr3nksjXHiXtGze89TASChYhC06shRZKSZxMgWbZmOfRGOahBUkniUUBTAlqBOgDwnUgpMdL6DoEITeTnzWWOnbuktcvstTSO2rTKlcgi+87K0OcBxtQcVTm6QAcwoc92i3n9oaASBDq+cNCFSdaYAWKcNKuUg4ORHXQpLUfZtqj5FTzhYaKR0mMk9SgRVcUQSQ7Kx+iRCK3Ao+7b1LwWduAxYZrYwWl76ncwkGWntCw1r47b10CjYTBmzqCJCK7rs+fs4jJs+6oHOjl1ipiZ5NA95swdrRK5HnsGW3rYvmC7cUrF6z2QklqMqtcYmmoCEFUt/ujDv46hXOwZYds2h4eHHB4eoihK11zo3XfffeJ77aLY29vDdV0uXbo09Jqu609NmfWI8UgZ3fcH0/2QyGazbGxcNAkiDCkl1Wq1623RarW6MrTl5WUsy3rqnNLD1BrfPNxkr3VOloXRNWC8HJ3hjWIIinkjyUY9fO+Z01Y2ALVyFKEolOWgRG8pNs+b5fCzzySmuVPZZbc5uK+Km2RcbwEhyObNBe7UNrkcmWe9rei4HrvE/cYD5rVptr1QdZFSE2SNDKpQBsAYIKFGyaqzbPt7lLwep/+gdUBOJGmq5fYvUobG6yIA5QhDgBsoeHI4Y1HxMdUDAgEhGF+n5G4QVV/gzereADR4D0kVDBLAxYBsiCggsJQohhJDFxaqsBBCo14RNA2XfbvAbquAlEfEtNHtzCktx5mjU/TKLMcus9G8j43HsVvmuK0nnjPneRCUod65RhJNeERVD1dWMBQ5RHcAlL0zLlnXqHp1LDVFw7c5tA/YaPRWORmyOIpNK7CZMPOYSoyKV+PQPqLmH6DUj8jJXJfGthSLSTMP0uCgVeXbZ1tMW+Pst06G9l/x6ixGZwfpjHY0/BavFW/zw7kXLrzGpmmysLDAwsJCt/D2xhtvdIG6w+0+zRjl9PZBj4GjFSLULkkpn6xV7XsYo4p6/QBcLBYHxit9twD4fDwMkP/d3iq6zNLxk1iOznK7EnK+OT3BrXKPD04KC6iRV2NsN3uZjVSynIn9gTWLgsJOs6MhhoJTYUZJsu325E1JLcJuq0DVN0hZOioaG41DoorFcVu7mzfG2GxusxS5xP22kdHV6CJbrV186bPR7M8mA8b1JJPmOO827nYzqECCIVxSehUpIHLucksZ/gkkNAOdITcz6TNllbvgNG1cwQsCdu0xav6wKqLkXawNPvWPUMQwLaEJg5y+yLEj2LVz+DIA6SKEA7IMQoSm8a2gz3FNQyA5r7dOa2OcOjpFN6SGmv5odchuawcriNBSHAQ+ugiHsAo6I6cEQkiiSpxxcxI3cGgFLZp+k5X6BovRq6zWh7n6tDKGEURJGOOUvDKbzYPhnQvQTJ155yqHdolDauyfk/al9cRIQAbQlYsB8xunrz8UkPujU3ibmJhgZWWFSqXS5Xanp6fJZrNPJXv1PI9IZLRW+gNf1BNCLAE/C0SFEP8HcBf4p8CfSimffBTzdyk6g07ffvvt7t87AJzJZLh69eqFX8p3My4C5Lrn8M2jTRzf48NzGU6cMgW7l+WmlSR7bX2xIgUbbYAdjyY5rIVZac6Ist484WZ6kvut3upgKTrLO20wvxbPc6++zxTJgf3PRzPcre1S9eByfAlL8Vlr7HA5ssC9xgaG0JG4pLUku/YBk+oYXuCx2niAjka1nRkHMkBKQUS18PAHpmwEEgIpcDAoe4K01uxmlB2GTCLwZBRDJIHBGoAfgCsNDlop5iJNTC+JrU6yb9vU/NFgUfaK5PQkjaAy9JpNk3njMkftomNESZDQ5llvnLLd3A/nzQkTXwahZ4YEVfjtXB7OKyMmzescOT1PirQ+xqndA2OAvdY+OSMz9KCQQpIjwbE4IqK654BBaV+nULC81Rxe+a037rEcu8pB65icMUkgBUf2MUdeKbyODUhpKcb0MQpugZyeI6XnaHgBm41TbtslZi2DQ4b9SwB2m0coiJF2+RuNfSxh0GrTJlk9xZiRo+EJ/rZwwt7cGTOR7MjtjooOYF69epXl5eXuoNKVlZUnktF14mEDTj+ooQkhhAyJ5D8EbgMO8DvAzwG/CHyV86XZD0DcuXOHr3zlK/zFX/wFa2trlMtlfvu3f/upAPDT6P67CJD/an+dlh8+3wwvy3IkwTvVkBIYl3Fu13oZy2Uzx73mGYZQedDsyY6mrAR3q4dUncGb5szuccmtwGHayrDbpxK4Fp/kbi1ccsYUk0IzTtWtEgRX+atChY+MXydraFQ8l7qrcq90xnw0SsYKQfBydJ7VxoOQg3ctnEDD0qewlArVvhvcDxTUdmeeE+icOgJFgC8Fsu1n3InrsWmqfd7FQQBOoIEQ1AOLpjtP0Wly2qoxYz28Kp7UJmg4w4AM4MqAtDaFIMdq4wDH30FC27RI4J6TpEnJhazfRmOXnJGgGVTJ6GOc2Bol95zHMZKcMTEycz8SRQI0AvwBT+f+T0+Yk2w2e0U0QxiMm7OowuTILpPWp1irbwxxujE1RkbPIaVJxTFZqxWB892Zx0waWY6cYUlhxaszreTYD4a7QBUUrseXqHgOu80aq5UKq31U0Ms73+K/ufqToy7ZyOjXB/cPKu1M9Og4v3XMhRKJxCNv+/z2v1+i3yjAB/47KeV/DSSklGXABsYeZ8Nf/epXuXbtGktLS/zu7/7uyPd8+ctf5saNG9y8ebPrTfGocffuXcbHx/nc5z7Hs88+yx//8R+Tz+efSjb83TQY+nc7vcLTrcMypWq7cCJBMwYLgF47nbyenKTeXgI/l8pzty1pW6sViCnh+S5GZthte95eiU6w1SyQ0XvuWJaiU/RqTBpprkUvUXMV3ijusVatYHuCmu/xoCz5t9vHbFdUXj09YSGWY71VZKVishRZ7pobeb6JE2hMBRk2WwVsr3fN/UCgiB5ICAJqnoUrNQLUATAG2G4eofYxZ7bUuuYSgtDxLPCTFNwqdf/hg2PFBY0XOTFLxdE4bsS4U9/Fkz6+VPDlsKqjE8EIvXAnHOmQ1hfI6ONtMB6dbXoBLEWvsxBZZMqcIaWl8XwVv62+cIOLKYDN5gOuWM+wEHmGnH6Jmq9yv77Lvdo6J06B9cYD5iPzRJQIs9Yc0+plTH+K3YbkjdIhb5a3SBrJC7efMy9+uJl6qIIQUjAmMkzLPDl1gYOGzt1qhVfP9tlvDj/4vnr4NgV79ANxVFzUpdeZ6PH888/z0ksvkUwmuX//Pt/+9re7DnCPEg8zp//AUxbAHeDXhRD/F5AUQvxPhNqqi7U/F8SjGAytra3xO7/zO91psMfH7y8J//SnPw30jOifZnS69Z5kuaOqKp7n4ThOT0pXPOXV0x7/uZga41bxhBdnplGFwlulnvtWTo+z0QrPqxWElfOEarLXxyP7MiBvTHG/tYHdh/26IogpJmvdbFvyXHKOU7vFvdoRm/QKcjfiM939xnUT2QStvTJQlBA4nEDydqFOQkyQ02E3aKERUNcBB1aaVZaSEWptSVeH9xVIqo5JgEpaC8jvnwAAIABJREFUaNhyWPPbCJos6Jc5dNdwfAUpRZfeWI4uUnNr3G/L2Y6dEsmHfCXVvikoKhppZjlsNVhpD4NNiRQaGq3AxZF6mxH2L7g5BYbUccRoLrjuBTS8BCV3+HcbVaJMmDPcrW0wbuQIsKl6VZZjlzls1burB0EEXQya/SAF0+YlKrbNndoGY4xxLHr7EAjGjQniWpqq65JRl3invH7+EADwLjJ2Ag5aBQTDsqmsnsLQolwW11ipHrPiOdA3FHavecb1xDQr1eFZfa70+PLuv+efXXk07+RHyWBVVWVqaoqpqSlc1+Xo6Ijbt28TBEH3/y8aOHxRUe+DCsYwSI7tAJ8CfplwjeMRUhejzVgfEv0GQ4ZhdA2G+uMP//AP+aVf+iUymQwAExMTj3UClmXhOBe37T5OdMD0caLjSHV0dMTW1ha3b9+m0WgwNTXF0USsq1q9mZrg7dIRdhCweuzROre72UiGAElOjXC/FlIGVxK5rhSuEzVHMmuOs14Pb9oZK8Nq/ZDF2GQbyCUiMHjldIuV2tHADWgKjd1GuNS+HMtxp3LIfDTNvdoRE1qEtXoI1PNqjNOgga9HUCMZ6tJhKTZHoW1e48qAMW0GP1AGFAKu3/MxntDzF16zhi8JgrCponOvzJjT7La2OHN7i/qKVyeqXJzZFdxjrCBB3J6lbCdZcQqUlF42VZZlZs0F3HYRUSKwlIunh89G54f+L6JEmDGf4TvFU1yZGXp9MXoFT2qs1jfax1xlTM9zJXKFu9X1gdWDI10mzQUglLAtRK4TVSdZb+5xEhQIRMCJOGGWWXLuBNlgFsfLslKt8lpxh3u1Q/btwtAxdGKjsU9aG+0hXHQrLESniSgmi9F5FiPLGEyxWvF4pXCAE6hUvdH3VbE2bKAkgLnIGGuVE+qePfyhEfF+KQVd17sJ3gsvvICUkjfffJPXXnuNvb29oXv2+5Gy0KTszoL/XwgNUlPAOGABsb7XHzkexWBodTU0Lvn4xz+O7/v8xm/8Bp/61Kce6ySednR64B8lzg9LVRSFdDpNJpMhHo+ztLTUfe9X3ukUlTRO7DBLNRSVFDFw+pZuEnYaoXJkQo9yZjeIKia2H/BcYhZFCCQST/rYgUdUxOkUxjJGlIPWGQd2+HnP1cgqaWyG+cKrsWleL7Yzdjf8MetOeN5TkRilVpWb8UnWmvvMR7JkDZ07tS2SWpx7tcH5cw8ax6hqP1Uhafg9W0R5AZ0AsN3ax1QNRBusIiJC068wZSzyRmVQURFRJmiMmJcHkJWz2CTYYT00Zu4PGWaWEn2Ajsib82y2RuukvXNKigmmWa/ZbIjwmO7VdvlQapm91hoZLUNES7Fa2yFvTTJlzVB2bbYaJ5zY28RVC00V3XPshOMrXIrcZLu5z0ot5IxNxWTSyCOEyVGzyqpbJ5ABzghq5MwtsxDJszVCVSGRTFsTlGq9zykoTFsTRJQESIv9RoWd+jBfvF4/wlL07uqsP45klTktzZnXYExJoqgWW80at05rQI2/Hr/HT84+P/Ka9ofv+489aNQ0zQH/ioODA1599VUikQjT09OMjYVM66hs+IOcIfc/PsaATxMW8L4JPAP8ihAiLqX8y/ez0UcxGPI8j7W1Nb7+9a+zu7vLJz7xCW7duvWBaGd8mMGQ53ldAC6VSt0ZYblcjsXFxS4nViqVBmiY7VqJW6WQQriRmeD103DJ90xsgjePT4hUVdKTBg3fYTk2wd1quKQ8cGuoQqF0rDI/afCuv4N/7vouBjliqoml6tyr7bMcz7cBU6I4cY58FyOuhLKudiQVi3fb7dZzWoINp8yEGWfHrRJVdbacUybNBLv2CVNKDDuoU/RCIJswxjjps/E0FBehDOqSZZCln+06sksXFslMZbDnN2+N0fAbvF0dBplGSw6p5xUUJo1rvF7ZByosR5c5cdZJ6EkiXgypKpwFJSqeTcEZXGrXL2hlBjhsHYEUpLQkmpjknerh0DmsVk65EX8WGxVT6jhBhbvVMzj3AJSiPJAdZ/Ux4mqOO9VdNEVjMbrAlC4585psNQocNAepkGvxS9yrbY48zgsmZAEhYI8ZaTJ6joYH67Uz3mpWgSoKggkz1bVb7Y+a1+JDqQXeLvcWyIaiMR8ZxxAWBDq3KtvsE4Jwf/zF3q1HAuSnlcFGo1GuXLnC4uLiwHTrVqtFoVAgl8t9oEG4PzQhhNrWHf8scFtK+S87Lwoh/lvgE8D7AuRHMRianZ3lpZdeQtd1Ll++zLVr11hbW+OjH/3o+z6JSCRCs9l8avK2fsrC933K5XK3q09K2c2AL126dOEP6jzt8ZW9cEWwGM/y5mkINC+mZ/jOQQjSTd/nRWuKd+vb6O2Op6vRMVabJ0yQYcP2eH37jH947SqnfhVDVdo+ugFe4JPR5kiaEjtwUaVJWqtAoHO/nfFeUpPseL0bb1xNUWi3X1uWBbUqs9EUp+Uq11PjrNR2SOgaCZmg7BZZjua5U9tk2hznTrW/ah8wZjUGGiXmjCvcrg1mtqduhflodsDZbFLP4wQ+DbnXxbnl6CJbzXUiYh5fDoPl+cZjS4mgMdcG4zCKboNp8zketG4B1e4UjQVr4ZyGGg7tEyKKjjOqw03RydiTrDgV7OBw6HWAcWWCb53tcknkcLSABsPLdU34GEr4PeTNWapVlx23jGwb37u+R8MXOELtUk/nwxmRqXZi3y2gSqVbLIwoFtPWJFIa7DRqRNUI/748fPwBknEzORKQAbYbBabUJHElhqNorJXPeL0W1jBUoZDSI5Td4e/otbMtXl15l2fnrzzUw+JpUwr9LdLLy8t861vf4vj4mJWVFXK5HPl8nlQq9YEGZ41eanICXAVoZ8U1wAXeN7n7KAZDP/3TP82XvvQlPvvZz1IoFFhdXe2a2r/f6DSHPA1A9n0fx3EolUrs7+8TBAGpVIpMJsP8/PwjF/rOqyy+sreKimhrXSUfSk91wbgT+0WPWMRkpRrelIqikBURsvYYGxyS0A3+dusEM2tT8QYLZAqC51Mz3D6pU/dKqEJjORqF9k0fIUKH1liIjLFSCcH4anyC1doxKS3C3eoBAsmJU+L51DQ1v07ZK5MQBiu1dqYk9QFITOh2t0gFMG3kuVMbbtwASKk5ym6JWWuBmuexVjsiZzVR2wzCuD7GTnODrD/Nije8jAYo+FXGVAFCktHGKTgWR07vOi5Y82w0Tjmyd8iZUVpByCFfshZ40Bxuj/bxGTem2XN2ulQBmBy16mzUiswhsEeANVKwHLvCO+UtZq0x9uwinuuTUU2aA7P+AiKKgy4TJLRMWGg9hwdXY0u8U9lkxrpY0LTZ2CejJym6wyoGT/gsKDN4QlCwbXbdGjv1Hre8HB9u3+/E3Woo4Ttt1wOSWpQZaww/UNmoljCVKK83h78LXwZciY/xRnFn6DUJfLu5h7jdIggC8vk8U1NTQ/TEd5Pj9X0fy7K4ceMGQRBwenrK9vY21WqV5eXlAUr1gxT9gPwu7Z9KG4wB/gzIve+NPoLB0Cc/+Un+8i//khs3bqCqKp/73OfI5d73roCewdDU1NT7/mwQBFQqla4SomMIpOs6N27ceGylRT8Pfad0zGatxEdy07x2us9SIsuto2FO90Gxyo+Pz/Ltsw1SWoSKZ2OeJmhGwkx7KZPhzcIRN6wJ3qz1wGUhksZpwqt9AL8QSbLu9DoYz7weXRH4SvdL79AYS4kcb5V3uJGcRNN8Gn6DklvCCRziihr6UkTnudVXXfd8UIxQ6qYqEl3onDmjWgpCdQAyhikmuFvbJwhCXtlzs1jGCRIX12kSUWI88IenI3eiFdgktHESWoxb1SqtTiOIFCxGrnC7tt3V5o5p8+w6K0SwOHZOEQiiaoSIEsFUTDRhIKSGEpjk9SR3a9vsNQYbT44poaEO8sl9YBxRDBzpdR375hKzrNZD1YOU4XlX3SSzyVnWG8Nc9VL0Mu9UNgHYaxVYiIxzYA83v4R88FgXkMeMNFk9R92F+7UztnA5GjGtBGCtdsC8lRs5YUYguBKdZspwOWo2eVA5Y6fSy9KT5sXjkYrOxfKzb9V2+K8+8SlarRYHBwe8/vrrmKbJ9PR0t036cc3pHyX6wf78ANX+8WoftBgyFxJCfBR4lhCcvy6lfCCEUNrFvQ+cuRDAL//yL/NTP/VTvPTSS+/53iAIutaaxWIRz/NIJpNdGsIwDAqFApVK5bEzdgif0G+99RYvvvgin7v1N/zVwTpndpO0EaHZkJTs4aVtTNPJqhbTOZNUROf2ZpWTShMzqlFzHW6MjXHnrEBU0zBzLfwgYEbGudeoE5xLu16anObVs8GGgIWcYMrI8HZb5nY9MclK9YiIomOogppv85HsFFJCRINte4/F6AR36pvoQsMUCU77GiBEoJGNVrBdnamYx7Qxz9uV4YwpHcSRapa12hmLVpam2MOmN49zQktyM5nGwcELTG6PaA3uj48mr/FGdaX7YzSFzrg+y+o5n4WoYpLQ6uS1aVoywzvV9VEzQLvxkdQ1HGo8qG8NdKotRed62+4DY4AbiXnuVHsPx6vxabZbW/hSEMhOuzWMk6CpDwLirJVnq1HsTgwHeDZ5mdXaMHBHVYtL0XlanmSnUeWgNQy+S5FJ7l8w5WROpNmR4QppykyTM9LUnYC1ShFVKLR8t90cMxgKgqRmUvJG25TORNLstZuPNKGwGBsnqkQ4rrX4H3/op7iS7HXu1Wo19vf3OTk5IZlMUq1WefHFFy+UrT1JVCoVtra2eO6554Ze0zTt78Lt7f2ZC4mQWPlHwH9ByNL7wCeEEP+7lPKr35VDfErxMAvOjq9FhwO2bZtEIkEmk+HGjRsjfwxPc9BpICX/z/59skaEimujezoH9uhM5pnEGK8fHVJ3PHxPspzM4rd8jtwWCU3nXjG8oRuex03GeOBVuWs3OP9d30iP8XZpuCA2a46x3ehlzS0/XIo/k5rg7fIuU2YcHYtvn4aFw+X4FMW6yniQJpfIDjjG+Z5K1GhRtQ2OagmuRye5VRkcLWQInbw6w+vlAkG7yPWgVSBjKcg232kJg5iW4i9P9rgWW6DpP/y6XzaX+fP9Iz4+fpmN1gZpLU0gzSEwBmgENpe8BU48jbvNLfJWktMRS36AcSPDa8U9UrqFLiZZiEXZaGzjSo+gA1RSsBRb7ILxzeQCtyu9opcMAh40dkcOZj2hyqxIUZLh9U+rSY7s+gAYA2w3jttDT2HamiSixDlp2azXztiu7fFMYnYkGAPdLP18xDWLuJXgOT/LSvmUe40W/ROnAZ5NTXOrPKwtDpDMRzKURhRYAeYiWcaNJA1bsnpW5K1ye6II8I39jQFAjsfjA23Sb775Jt/5zne6hvaJROKp8bvfj5I3GKxX54FfBf57KeVXRDha9z8D/jlh+/QHNvpN6s9P9+g4u2UyGa5du/ZIE2qfBiB3flivFnZZMNJ4Z5KPReb5674buD+WE1neOGrrfmNJ3jk54aRaJa4KjjyYjca4W+t76PgWpxdoUDOWxUp98OZM6ibCsah74XL0ZnKK25VDdEVhp3FGEIS64b85CUF33IyzX3W4ZbcwMPgHxuCDK/BNAulwXIsDgu2qxlR0mX1vFQlMyix7Dcl3ZO8YBQFx00G21QZjehopNVbr+6S0GOv1Aq3AZS7e437747K5zLcK4fH/zfEZPzp5g7XGLnV/mP6JKRGmzUlO6i7rbpg15vT0SEBWpYKQEZp+navxPO9UNjloVcjoaaaFypFzRkZLkDMmebcNxnkry/1aH30TQNiO4nNRMpSLTFBqlDECDSdQqMpBdcK4kSZnZDG5xBulHd5sVIDB461fkKkCbLUKXIlNsdk4Zj4yTlSJcdJssl4pslc54cXMHIULNMLeQx6Exb7CXUw1uBQbQwl0dso11k7q7NRHT+z5+sEGv3D9xaH/77RJW5bFSy+9RKFQ6HbgPS1D+4f5WHzQi3qd8IFsG4xVKaUnhPhT4F9e8NkPREgpaTQavPbaa9y7d49PfepTXQB+XGe3p9E63Ym/2d9E29e5tXXIs9PjWDGt62XRCU0Imi0XCSyaUd45OWHcMNltNFhMRsADofe+qpuZcd7cOSYxaQyJ95/NjPPuuYr6M5Ese02bb+7vs5zOoapFKm54Yz+bnOKt0h5BoHBkhwARVXU03+Ss3QY7r8b5fzdPeenyBNutY8bVcR4EFWptMJ7Q4twuHyHL8JHEHIYlebcxuGLpgHGnAHg5Ms1es9Sd0pzTcxy3h6uOadPsOoPL9kt9YAzh2KrVSp2YERuSri1H59lrntHyVdbd3rW4aGRqngnWGuGDo+z0+Oui26AIRH2TD6fneK0UrgAMoaFIyOgJIqpJ2bU5aFUACX1TUM7HVuMUVWpEZYpDysRUi3E1g90KOHabrNst1tlnPjrWbZU/H9vNAlfjeVZrgxlrWkTIR8cxRRTHbvBWvZep9vZ/NrI7D2CtdkIMjTqDv00BKELhB9NLHNYbrJ0VOSkOPgCnInEOm8Ma6TcKe9Rcm7h+MSWhKAoTExNMTEwMdeJdVAx8lPh+zZD7O/VKwK4QQidkMFQgA3zlcTf+Xn4WX/ziFxkfH+eFF17ghRde4I/+6I/e1/Z/9Vd/leeff56vfvWreJ7Hpz/9aT72sY9x8+ZNpqenH2vSBzydDBnCDra7dwqcVhoICVuFMsuxYTesK1qMvXqNiKJSa98tC+nwfUXPJaEbrJZ6/KPvSKSES7HBTjGBJG2Z1NogPR1JsmhkuF0uU3JCAF4rlVk250gocUxFY71+ih8oXRBRhWDWGGOz1imWwZnnEgD7pyZjeoKdmo0X9HwgkiLSvckDZZx9Z1CVIAiIWz01xvXoZe7Xjqn7Yba2FJ3lbl+2WXcHv7MxN88rhUE52LPJafbtIrZnocjwZ5zTUsxbM6xU98npk9yuDqo99qvDRa1Fa5a1PineVrNAVA5mVg3f4ZXTda7HF0EKbiQuo4kMESXFSvWYg1YH+ARXYpeG9tEJ1/d5JvYceDGWItc5agjeqRa555Yp9rUnbzcKXDbHL9yOJwOiqsn1+CzXY5eJyTG26pL/cHLMt0620cVoICrYda4lJke+5iNZToevJYTBopJmSZ1C1GLcPWpQa8Ld07Ouv0p/zMVSo883CHjl8OJpLeejvxPvQx/6EL7v/3/cvXuMXGle9/d5zqk6db/f+2Z3t+1pe+yZ8Yxnhl0W7SoSQlnBKomSAKsoQAJ5JfjnBZQlQiEJBLKQkCA28GYVKQlISRYCG6FFaHnFsoF9d24ej+2xPW23+36t+/1edeqc/HFO3bqqemzP7LLDT1qtp7vr3Oo83+f7fJ/f7/vjvffe4/bt26RSqaciSZ9UhjwAZF3X28B/oet6V9d1Vdf1nq7rGV3X/+WzHLjvZ/HNb36T9fV1vva1r7G+Ptnq/Cd/8ie5e/cud+/e5ed//uef6hy/+qu/yr179/id3/kdEokEV69e/Vge9sfFkB9nq+wflDkuVUn4XNTaHbTi+JJx3uFmt2kwvCvBKFnz3yeVGgG7nbza4UIgOBgIq94Am1kDQGza+MB7IRDjcTWHIsm8EpgnXW2yVSmb2/3G/1a9fmpdlXoTUK2UO52xHOKr7gXuF4cyw5o3Qs4Ezr1KnVzORnPEIyFsdbFpem4oQmazXCCuDLNlNA0skrHlKHSJC45z3K8eDDbNnJKN41NGNdv1HIowWNE55QLvt8bli4DFyWbdYIgHzTzL9lXWnMsUu01262kWlYUJMAYo0MAxUirtt3jYb4yfWweWPVOydXTItxtc81ym0Na4Xz42vUKGG3cApc7w+9V1w0rUipOodY6aKvPt3CaPewVul/Y455ydUdo5tfKREJxzRnnevUyzY8GhhXknk+OdbJKTEXba0zWW3bOzlWzyJFgrksyaJ44dJzFipMsSD0ot7hVL1DRjHOQb0w2UgDN1/39MTm8e8WGdiux2O8vLy3zqU5/i4sWLVCoV3n77be7fv08+n//Qz39SGfLpK34ghPhRYA2jfDqIoS3/x7quz85FmhKjfhbAwM9i1GDoo0YsZszoH0dfvdGQJOkj9/tqtVps5KqEXAq1agOXbAzaaqFnPFkMjHQJGydanQveAHczhta56g2wUyxxNRohX2rQHgFA90gJcq7cGkypAvDYbDR1OzbNxrumHk1PgEUfYMa2mdgvNB3F1WHUc/xF3wI30+NyR6M2lAOu+sNkRW7Mm33OHiTZNNjtFU+C9/JJOurQP0HVJDptB9WWnc/GzvGgNt4xe94e58Ep8OzoKlHrAhZh4Y3cZNbAostPtl1lRfHRUuG9fJG4YiNiCdJWdR6p04s4dCBhC7HTPEHoAofwkpzir6yauRgBqxtXx4qQrJyoVfZqZSy6nY16Ckn0UHXVMNpHxyIZPnYnrQySsKDrOh3VgtuqUO62KXcn9X7lDMP3416ZK55Fup0e1UaHw06TeyOa8suBRZhS8gyQ78weqhuVNE7JSsDmIqJ4qbc0HheK3C6VEJQI2Kbn8u/VS7iETH3KxuFmOYddkmlpk797Mz2dIT+Nva3H4xk0Ji0WiySTSR4+fHjmZuAn0QsZJgH5P8Mwpa9jOL3lMDySnzqexM8C4Otf/zrf+c53uHTpEn/wB3/wTAnbH3fn6WeJUVe3SqWCzWZjq1jH73NDtYHL6YJCjUKlxfnzPvbqZV4OJbidSiELQUc1MjIA/Gbmh2KVcEoSj0sGI553efggPRzYh4UKvoSdcrfFC8EYqIJgzU/b2eGwvxnUlYzyHvf4YLFYNTpFO9eXAjxsJFmxBbiVTjHK9uI2D/t1Y3DLQlClTm2kf5/XYueDslnEgsRh3TjnZrGCxSlQNR1NNya3C+4oN3NpQi4bTc1gkcuOxAQYAyRsAXQ1wBvlEdN7zfS5kG3sVupk2i12GF7LltphzRnD6xDQnL7JBKCYDPmSa4W75fEUPbtkZdERRdZtrDgWeVhNAi3ogQWJFUd4AMYWSUfTQe3JWOUeijx8vt2eoKUq6AiWnAkezSh53qidsOQIc9DMDc6/5Iwio3BUq1NvK9wrTS+y2axkjEKjKYrwfr3AOWeA/cbQGdBtsXHeGUZoFiTVylvpIzYZ3yDUgfOeAIX2ZPWdBlwKRgdVpqPR1npcC8a4XzAmz4TDw7zDR6fVYztbYqtY4EJgXKrr9XpPzWCFEASDQYLBIJqmkc1m2d7eHnS37suUcDZD/kGWLCwAI3nG/wL4WV3X3/+oB34SP4uf+Imf4Kd/+qex2Wx89atf5Wd+5mf49re//dTnmtbG6XsdqqoOALjfxTYQCJBIJLh06RKNjsrxN97F7jF2i3O14bI7Jrmo2Tps5A2G83I4wa2U8aLLCHaKxuRS6rSYtzvYbBmgGLe5SY0tVATnnH4eVFL4hJ031pMsur2oNQ3ZLuipGqgCgeAFX4T3zc2+BaeberNDsauxsV9lKe5lr9qcyGVOOLycmID8cjhOVSqPmbEuOyPcahns+Ko3wZ28cfxSt8N1R4iNcgmEwCdZ2awZoHPVusROexObsJDvjMs3q8447Y6VB/kce3Iau21YkNHpWHA5u1TaPTR9MgNDFhLpVof1coO1SIBUe3pLp5ra5rwjwb3yEUIXzNtDuGUnNU1lt5bjXtsAlec8MQOhhAHGF71xHlZOQO9hteioPcMtTu1JA48KTYeWakXVhmlvD8onxB1eclOM8wUQsfnxWjwUWl22KgXStSHrzbYauGUbtd5kZkS11+ac5GFfm54CF7a5sQgLXtlJvt5hs1gkUzCOfT0023lPfYaiCUVIuFSJG94FkqU6yWSN9AjY30qeTACyqqofKRdYkqRBd+v+ZuCDBw/QdZ1EIkG73Z7KkH+QwRgmGfI/AleFEAWM98UKuIDtp5UsnsTPYrQy7xd+4Rf4tV/7tae6+H54PB6q1ekv5kcJXdcHX+Cop0WxWByYCoXDYVZXVyderjsHSXTguFTFY1M4KQ2vr57vMBf2cr+aYd7l4d6ICdFaMMR6No8iS+xXy8yblVJBm4MPkpPLa2tP5nogweODEss+P3tFY2J64VKcO+mUUSEHZPNNPIqFeaudVl2nVDMGXl3ViVpjbNb3xo7rkq08LBkg6pZkNKnL4UizVIdkZaNigocOueY4q7LjRKeMIkFpRGNcLxfwORSWnfO8XznEIiQuOudJ1lVuZYabY41el2U5yGEvi9ozPOt11YqmT1/mPu+Z42bWANN2y4MkSujoOIWCS7bjVhwowopFWFB7Ei96gnS0HrdKB5xuIQWwUU3zcmCJB+VDLnrirJeP8cpe6l0Vm66hmjOTqklIPZ1OT6alWjidYtHTNaJKaADIfosLn+bAanOxVSnxj8kTvBY7xe7kJNPWVK4EEtyZUp4M4PG4oTx8rwKynQVHCFWTOSp22K0YJkKnY6uSn8mutyp5LEJCnVIkMvq5JZefmM1NvaGynSmy36yTbk03anovecJPXbk69rOPU+PtbwYuLCwMKgOLxSLr6+vMz89/Txqofq/i9BP5APgV4EcxvC1sGBryl4HbT3PgJ/GzSCaTJBLGbP2Nb3yDy5cvP9NNfBya77RjFgqFQVl1r9cbVPOdO3fuQ1+mO/tJIm476VaLtViIjdSQ+YgGWAtw1R+hq2mcaMMNE6vRZ5Ylv49kq8phx3jJV91+bhcn9dRsscGS10e2nmcpPgTk1EkNGxY65qCrtjr4em5SFZVLkRB7ZlbDqi/AcbbOaXfM5zxRbuVMNzibg540ztIuuePcKvS14zj3i+OTxaNyEYFuZmMMv5uq2ua67QLtXpsXnc/xoJzlzSnZDwB6z7ioWt2Oz9MiIS+yNUUfViSZ7cqQgSbrLV4MrXG3ukNV14EW1A3GNmf34ZCt7DSOsEkW4jYvqSldLoQuqHW7LGsh3JKbiBzFabFQVpM0dA0hoNM1Mk1swklLnV3pHcoQAAAgAElEQVRGvFXLcs11gcNGlf1KGajBiCXmsjtEsTj985Up5j392K3lueqdR9at7GQKHHc7HBeHz/Kc289+bXKyqXY7rPnCPCpP6toNtcvz/ggflMa/T4/Vxqo7iE/YeZQucHRc52ik0UG61SRgt1NsTeZJ30pOFpx8rzbd+puByWSSS5cukUwm2d7exuv1jtly/qDG6SdyCPxvGNpxG2MHoQNMb0lw1oGfwM/iK1/5Ct/4xjewWCwEg0H+5E/+5Jlv5KMuRfoFJcVikUKhQK1WI5VKEQ6HmZ+ff+pcyNt7SQJOG+lWC+epzwbsdu5upViOBOjFh2DlkC1sFAwW6rUpuBxB7uRSOC0WNqZ4XwDMO33cPc4gIdjKjYC+bnT66MeqL8S9TIZXEnGO6kPW5JFs3MukCS45KHSG7aRO6gZgnHP7sKkayRHQsgqZnZGB3uie3szRaGgqVtkywbSe9yR4I52h3dOIWt00z2hwftxt0NFlBDrNlhXdPn2T5rJ7jneyQ6C+4A1xM3/CgttH0nQykzSJl0OL3C8f0THP2dZUXBYF0TamjIjiIWEP0NV0dmt5diol5nDyQWaPK74o281DFIvxTHUd1J7B1qtqh4BdoalN5g4LICQFqHcl9mcUUOTaszMYtms55h0+jk1dfNERIGz1UmmpPC4WUWUbN3OTgAdg78x+th5ldoGUw6ogCcGKO4hc76LLdnZyRe5lc7yamCdbnz55LHp9UwE5Va/x1gcPuL56YVCY9b30sejH6c3Aftn2iy+++D0970cJC8CICf13gE1AAbzm72MYGeZPLdJ+/vOf5/Of//zYz37rt35r8O8vf/nLfPnLX36W654ISZKe6kvuF5T0JYhGozEoqV5bW2NnZ4elpSXc7ukdF86KaqvNZrrAhZjR06zeHg5UCdjPGiDhd9hwdLXBbPdcMMT7KUO+aOs9eqae95wnyL3KJJuJ2R2UKlWaqsqyy8VedThQokEPyZwBsMt2J/cyGSRhZBCcmIActDtYz2QBwTl7YADIV3xRHhSM8y043dw+zPCSL8Y91dgxX/MkuF0w2PNFV4RHpfHJwmLpYZFk1BGwFcBV1xy382l0BBckL49bNZ73Bng8o+18qdNC7zqwKm0aTYW03pigEE5Z4VFpqBcvufzcK58YntE9O0IXxKxe6qrEXq1MWwWP1YnX4sApW1GEzOu+FbYbJU4aZU7Mfm3PexOkWzW22hXidg8btWPsigHGPU3Q7o73B4zZ/ew1Jq0zn3Ms8F4xQ0SaDYAHjSLnXSH2pmRNeK12VpwxApKf3VKFjUqTjREhv3eG5tuzW0e7L43FSWNyVRCyOTjvDKB0LXibLnaK/b8Znq/Wmd0NRDlj7D2slJHv3UMIwdzcHLquf9/S0kY3Az9K4+LvR5ze1Pu3MDb20hh78wvAHPBbwIN/qot8kvD5fJTLZYLB2W3Im83mAIBrtRpOp5NAIMDKygpOp3OMZX+UXOS7+yk0XafY6iCAo+Lw5b8YDbF5bAy8o3yFWrKN65yVerdLZ4RpFlpN0u06FiE4KkxpmaNDwuHkfXNj0OsysjkALoQDrJuA6pAtlNqGhnslEkGMvI8rbj+3y4YM0msN713TjH9fC0RIlVo0ezr3jiu4IgrNXpfkSE6q0E8Pwh4uxUJbHxaH2JCI6G7eK2bANKs5MSvRFMvsKi6tJ+hpOpjabLpVI+q1UdeGoHDRFRtjxw7ZMjDwz7ZqXLLF2W/XKap1LilhNL1Cudui3B1nci8HljhplgkqTuYdAe6VDNZpQXDSrGKRJXS9R0eV6fYmvSrc8mS62Avec7yVMSaurNbinN3HUWc6rwkqTvbqeWQhsewK45acZOsttgsl1lvlmeXJj8t5rEKaagy0XSkQsjnIT8maOK5XWHR5cVtseISdXKXJXrpEEWNyDM6wst0tl7AIMbVAJHdG89HdTpv/5LM/POjucXh4iM1mw+v1fuwG8mfJl5+ITb0Rhvw2hnZcB1qAG/h3MX2Sf5Cjn/o2Csj9VLRCoUC1WsVmsw18jd1u95lfzkep1ru9n8RtU8jUmsQ9TlKV4YtqN3NPl8N+9jIGU77sjrLTKPHYlCvmPG7CTjvHzSoXbS52ipMD6qVYnFqnjQ6GpJEdsquI18GGybauBMPcThmgq+o99sxNIIsksZsfyg67mQpyQBC3e3hYymKVJJyanfumSVG9o/KCLUFP7nLX1LLPOYI8LI+z28tKlCz5gU172OKk24ad3vAZrLjCA4Z9UJ9u9qNp0DM7lKjqcBaJKX52Wsb5fVYH9wrDlcM1f4wHFQOcX/THafU67DVqlMwshcfVHDeCS9yvHgxczOy6BasuUW+2+ZHgRW6XD00wNkw8OzqAQNNkGm0mumb3Q9fHf37JneDd7LjeHXZOB+SI4kLSLVx2LLJRKHOvPO5hcVgvM+f0cNKYnJibvS5XA1EeFCfZuQ6c8wTGADnh8JCweWhU24hSj436qMY8vIclj49Cc/K9a6kqFwIBtoqTWSwHlTJeRaEypcfl+2Z+e7+7R39F2zeQ7+cUe72zO2U/aXxSi0Lg1AJQ1/V+7vEgTJOhL34/L+pZwu/3c3h4SKvVwmKxjKWi9ZPHn2a58lEand7eTzIf8vAwk8dvU0hhgJEiS2ynjRfZ7xwuYavJFsuLAW43zJcWHb3a4gWLn3ZnkvmE7A4kIdgsGMe6FArxftIYkM9FQuxVjUF/wRfkbtr4+ZLXi0WWqJhLzqvBCPeOh2BabXVZdQbxWu0cVmu87Jvj5vF4zun6cYWlxJDRuiU7o0rWcxY3naJAkf0EozUiNg971SrVEV+GJaefu4UhUOXbDZa9fo5a45tPXslJYcp62zGy9D9nj3DT9HSwSTKZdo2ozUXU7iTfqdFUtQEY96NVVxENJzVdpYoOA/P5NlDiObebqt4CAfpIRoemQ8LhI9WaPoFUu8NrnXcEeFyqTLDIx5UsFiRkIVh1R7HqNg5KFfYrDfZJsuLwUutO1xjmXd6pgAxgn1J9N7huTeeqP4YDhWSxxslIStqL0RjUpydPyWeMlYDdAUwCsqbrLHn9PDDL3OfcHuadHtoNlZ2jIslKlYTXAxgassfjIRaLjeUUt1ot4vE4iUTiiYzApsVZHeM/EQx5NIQQAeCHgKyu67cwGPOffb8v7ElC13W+9a1v8fd///f87d/+LW+88Qa/+Iu/yBe+8IWpqWhPE0/T6HQ0Ks02W+kCL5wzym+1kUH5XDTMBwfGy3o8IkOcJCvE3cMlvt/jZuutLAGnA294crAterwcVoZA2B6ROnwuG+vpLBYh6HR7g/MHXQ66IyV2tcZkF4yQcHO/lGLR5uFxrjSRFLXiDlFM1vH5bVg0wf3SMOvjeiAOTZntbo1yrc11V4IPGgcT/f9sQuG0hX3I6hkAslOy8rw3we3s9FZGqnkLYcXN3cLw/Ff9MYSks1FJU+oKmqpG/pSB+iU5wO1ikXMeD7UZxSOpdhskMcF4AaJ270xATjbLIIHf6qTShLo6fL4CWHIGCVnd0IZ382lulScBzW1VxvK8R2MWUAPs18bvZTQl7eCkSqXVnuoDvV+ZvS10Up5daNWcQVTssoWI3clr4TlSuRrJoxrZkUyM72zv85PXjfS30wbyoznFqVSK999/H0mSmJubIxaLPRXj7Xa7n1iGPDYNCiHcGHabvwD8CyHEvw2owOenfPaJ4sMMhvrxl3/5lwghuHXr1lMd/7vf/S6f+cxn+Lmf+zl+/dd/nZ/92Z8lGAx+5B3cZ5Us7h4Y+nHdHECFxlCrVE3gjHvsZCvDF/VCJICnM7QbdPas9Ho6CwEPR3tV3COz/bVwBIskkTF3ukMOB4/zhtSxFg3RNHuvvRSJc1g1wMMpSxxWyzwyPSpWfYFB8clo6FWBmhYoqo1ie1xjtUkSeyd5Dgt1LFkn55yRAay+EpzjoFEin+5SNo33ta5g1TmeYnTVG2djyuZkw3zOVzxx7LKNekeb6Y+QM3Nd40qAjrmh5ZftVNU6d0tHeGUb1VZnAoyvexd4UDW077Dt9EathhA9hOhSF82ZsoSYAtL9qKptYjYfTuEl1aoTsDp40TfPNfciLtXL40yNt45TVDvqVP0VIDvDGhNgs5zHa52e6dNUu1xV/LziXSDW9XF0XOe9nTSPUnlKrTbn/YGpnyu1WizOkAiSzQauGaXdu6XiADgWPF5ejcxxzR1FLsPBcZk7WymSpcnMkX+zPbSeLTdanFQn2bnVamVxcZHXX3+d559/nmazyTvvvMO9e/fI5XJPlN56FiB/0hhyEPh54HmMXORfBn4c+HeA/+ZpD943GPq7v/u7gYvTF77whQk/i2q1yle+8hVef/31pzq+EILf/M3fBODg4OBjLZ9+Vsni9n4SIeCwWMFjU8g1jEHmtEhspQzgjHo9pMtDwLMg8Xgzz+rLfjKtBrWs8btOt4eq6qwobu51i7isVmrtLtv1Ibs67/Nzu25IAJqk86iQZ9Ht5V56yDDnHQ5cPhfZrAFSHnlyI23J4+VuKkMk4GK7Ovkc11xBHlSN62+24YM7Za7MJwhHHDTqXS515mi5euy3KrgsFh7l81x2DDuIW4VEujm56WMREuiC1/3LvFXYY8UZ4v3CdC8KgONGldWQf8CONU2npcJRq80lW5jjbo3KqfSza84476ROkK06QuhsNZNYrKppqqSjawJVldA1K5Kl//PJOCxPz5cGA+QTcoJqW+XT3gj/cLJDqjiZPbLfqMy0wEw2qiy6vIMS9NFQdY15xU2lW0ASgmV3gJDFRbnWZjtXRHPL3MlM7xYSsjvYmSIxAMScbg4r01n/eb+fDwrj92yXLaz4AgRlB1upAunDGumRrtMHxTJRt5NMbfK7fmv3kF/5+je5c5CkWG/xY2sr/N75pannBkNvvnDhAqurq5TLZY6Pjwd68/z8PB6PZ+rnzpIsftDjtFCUx0h903Rd/38BPxBmirTxJDFqMKQoysBg6HT8xm/8Bl/60peeWTMCCAaDH2v59LNIFr1ej3e3D4m6HTQ6XQLKkGFcikUGy/fMyCbfYtDLYzPrwlu1cM7nY2e3gFWW2E0bwFg7NkD9ciDMgsszYJQAOZMpPx8L41AstHs9bJKFrskeJQHZbnugcQbtDtbT40AhwNACZI2TzqROGbQobGSHIH3JH6SjajzeL7G7VeXOeo69TIW+5HopGKLd6/HBURGvmUVxxR0j3TIG7pzDy8u+ea44EsgNG/ePi/zjdooForiF88w+Ybqu4+66UHXDglRRFWq9LgtuP5vNNsuOcXvJNWuY93I5JKuOkHSsikZHtJEkHV2DbttCt2NFN+1EF+2zndJyWgvnKWvLRUeAq+4lUkWVN5PHNNoaN9MnyGI6u6yqHZYdszeu4s7pIBO0OfBJNi7ixVm1sXtY5dZuis1sEU1nsFqYFme5sXWnGAL1w2Km5Cx5fbwamWNNCUBB49FWjkqhRqY8XX9e8Jt2nDqGCYYKogu9tsa3H+5SNIt0HqYnV0vTol8V+/zzz/PpT3+aQCDA1tYWb731Fru7u7RO5T7/s5EsRsqj/0YI8d8Dy8B/BfxPz3LwaQZDx8fjZil37tzh8PCQH//xH3+WUwzi43Z8exLJQtM0yuUye3t73L59m+++c5PdfJmQ6V8R9A4HV71pSAnnQj7SIy+yf6SJ5PZmnnjXQa+nsxIN0u4a58/lm3wmukBru0FruzHI9zzn9XJQNozRy2qbYrvFj8QW2B7ZAb8SCSMEbJq5wituP6o2Dnlrbg/7lSpej22q1hgRdrq94W9yJWMSiHvcnFRqnAv4SNfqpBrGfdXMXfZuT2PVHsEnFFLNOq/7lgj1fBxmmrx7lOF+Jjc2ubiw02jMBpaE7CCMk81ynXnFi9YVtDSNV4LzrBfzNHpd7ufyxBUD8K57F7lXKiFZNYQwdOFuV6LblWg3TSA+VYpdb/ewzABTHVhyGzLMRVeERRHmca7Bu+k0AsGKK8wHxQx1tcNz/tkVYS7LbPZWMqUii5C47Ivwmn+RFSlMKd1lM1tju9SkoU6CaLLVnGBX/RiVGE7HNHbsslp5MRzDpkpEO3aSB1XubqbYzpTpma9OuT2lEzeADluZPEIFoYLUM51fp/zpQbFCfgqTPiv6evP169d55ZVXkGWZ999/n1u3bnFycoKqqv+8NvUwikLWMbTjX8Yo9P/Xz3LwDzMY0jSNX/7lX/5IFXr9+Lgd36blIfeLSQqFwqA9VL+Y5OrVq7y1c4Kuf4Bsdveomy9txO1kx6y0C7ocHGQNJh/xOHl0OGSrTsVCJW0MSKd1/KtpPm5QyTc5aXd48VNx3i2miDhdHJSqXItHKffaBKsKlVSD+ZCH45rBdLuaRlhRyLc6WIRg8xQriTld7NTreCUry4qf7KlS3ed8IbZOhgB/IRBg78h4zgmvm2S9RsjlMLpm1GskXG42RyaEbK7DebcPteGg2dPJzMhVFQhKzTaH1Qorc372GiXD2rIhg6Kz6vbjtzkRwsh4kCQ40ptc8Ue5lR1mg7R6Kvael+teL+8kkyYY978/Qa9qQThmc/BMq86r0XneH2lc6rHY8CsOnLKNiOKlaoF7meE9KkIiIblZH5EoHPJs0E1PyQsGIyXNb3HwqnuBh+k8G8XxlLSiqflOA9Fmr8eiy8XhlKyJerfLij/ATmlStii0mix6vMhCImpzUat12EkXWM8a9xJ2TM8TP6rUCDjtBtvVAR2EZgBvTe2MA/AsjQa4fZjkRy+vTv/lh4SiKCwtLbG0tESj0eDk5IR33nkHXdeJx+NjXjSflJgAZF3X/0MAIcSLGD4W27quz5gOz44PMxiqVqs8ePCAz33ucwCkUim+8IUv8I1vfIMbN2481bk+bse3PkNutVqDYpJqtYrT6SQYDHLhwoWJjiS39w1gyNTqSAKOigYoLvq95E2vglRxqLcteL3k80OAurwY5e5BauLvwm47u0cFnluK8OgwS+5RGSUhsVcsIwmDTS3X3BylKyQbNdYCUY6pEnfY2Sjm8ZnSyZovyHp6fFCGbHZypRrLSoDUURURGI4dSQi6zfFJySOGG0v9pXCqVmfO7yFXajLvdpMc8WlQ24I7+RqXPTbsZzDDy+4A6yaQixoggd6SES0LLgW2a2UwswnsssxFf4A5h497hclsDI9sp1PTkSzaoBBG74FWshoHdnQ4qwmwRbPygusch/Uy+WadPBp56kCdTblCyDbcgLVJMiuu8BgYA2wVcgO3uNOR6TRYdPnItupc8IRw6ArJ0jAl7cVojEZ3+uos5nTN1Hz9im0qIMOkjuxRFFa9ASw9Cbtu4Z29Y46ZPG7M7SLXPLXZqIOuQa3WRvTOxFvAyHnv9aavfG4fnDwzII/GqN589+5dKpUKb7755phnMnwCGbIQIoHBjOMYyYaLQoi/A/7kaQ18PsxgyOfzkcsNGdvnPvc5fv/3f/+pwRg+Pobc7XYplUrk83lKpRKPHj0iEAiwuLj4ocUkd/aTeOwKyXKN8yE/+znjenJlA3SXQj4OTXbssSlsHA/vPeJ1oWoaak8j4XeTzg9BLeRQKNHEZgJrqdDkh68sku21uOYO8fDfHMOSn4DHSrIBGxtpIqs2Qk4H591+irUqLUuTVmf8+7seiXEnmcLelfC77Oyny6wuBdiqGgP3pWCM9w+GgOdWFDaSxjUrksROociCz2BskaATdDg4BRYxl4tOt8NGpsCK8DMtJF2QaQxZ43alwYVYiP1iAxWNeVxsjOQ7vxqZ414+Q7k7mZVw1R/l9kkaydVFWA3PCb0r0KsjGQqamNw9MeOKL8rbxyf4bQ48TiunYbHVU3FarAgMMF6eAsYApV6HBZuLo1Nm8XM2FxGLk5DiJZNqsZ6f9CixnJEDfJZWrJ2RWdTsqaz4AoQVJ9Vqi510kQ8yxnVfic1uF9Xu9YYMWDeep9mJi95IPsqZoKzp2C0yrSlSy53D6d2snzWEEMiyzPLyMi6Xi2w2y+bmJu12m7m5OS5d+sGucRsA8kj59L8EPMDvAMfAVeC/xajie7qDP4HB0McVz8qQp9lqBgIBYrEYlUqFl1566YmOU2622EoXeG4+TDmdI+Cwsw9E7QpHBQOkQi7HAJAvRkPc3R6+jBGfk5app8Z9Q0CWJUEqawzqamOYPdA5bnO0n0P2O+l2NI7yFRweg4HqumDF4qfX0JHrgpO9BhciXqqJ4WD22+zsFIv4LAqdTJekMM4X0h1sUcRpsXKQHQfX5/xB3jer9FbDQdZzOaIeJ4fVCsV2i7VQiEf54a68x6qwns+xorgo0OE4N87A+/FSKMbt9DBDYN7lRqvLA2/eXLODsIMu4BVfgt1KeSoYn3f72c6WwdYFq4aug1aXoT3OOyRNQpMmgW3R6WOvUKGn6+RbDfxKkDXJh8fvRUOjq/VoaSp1tcMPRZYotdpTwbgfc94A5XKXRcVDr6GSqXdIlTqk6PB82Dqmy49Gqj7bbGj/jPzg3VIJWYjB5rHPZmPR4aZb75A5LFNtqxzqk2NkM5fHYbXQHGXl5obcdqY8rgGfQTDDbgf52qQc09N15uxWDmuTgPw4nafW7uC2PX0j01nRz3EezW/udDr/5E0sniRG39T+o34B+LKu6xtCCIuu628LIWrAuWc5wYcZDI3GP/zDPzzLKQBDT3qSNDVd16lWqwMdWFVVfD7fVFvNp1ne3N1PoQN2xfh83/TFp1jJmRVnKTM3U7FI7CaHzGg1HuQgWx5IAI3WUCG6GA+ytZ3DIgkOzUwHiyTYPM7jcyic5OokIh4kn5Xtwoiu2ZD44DAz6KEWVOyITAfZaQzYcx4v9zMZ1uQANU+HjJkTmjqqgh+e94a5fTieRpUvjZSAm/eZbTSwW2X2K2Wuhcd7xJ33uNkql9k1vTzaPY05p5vjES8MmySzXx4C/4JZ3XUzOZysCt0uz4UjVNQmak2QcHk4OrW8Dtkc1GoqTamNZNfQNdAqVuhNss05i58jxpmpX7HTamrUul3zGUvIbYlGB7NzyHikci2uxSZ74g1S0qxOREuimdN5pE9KAVuF/Bh4jsZxtUrY4ZjwmAYot9uc9/nZmwLMrZ7Kjdgc9KBcabGbKbKhD9+J1YCP7eIkIHd7GhdCQR6mcoh+ZgQzsPeMRXLC65kKyADRYIDD2uRzdFgtrB+neW3l6TsFzYpp7ZsURSEen9Ir8QcsRt/W/vN/gOHwNvqzdzBS4j5x0d+IOzo64v79+9y8eZOjoyPsdjtXrlzh1Vdf5dKlS0QikY+UKtPXj2vmRl6qUkMCMjWDyS0GvQNAvhKPUDbzkwVGN+LleAC1p+FQLOyN6Lwd8+9CHoWuaoyUS0sROqrGSiyApkMg6MLtHjIMqyyxlylxMR6i1jILVMpNjvZL3PDGuRqKcC+d4bI7RLPaZS4wzAbJFRu8HIxz73hcmz3v8Qw0cYBktUbc4+agVGEp6EORZB7lhxKMBBzX6jznCzJa/R05VZRxLRAlbxZ7LLq9JE6BcT/smpVzUogHmSx76QryyGRply14cJBr1hFWDb0tGXrxFDAGOCpX8FiGz8smyYRk91ie9EveONv5Eo4Z2oaqa9xNpbgRnCdkc/BKcI7rnjm8LSe7RxVu7aa4dXSCW5m+KdbWNObs0w18ABa80zs5A4SdQw07YLfzvC/I844AwZYduS64s5lkJ13kNNb7XM7xH2hAz8iGeHScMzIitDNJMAJwzWCzB4XZK9Rqa7i6W/B5eM7hYrVnR97rcvfONp0p/hfPGt8Pa8/vVYx2ne7Ty/8T+MCUMPpU7f8Gnryf9z9h6LpOu90mlUqxvr7OzZs32d42DC5XVlZ47bXXuHLlCvF4HJttttPY6PGeJG7vJVEsEnpP40YsxorFw2d9CZxmtVPYbQwGSUC6MNQVr56LcZAt0zH1tQW/a7CU9TmsHKYMEI9FhtVW/bS1AWO1CHZGdv2vzEcoNVqDARlwKBznDTDdeS+Nt2bhWjeAVOiRrtTRxfg9+trKRGrcaHPVuMdFsloj5jYBpddhXlFojWSlXI1GKXc7pMvjWRVWbQhwLouVxyarX3QbXhuP89PnfasuU24aGSiFRovLXkP3lBBcdIbZK5VBF+hVCyJvRWizdVgdwZLLeJ4CWHPHxjJDrvjC3Dkyu4/0Zn//OiCpMpaalTu7Gd4/ylBptcd+f943XTcHiPtn/06asTqThcAmZF4Lz3NJCVJLtXm8X+Jxsky52R7kpU+Lg0IJNIyNuC6ggaTNSEs7A5WXQ9Ovu9JqsxSczLF2WC04ZQuvhxMsNR2UPqhwvFkjnWyiaTr5SpP33nuPO3fukE6n0Z6hjdTE5f+Ab97NisFbK4TwCCECuq6/r+v6uq7rmhAiKIT4BeB/BH5gXZ0rlQp//dd/Ta1W48aNG3z961+n1WqxsLDAa6+9xrVr11hYWMDlcj3VF/WkFpwnyRKWjSY3al5Kf5/Bc9Bj+x+O0LJdpEdNzgd9pEsGCF9JREibTNlmlTnIlnBYZTaTBhDJIyNhJRoaTAh9a06/287mSZ7FkJdjU+O12OQBA5GEkaHhddgGx4yOGBkthP2s30/hFFZ8LgNQk+WhhBB0OUhujTMdl8XC9khhiM9iTDIZsxRZU6xIp4p6Gt0uV0LhiYqt+oiHxmVfhEqnzaLHS73XQdHlqU5hy3YnjU6XjZENMKltvLrX/QnuZ7JGJ2xdgA5SV2bBebZrmKIZ9/BKYGGsws0nWzlKV+kjUrarIs1Ap9ci89zaT7Lomc1mz+osXWnPLpXeLw7vNWh38Eokwcu+ON6mjfubaR7spNnNlDiNnHuFEmHXCPMeYcH5cnOMBZ81ElYDs+/JZpl9T2G3C4A5r4dX4wledISxH/bYejdFardEIT85YbR6Ep/61Ke4cOECxWKRN998k4cPH1Iulz/WTkCfBJAepRH/KfBVACHEohDi3xNYlyEAACAASURBVMNo57QGfBu4/6wn+TA/i69+9atcu3aNl156ic985jOsr68/8bGPj4/5sR/7Md544w3C4TB//ud/zhe/+EXOnz+P1+v9SF/CkwLy1/7iFrmjCmpPR5YEh8clhIDjZIl2o4f/UGXB6iToclCtDquKFnx2yo0252P+gZaYq5pShoBUeqg9HueNfy/F/WiaTtgsPnE6rAPfDIArc1Ey5TrLEUMCAWi2hvfgdRmrAmERVNodYj4X2RFj+2W/j3S2xvmRJfOCYqc7whQ1WSbicnDSaCIJQ7vcLAwBZMXnZ6tYpNOafHYn+RoCI0VrPZdlyeOlqnZYdQd5XJzMOIg4HGTbnTGvaICHJ3leD87zXjI91DWFjlQzXungjHb2/cgWq6zK7jF5RBKChNVPfSQboNHtsuSeBPcboTlu7RspimdhRrI+u9fjTqmIbcrS2iIk/Iqda3Yf86qdaqrFvc009/cz1Fodml2Vi9EZFYW60dGlz4JFH4CnseAzrjt3RsFGvj6pE9ssMleiEQK6wkrbTWW9wsNbJ2xvZgdSm881/TvJFIyJ3ePxsLa2xqc//WnC4TC7u7u8/fbbU6vxZsXH3crt+x2jgHwTsAkh/gfgD4D/GWjouv6ruq7/L7qu7z7LCfp+Ft/85jdZX1/na1/72gTgfvGLX+T+/fvcvXuXL33pS/zKr/zKEx9/fn6et956i9/93d9ldXX1Y9WinqRa7/C4yHfe2CIacbOzl+PiapRypcnSYpBS2XhxLbrO9j8eEtpp4jBXYyGPg8OS8ZJp5qSxFPJRMDdFLsRDZIsGq/a7rFTqBlAXay0k4DBlMNa5mI/HI/36KqbmXDZNjfxOOyfFoUSSrzawWWVSlRq72SIx31DTVWSJvQMDFF314bKx2uiN/025wqJZHrsY8BF2usbGttdmY9nnZys7WYhQ73SJO92suoNE7W4STg8veqPcTk/6V0gIAoqDmNXGzqkd8oTLg94yX98eBhi3BJL5StvF2fsBmgrl7qk0QG+MzdzkNYcV19h/vxqeG9vwPDgju6e/QTctuprGit/w7w47nLwSSXDdG8fdsHJwWMVpcZOpTc+Zto/ud+gMWLBQIV2sPRELFsBqZHpDh0qnS9gxWyt22xRiHhc3EgleckVwn+gc3M6Q3a+QyU6fhJz26d9JujieVSJJEpFIhJdeeokbN24MqvHee+89ksnkmSTpLC/kTxRD1nX9TeDfx6jSawF/BXSFEP+dEOI/MG05nzqexM9i1JS6Xq8/84MLBALfdz+LP/uLW2i6TiTiQdehOZAOjM/ZbRKHRwa7nYuFyd0q8NJclETAQ0ft4bJZB6ZDIfdw4NpHWnv4nMbASIQ9HGTKXJgLUqwawO3y2Qaa81oizGG+TMLvHpjfnwv6Bgwu4nNylKuwuhAiHnDT03W0kU4Tiy77ILWuVTB+vuoPkKkP2clqOEin1xtICwGXjePqkMkHHQ4e5LL4pxgY9eOcw4elJ6HVdVoVlZu7SVY9k6/X9UiM7WJxjLGCUdXnlKxsHBVw9wefDlJnyDZ7Z2i/AF7FTVQergIuuv3cPZ6ewjZq9/BqeJ73Dk5lnzSbzJ3R6mvRN7n8t0gSl4NhFmxuVix+ykmDBT84yAwqPBud2eXJ6yfpAQuWVAZ5wtNGjpgtp+N3zPaP8dvGMxVkSbDotHPZ4WEND/WHNR69e8LWRoa22b+vVJvNZC0zNtry5frMVlRWq5WlpSVef/111tbWqFarvP322zx48IBisTjBiKdlWHyS4rSXharr+v+h6/p/BPzXGJkVGvAjwH/+LCd4Ej8LgD/+4z9mdXWVL33pS3zlK195llN97H4WH+b4tn9Y4LtvbaMoMls7GYIBG/uHRRBQLBlMdT7uRTWXbKlUBV3TaT4soajG0FlJBAfSQp8Fexw2NvdHSpzNURbxG4N+1JC87x8BDKqhRllvpz1Ek7mQMfG1e13qpmfC/ggj1NXhi5zNNTjv9eGTx1mS3Woh4LAPUuwcNiupkeqwZZ8Pr6Kwnpydn6uXdXpNnYDDzuN8ga6mjVUAAqwFQtzNZHgxEiNzSmu97PWxlS9S73RxYnxOro+jTm5Kelc/XgzHeJwt8CiV50ZwjoDNTqmiMotPnlQMBmeA8XQnulmmQMYNG/8Xc7q47PJx1RXGWbOwtVNg66TIYW565d1WroC73yD3FAvutLWxjIizKMxaZLavRrM7uwi3p+mEXU5eice57o4QyEoUHzfZf1ymW5++Ei1Vm8jSrK4q0yfJnqaTK02vMBwNl8vFpUuX+PSnP00ikeDw8JA333yTra0tmmaK4D8bhtwPYYSs63pO1/X/Xdf1/xLDXOgbz3KCD/Oz6Mcv/dIvsb29ze/93u/x27/9289yqo/dz2KaZKHrOpVKhb29Pf7V//p3aLrO/JyHVrtHPGqwvKWFAKVKCwEU8mZK17yfnFnsEQ152Pv/DnglEaNjMnCvQ2HfZLWr0cAAxAEqjS4IQ0e2WWV2THc4zQL7eQN4liOBAdPu680um5WdkXznQqWCzSLYz5c5rjSIeV1UOsb9XYgEOUqPg1hCuNg4Gc96OK5UOR/0D4zvR83KFUliq1hkxRuYyNLox0uxGBqCrqaxkSsMXOke5wo4zYHkt9nJ1g2znGR1fDkbc7rYNTdFNTQyrQZSWyBOmQQ1Z0CUXbaQHpFwdpNllq2BMd/q05Fp1HnZFZ4JxsDgPkZDkSQuB0I4NAvnhJfCSZOt4yobxwWa5nM/KlWY805h1zqoqkav2xuw4JlaMMbPZ8VuvohlBkju5UtjKYSyEFwKh3gtmsBW1Glt1Nm4lWTzUYZWc/hde6ZdM0ZDhqDXOfV37Ynu5MNIF2YXw5wOIQShUIgXXniB119/HYfDwYMHD7h58ybJZPITm/IGUwBZN6IHRvWemf62p+v6U1fqwYf7WZyOn/qpn+Kv/uqvnuVU3zODoWazyfHxMffv3+fdd9/l6OiIXL7Nw8cG2LXaOk6nlZ09g9V6PcYy8MJKhJLpe+zzmWlvkuDw0GCXufcyeHqmc1t4CHLFwnBDxeO0Uah3WEkEyZUbXEyEaJpNS22+IavsmxFdiAXJmvaecZdtwL4VWZAqt7iwFGExGqDZVYkHhoPKOdGsFKwVnfaIXDDndZOu1WmYrCrscmAdaY56JRKh2VXZzUzqsADzbjflRgtN0tjIFwaVeGAA+2VfBKHDvMtDodXkpWic9ClvhoDVPkiv0xVj00pSJ9fkhUYT+5SB+UIwOmhjH3I4iFpcKOrZevMrkThTCgPHYq9UQkKQcLm5EUnwgjuKUpHZ2i3y7uYJCrNBIuEz2bXJgukOndLand6TlScDS/7pLL2t9lgOT9eKG50uVxIRrsfjvOKLESpYOLmbY/32Can0jPYlMNObAkCZcau1054YI5EpPjkgj4bFYmF+fp5XX32Va9eu0Wq1SKVS3Lt3j3w+/4nb5DuzyZyu69pIA9RnilE/i06nw5/92Z9NlExvbm4O/v03f/M3XLx48ZnO9XFpyN1ul0wmQzabZWdnh83NTTRNY2VlhVdffZUrV67wr7+9i67D+XNBTlJlVs6FabVVEHCSMq5BH9Ex02bGxIXlCJWK8aIvLQTZ+fsDXo1GByPtfNTPcWZ4D/MRQ2Zw2A05od91RAfapk4953fz8MiQCLSRNu3KiNywGPHS7Wl0dA2neax+ZkfE7eTx/qTE0Eg1xzpKxLxuPDZlkFHht9rZuJ1i1WvkpRaaTV4IRyhNGXgWAVZdEAu4+SCbHwPjflQabW7E51jP5bDLMjuFcWC/6HCxkTPOraGhW3Uc6nS9UAdi9nEWF3e6uH9iFLys+gNIbdjJFcdS8U7Hq7EEt5Mp1otFroYmPR9ssszzoQjPeUK87I6RO27w/maah0e5scnMY50C+mZV3IPD9IAFSx+yGedUZk8eEe9s2cRrH2r6khCshgK8FktwWfLjzgs2byXZ+CBFY0SKUDWdgGe6xtycZb8JeGdkUxQrswE+PaWz+tOGw+EgFAqxsrLC4uIiqVSKN998k8ePH3+kvanvZ3zPXZyfxM/ij/7oj/jWt76F1WolEAjwp3/6p890rmcF5L6vcb+cuu9nEQwGB0A8Gls7Wd5+10g6sZkbH5mcMcMvzQc4OC4SDbvZ2TVALhZ1k0mbDGBkxs6bEkbynRTxaxHWEmFssszRyLkUq4xFEuwki/hddraPDAmhZ2UAajazS51VEqRqxoCyyhIHmaE2KUkCt0NhK1UgEDTY+nHJdKPzenmQHE9zSgTcHOwUubg4x6FZptzodlkNBbhjdrHuVDtoGliONZ4/H8atWpCy0xnJlWAIUHnn5JjTakbM5WLe7UFVNbS6xnPeAIokeDCSdxxUbKRGSspdbitRu4v9GV2rAeynVsgxxU221+R6NMaj4xwdk+Vt5wr4/DbK/cnMnCt+eH6BvWIJj6JQ63RIVWt4rFa8Njtxh4tapclBvsJjszXWywuzS3NzffOkfmnyyCZct6c9sV3lpWiI94+mdwZpztoEBPLFEmt+P05dIXlYJn1YoH+U0KXZK9aAx0mxOinnlM/YvLPNqOSrtzo4bVYaU8D8WRny6eh7IQcCAQKBwKCz9cbGBjdu3PiB3/D7vtjqf5ifxR/+4R9+LOd50k09Xdep1+sUCgUKhQLdbhev10swGBzzs+j//nR87S/eNc7nc7C1k+XCcpitXVOu8NrhGKIhN9m0AXgel5WM+ffbOwZILy0GOdg3jr18Kcr9+ynWViJ0Tm3IVxpt5sMu9nJ1lsMeHpWMgd0z3/mgy86RmT733FyEB6Y728VokI294cZgqlTn/HyQbKPBYblKwu/mpFrDbrWwdzh5jwm3myIVyvs18IIiCXYKJVZHKgYLpulRKlXlsjuC1u3x4CjH+TUfeyMT4wvRKLIGsrByzgsn1SoJmw2nRaHUUUmV6mTNTR0JuOT1YLWPD+p5t4/7qQwCsFok2q0e+pnrO3COlEdf9PhZT+e4EUtwZ39cC+7pOhc8Ad7Lp6AH1iqoTnhn4whhQqUiGeUhL0VivL13TIZJANnPTqkyNJ3SUuUGo35GupjNgm2yPKjcPB2HhdkT0G6+OOaPcT7gJ6o4qOdblI8aHFWnS0nqjHMBOO3TwTVfaSCZ3tSnQztDJnBYBY0pysXHBcjdbhfnSGm5LMskEgkSicQPPBjD9wmQv19xFkNut9sDgK3VarhcLoLBIGtrazNbR03b1NvcznDzPaNZ48J8gPsPTxAjdompdAWbIrO7NxychYIBogtzfj4wfZFdZiqbbJE4TJVZW4myd/eE8zeGbMUqCw7TJaJ+Y7nZz2tG17E2Yc3iwxvx8kbV4NTqiK5nHel6sRj2cFio0lRVIn4Xh9UqUZ+Lk2qNtWiI9UfjjEsSgqSpc2eOK8zNebDJ0BUSHTO9ySoEWlNDQhANuHl4kGMpZMwmEdnBnmmXGXW5qDXazMku3t07IeRycH0xzs3kCadbLC/7vPQ02ChWkITgwlyQrXwBl8XK/UwGLAa+dTQNqQWRkHPC7nPs+5OMASgLgdrscc5inwDjfqim4YbUBrkr8GoKFYbL956mU2q0OC7PXlrn210iDhvZRtsoUe7bVk774zOkTbWn4bEpVNuTmQyFRpMFv5ej0uR9S0LwQ4vzdGoqyYMSuaMiuREPZLsi0+pMgm91GkKaYZGnz3qaphPxuchOaePUOkPOCAd95GuT8tjHIVnA7CyLT4JcAf/MAHmUIauqSqlUolAoUC6XB8uYpaWlD/U17se0Sr3/6/8x2LEsCQ5PioSDrgHrXZz3c3hS4vlLcdYfGhVgiZiHVKqKQCeTMV46u93Czo7BXi9ejtNDZ+/OCVpP4+RhBpGQ0XWI+ByUml1SlQ4Jv4tU1rTkbOroDjh+mKNdbHH1SoTDcmVYfi1gPzUciF6XDVejyXamwFLCYLj9zsel/KSudyEe5GB9OGgSwkFNtFhquUkWq1h9EkpXIMxjJMIevB7boDfg7nYeW0RG7WkEFRtKFR7XjN/5bRbub5wQjdnJNIfdUc673TzKlQY4pek6xXwdJKhr3eFuh2aApoRA+5Bc434e7wv+GO6OlXdyJzP/ditbwOYQ2LoyHXos+Xw8yE4Cx2GpQtCmUBgFyxG/4Hy1PfALPuvqBOC1K1Rak6Cr6TpLAS8fpKb3nIt6XQNAXvR7cXU1LB0rh/tFNKHycHu6x3DI5xqU249GfkqH6NFrmRU+j30qII/axJ4O2xQN3G6VkTqG/0w0GkU6ww/6w+KfVR7yJzlUVeX+/fuUy2U++9nP8s4771AqlQiHw7z88su89NJLnDt3Do/H88Sz5WmGvPs4xf0HRg71hQtRSuUm8Zh38NL6fMZmRmnEptJvZlecPxcma1YwLZ+P0G6rCCGQhMbu7WM0k922Kh3OhQ2mGQ76WJ0P4bZZiI+kEsltDbkDCwt+stk6ye+meCEepWeuH1ejIarN4aAo1lvEg06cNiu7ZvXccbHCpWiIkykD1HHKmKdx0sJeEuzvZUmfVLlsC9CtGc/FabOycZJjdOu30epyJRDm5UgUXw26jRblZptFv5vtfJWeBn7djiwEV2NRXLLCw1wJTTOxTYAmdIrqCHPrmey1ZRjaAyTPYKsAuXoTr6yQPqyNdWCZFqqmcd2VGKwAHPJsrrIQ8E7kBfc9IkZR+MPestUZmQ8AygxQcVgtuCXr/8/emwdJdld3vp978+a+71tl7VXd1Zu61VqQhJE0GPOEsWAwDDwzjGcMHsJgJogwi4mZCd7EmyGG98YGxjbxPGET9jwcQbCMHYwAY4SxBRKSulu9d3Xta+77vme+P/JWLlWZqZYs8SxGJ0IR6sq6mTdv3fv9nd/3fM/3cK/dw2RZS/pGlv2VPNtbKZpy6/6oMOqG7wQLlQYqaTgUVGqjdfha1fBzTOVGt10fmCb57CbOT3o4pTWj2awQei7B/l6s62ORG7PzGRfjdMivhfiZAPKLeVn8/u//PidOnODMmTO8+c1vZmdn5yW9/w9/+EPOnz/PV77yFVqtFl/72td46KGHmJ+fx2azvWxd4mFA/vG3LjIjtFGpFFRrDZSSOMC/RmN5ZiZthMI92iSZ7GQQGnXvJknL2zOPQ8PqpRDtQ0ScXZ7UXK7WyN1Mol4poch2sj2h0fldqdzCbJFBut0mdS3Z9bfQ9z3MdpOWUKpAqdFgymWl2W7jtxlJlyqom0cfXq1KYntzMDNTN0WUJYl0sXMOW7cSKORnbmHCjsdi6GbnB1EP59m6EqNebaPQaxAARR/IbUcyPOKb5GYo1nEoa4OiKoOaCLShpQKp0QFhRRXEptDldAHihRJu3XDNK0CqWEaZFEkVy+ylc8yOcFebMBrwqdRs9HXqxQ43lsjFOKEBN/YSPUXEi6iqfObRHXyVMU0ZiUIv8/SaDJx3ezittqHeaxJdSbN8NTzUqKcxprNUPUqPBjgsw88zlR29kClG0BnVegOTfhD8lZKC4xNOLG0JXw4Kl+KsPbXD7nKsm4xIkoUHH3wQu93OxsYGP/3pT9ne3n5JdgijMuTXCmXxqgPynXhZnDt3josXL3Lt2jXe/e5386lPfeolfcYjjzzClStX+JM/+RNUKhVer/cVOXdRFAcoi+e+f4Pd5zZZ0kqYqnXu8Vloyg0cE14L8WQBdZ+8ye81E08UUKlE1tc7xTa7VUskUqAttIlFS0fAGKCwlwMBSsky6VCBgN9EYrkDeFKxRUspoKi0Scg2notzLsL7GSYqKgRgv08253OYsBg0BLMlmjJ6OAw6ZqxmtvaPFvMW3PZuG+xBmI1aVH0LikarRNHsVKW2YkkahzrpFKJAoyKhQEGp1WItluJMwMN2oldwXXTb+cmNHe5zdP5WYr0DcFIZ7na4MYgSmhhMNo0IrUEg7g+fYbjUy6hUYUPdbUMGsKoGQUKtUHDe6yWSKmCWNOT7ssFIodShHuQR9vQ1ZxyJMc+6cYzF605yeAFaJSkwKlU84PIxU9WTv5Vn5VKYrY0kjUarO/V7WBxMNx8Wo64hdPTuwyJbHM0vj6MzrEYtNqOWs1MezlrtWIJ1Qj/ZJ3IrTiE1/D23dxKIoojL5epOlQa61pyxWOxFrTlfy17I8DMA5Dvxsnj00Ue7ldE3vOEN7O/vD3urkaFQKLor4Iu1O7+U6F9VN67vEw92tvtSqc7a965T3EmyINMUFqsOq1nL+kbP2F1SdIDN7dB0O+88HkvHJVISoT785opupvEa9WQ3skiSSDKYIx8v4bcZUZbl93EYu25weVnfuXsjxkMBH6l8jxeuN5r4XUYEATa7Zj9tZnJqJm1HPRbqhcFsRFIIxNIFVvc6WbNCErrdeQGbFrUksZceBIjTATeheMczw2rRoVMpu/MFoWMRWpXHBV3ZDHOfw4NH2zPwUTVEGqkmYkMgmiqgHabhlUPRPgoy59xubKjJHtJDr0aTXXe1WbMJEwou74TRKhTsZgp9dAnU2nRA+KA7bgzo2sb4QWwnM6hGAESp0WRKtrm0qJQcN5k4pbZiDLXZfSFOIVIiHj+aoVbrDSyG4Z85ji6ojRicCsO5XYB6o4XVMFxXfLh4Jwgw47ZyT8BDoKWmcSPD5t/vsHk5RFVeKGKx/MhrubM9uDNTqVRMT0/zwAMPMDc3RzKZ5JlnnuH27dvk86PpqmHZ8OsZshx36mVxEH/6p3/KY4899rI/75WePn3wh3z2r3vuo8lIFpVWye5qlPW/ucmMz0wxV8BTLKBS9i5pRe5iK8uTmxUKgZ2dlJzhjt5attttytt5xEqLhTkX+bSs+UzWOu2zzTZOVyczXJh1DlAkUrjWzdLVkoLNSJpivc4ptZZ5NPhMelSxFqVICesh/winSc/W1iD1MDfpwGLqGRhN+XuNIiq9DqfFNFC80qsk1rc6i1KNFmvxFMe9DjJ9rclnAh525ZbvY147AgKxXLF7fLLP1L7d7kznHhX7yV7x0qnXcZ/LSzJVGqqGKNbqzGi0LOj1bMdzJGWe/bjLRbHa6PK/QxszxlATuUq12yl5OGrNJrOOo8ZJkihyzGln3mBhrmGksVlj/1aW7Y1Ud4eiG8HRAqhHrFHZYqU7XutwjFNTjDLEh07xbljE0zm0KolTASfnXU58GZHkcxFWf7xDq9wces0ajSYWy/BsfHt7eBETOgZkS0tLPPjgg1itVlZXV3n22WfZ3d2lPob6ea3Fqw7Id+plAfDVr36Vixcv8slPviwfI+CVb5+Gznd47vsdQHYFbET3Unhn7NTkRgVhI0zqB8sImRoL9g4X5/OYiMXz+LwWkskOGM3PusgXqh0+ddzOSxRoZesIAmRjPWDJydNDxCbkZLF+o88fwGrRsnw1yEkZAGY8VuwGLer1KsayltALKRxbdSK34uxuJqhFB7OpSYvpyENUKBaJypV0q0lLVa6AB5xmcqUqy/uDo54W3Q4qtRYGjZJytYhahGt97mhGjarbWu2zGUlVKqxGe4vAmYCX3cTggmoaMQYJIF6uYpQkFvR6lOUW4VSe0Ihi33G7FTUaNvq8LOw6Ldf3Iy/KBSsEAdWI6n+z1WbaMXr6xwGw2vVa7paNeixxgeDlBOVYlWhkeAGrXB5tuOO0jf48u2l4RpvMjs6e62O0yPpDNpxem5Hzk178LQ3arTK7P95n7fl98unezqw2phio1w9faHZ3Uy/a6nwwuPT8+fOcO3eOZrPJhQsXuHLlCvF4fOTxr2fIctypl8WTTz7Jf/pP/4lvf/vbdzRaaVS80o5voiiydm2HyE4HNFQG+ZL1/YEtOh00W+yvRth58iZGgxq7rQPMVnPv4Wg0WiCCIp7H5RxeRGm32x06A5ifcxHd73yXltjRfrYBvUbFXjDN1ISV7Z0emPmcRprNFolrcdRKBWq1xKKoJ7mbZWe/40KHqGTSZ6XdahO6HUfZV5gJ7gxmKEa9Gp1eTzLXAf/pSXvX1tNi0uIxG7rKDoCA3czNjY7OdzZgp61VY9VrB9qk5102cuUqZp2aeruFz2LsTjvxW43sRIc0L9RHP6QTZhMntA6kooSnrSE6pAhl12k543CyvpdmeT/BSUevBXrKbKZR74yzF1qjzeZb7TZzrtGqCI10FGQUgsCCw4axJbHYNlFdKbF6yKhnXAtyc4RBEzBWTXG4oHYQpWp9JFdcKI8unKkkiWN+B/f43MzW1BRfSLD21Da7t+I4ncM5/IMu1KHnPkLRUS7ViEXvXF2hVquZmZnhgQceYGZmhkgkQqlU6rZKvxbjVQfkO/GyuHz5Mh/+8If59re/jct1dJLvS4lXws+i3W6TzWbZ2tqiWCzyxFd/1H2tUQaVRiKyLRu5GzVs3tgnMO+mXm1QzVeYMWuJJwoolQp2ZJCz2/RsbSVQqSSkYArLkG1gPxgD1Au9LWZLI4Eg0BZhxmOh3e60VR+EQadic72jEsinypx02tEV20TWkswsOCmWahxfcBMMZWhXOwDZarRxyb4WXqOGTGZwSzvtt6GQP2PaZ6Mh9xSbdGoi2XzXP+Mg9KLEAfYKKhGVJLEW6y2OboOWa7sRJIWA1djhn6/2Gb1POyxD5WmZ/FGttFIhcrfbTTKY59pmBEW9zu1IhjO23iQNAThht9LIN7i12zvXugyGfpORqzuRAapiXGjH6Fv3ZI8Ni1bDOY+bu00uHCmJ8JUkq9ejhILD78n0iCnNAKI0OjGpDGkaOQjVGM7dahzlMzGYPZt0KpY8Vs7a7GhjNcJPB1l9ZpdEaBAwD2SdhyMpF7OHRbMxeqHpTzDuNARBwGw2Mzc3h91ux2g0sry8zHPPPcfe3t4rVlP6WcSrDsj9XhZLS0v8s3/2z7peFt/+dsfR85Of/CSFQoH3vOc9nD179ghgv5R4uZTFYUe3UCiETqfDYrGwfaXzMDsnrER2kkwe83aLFJOLbmqVOip1DxyLN/dQroQ4udbf9AAAIABJREFUVqsz4+7wnx6PmVarjU0EsQ3KYZ5NotD5D5iesrO30QHzFtCWuWmhBdV6C6/b2G1IAZgJ2KlWejeeFKpQ2coRi+URlQoEAeLJPGqVgu31Hs3g08uaZ+3gVAyASr3Byl4cBGgqICJTFzM+G16LaaC99+SEi/W9zsPktOppKNoDvDGAxaCj1YZZl4XNRIZWo9at1J+acLKyN5xDDCVyA5n8pFGPva7gxlqUZqvNktfOpuyQtx7MMGky4TcbWTBZWd1LDygtADZjaZbsdnSiRLs1WLCbGTNLbthYI1EQmLVZmdJbOI6OxlqZtYsRVm9FKcpGPflSFcOI6RvJbBHFiJUgNcaMpzDGLnTcpt+gHb6o5MtVFnx2zgc8HFcYYDlP8NkImy+EyAyR1x3EqGwXwO0efi3z+dGLyeHC3kuJRqPRVVndc8893HXXXdTrdZ5//vlXtK70asY/Ci+LJ5988hX7rDulLPo7+TKZDGq1GpvNxuzsLDqdrss53b6ySURug3b4rMSDGcQDcBAgttu5geJ9emSzVc+tZzewvWGe1W9fZvrReQq5MrOTVtLPrgJQOeT/ejg7VvbtndsqsYsaKqXIzl6SY3NuIuFOtqJWS+xs9W5kUYBSpoLOpEJSimxsxwlMGNnZz3NszsnmtV4nVzVaRqVUENwbvGY2k4p2u0Gj2eLkgod0uUIonUNSCCQKRRJ9fKRKUpBK9raIfpcJFCLhTI/LPel3cms/ztkZLy/shjk94eK6zD8rRFC0mgPqkP5oNts41GrS1RqzWgPrwV6W5jTq2U/2PqfWbDKtMXFhP0S0MZqoNwkSl6LRIxX/cUNJ99JZLFoNrXabGbMZqQKh3QyxvRQxYNprBYZvla1GzVBaoN3u+GOHk0d572yxglYtde1W+6NYG/3dqmP4W1Vf04RWJTHttKCpQ3wjjapZYW33aDdjNJJFEIbTOZXK6M8ymXTAUQoqm6ui0UhHjhVFgWTs5TWEQEeD3N8UotFomJ2dZWZm5jXTvfdz06l3EDabbehq2G8s/8ILL3DlyhWy2SxOp7O7mgYCgSOTqZef6TWpZJMFJJWC3bXONnvuhI9kOIvTbyER6gFaeDOOpO6oMAAUwTSR717Bki5Qkgsfya2+7f4BGMuf6/GY2Ozzl2j1Cfonp2zotBIrqz1PhsVpJ4U+R67AhJFoNMv+fprJWQfNZptSufM0KQ7VbiIrSU74nJQOWVD63DaShSoqpcBWNIlRLsQcn3Sx2NCzZOvxqaf8ru78PwCVVkKzXsEoG9MoFSLJfJnTk25e2A2jUiiI9HG9Z6e8bL4IdzhntmNpqlnv2zIrRAGTWkWhr/140WXn8kqIedtwvtekUXHMbubiTnRoJrmdyAyV2U1Zzdzj9nBGZ4PNKhuXoqzcjJLP92gewwgjHgBao7li8wjOF8BhPrpzgQ6No1aOGImUGS0JU4gi5yY9nDJY0GxWCD4dZOP5ILlkCYN++PnX6008nuHZbjIxmiseWUdrg9HYuZ80Gonj8y5OTdqw5kqsPHlz5Pu9WBw4vR09D+Ef1I79s4zXxlm+hOiXvVUqFUKhUHeawP7+PhqNhlOnTnHPPfcwNzeH1Wod+8e6+fQ2AHavmdBmnKnjPiryVrQtE6aOvpt1eslLOpZj5pSfSqmGSqskHek8IMo+rVI+ke+2Wh8GBptR2/1hS4D+Pa1CkpiacNCSCz6iADtbPfDWaCQyyRpzC27K5QYKlZLjxzzEkwVUKomd9UHet9looioNZluCAG1RIJErc2zOS77aYF92GVNnqzTCJRJ/F8Rh1GEzaFnd6lEgfqcJMdNg92aUk+1Owef0hBuXTte1hzw14epOurYbtLRbbYpjMi2nUUdhv3xkC39XwMNmnxm+z2wkmsjTaLa6PHl/nPG7UTQFVuJZRvnFN1otZmxW9ColZ9wu7rF6mChoSF7PsHwpTLPY6l77wzGukG/SDwdWGK0BhtENGzAarLN9XhIKUWDea+O8z81cQ0N9NcvGUzvs3ojSPKSsGPccWCzDueJ0uojBMPwciyM8LSwWDfMBJ3NmNaxH2XzyJqtPrZBPFAhuJY6c153G4Qz5tRg/V4BcKBS4efMmzz77LL/6q7/K7du3aTQaTE9Pc99993HixAk8Hg8q1ZhMpi9Cm3GiO50H3hXoFIsOilwOn5kt2dei3Fd80+g7N6cor9Rzp/wUMxWsHhO51OB21mHv3OQCoGy2kCQRu13P2vXetrGtUXSfdKVSQSyWY3Wtlx1PT5gp5HvZ1+yUg3yuQgvQaCX2QmmCsmH+TMBG7dD212zWoswOZm8LU04a7TYum4FoOs/d025mzRbe6PGw83chQvtJKoUagYxIwGIacBDzO00UVzKUy3W2f7zPfT4PqmwLc7RN8scxzmMl01fI8qol6pHxPhPmusDWdpIT3p46Ysnr4Mp2j3oxadRQa3dpgY1wimPOzt/MZdRxwu3gxk6UdLlKW8nQCp5Do+KE2YyjLiHt1Nm8FOX29TCZTO98R4ExdCSCo0J4md1jKmn0ccYRmbVWpeT8jJe7rFaskQaxn4ZZe2aX+H6WaHR09pwaY+05qk0aGKm0OFBMiKLA9KSNu+ZdeGlSurJHZS/F3tV9WodopXqtwbVLKy/akTcsRmXIr6X4mQHyi/lZPPXUU9x9991IksQ3v/nNl/z+X/rSl3jzm9/M8vIyRqOR//7f/ztnz559Se5uh6O/GSSfKnasMmUawuk10263UWmV7Mv0gaRSsHM7jKSW2F2NYHEa2bi6C4DBriW0OajZ1fbd5O29OEuTVrxOU7e3vw20+ralDqcGm0VFXe7wE2hTKfbA0GHXsXo7glYnsbkZZ3LWxcyMg7TcaisNwZKA30L4agixT0al1SipVessNNTU/j6MYavC3nc2kUJVLBYdWVmNkVhOkO1r00YAIVUfkF3pgw2CN+KsyxRNI12iciGNz6Bl2mbCWtSiKI/Oau7yuwkGO4Cdlzlzm15DOF3ocpqSKODR6Ykeci1TNODugJdiscbtYMcASWyDVe6s0yolTnqc3OP0EKhqKW1X2V7JkAznRk6tzpdHF9OaY0oyxTGysuSYgazj2pP7KYtJp5nzAQ9LKiPC7TytvSJblyPUSoMLcLlcw2odrrQ4rLI5fNyo0OuPZsg6nYqAz8y5OSeWTJHQUyssP3mTpKxOKhVGf9b6zb2uydC4jrzDMS5Dfl2H3Bd34mcxOTnJn/3Zn/Frv/ZrL+szPvaxj/Hcc8/xuc99jmaziX7MFvFO4wCQTXYd+xsxppd8lItVVBqJnVudLHZi1kVdLqJML/ko5ytMn+jQFU6fmZq8FdfqdF2q4yAa+XKnUlKtI1brBJ9eJdvPpTYaCH2Zgt1uRiH0MoDFWReRvi49m8VAq9lmctZJs9kiXy6wud0BQkkhsrd5tIKdCWUppsrMezucq0GrQtWEzN/tsrfaWUDC8mckEgU83h49MzXtxNnsga/frid6fdBbOV0oYrcou4Ums0FPrdRAcb2MOaugUWgS3UwPbS+26jWENnrF0kS8wozFgFaAbJ/K4KTbxUZ40JdDALQKJYoanYGiLVA1BKRyx73uUW8AVbjJ1pUYt66HSfa1f9fHON/HxwzjTAyxouy9NqaluTUaLIYV9KBDcxglJec9LiaLEpkLMdZ+vENwNUG71UY7hs/WaIZ/Xrlcx+4YYTKUGv3dDhYNp9PI6UU3i3Y97bUImz+4SSWapTDk2ER4dOG9URC7JkNra2t33JE3jkN+rcTPBJDvxM9ienqaM2fOvGzy/eC4V6oxJLKTZHu5A7o2XweEJJkDnl3yUZKLaMohWktJrWJywc3a5U527J93Iw0BnNDtILRBlHlQq1uLUOzLlkQwJHMolQpEhYC21aZ8Yx+VzDlW+wpx83NO1lc6YJgvVtHpVVhtlu68OI9TR/nQguD3W7oKEoP80mzATuZqlKlZJ7lMmcCUnVSigNNtIhLODuz0JaWC4OUIJpnn9Kg1ZFNlQgeTqgVIZCrUG73vnpSLmjqNinSkyMZGnHqtydQQX41JrYnC4XNWmin26VgXrUZubA8uAgJwetLN9c0IV9fCnHG50LUUtOXjNEqJRDBPfYQKIz7GH7hYbXRnEh6OUqU+skCXL1XRjzgulSuPbF1O53tA7rToOTfp5rTRimG7Qm4lzdqze2TiRwFv3BQQm310l5/NNjyRSadLR7hiURSYmrSha7cJiJC9sMnK39xg+9I2TbmDVDNC7pfPlDBah/PSO2vRrsnQ3XffPdCRN2546esc8h3GS/Wz+IeEJElHTOVfTvTTFaV8BVES2VvrZIzZPg/h2O5Bg4iWnVshlBolO2uRgRvG7DBSGeIjUEwUEHJFRDl7rJdbRK4Ecbk7/hBSu00tmOL4hJWZKTuVzRiJzThLbhOzk3Z2ZRG9KAqU5Gq/22dmfy/N1LyLjT5Np1FzdFtp62tOiVwPI4oCZhRE9zOo5AfJJBdznJ6Onjraby0az9OoNfGrVZ1CYKyESqUgm+mAqMWhxWbWEJalTFazjqj8/26/hWmvtUvPWMXBB/e0z8XqyiDQSgqR7Z0k9ooKm0HLGb+bjcjgllYATgc8XN/sHNsG9mLZrs8xwJLfwW4ojTRCAFxvtLAZRxfTDCMaHgDsIxolOq8NB7tGs4XDcvQ1URBQ0eKkzchsQ031SoqNp3bZuR6hXmuObLkGiESGj2sCqA+ZGtI9lzHjtV0uI1qtiuPzLk5OWDCnC4SfWuH23y4T3Tpq5g/jp1M7vMOVGwcqpoPo78gLBAKEw2GeeeYZNjY2qFR6O6XXM+Q7jJfiZ/GPJQ68K8x2PbHdDJPHPJTyFQILLsIy0LkDNlJywWxyyUu91mD65ARTxzzsybyyoBAJ7yYJrh8dTCkAguzbYPebCG/GaTdb2FRt1CqRltwJF3t2DbtKYutqxwVv9fvXsfQJ/E8c9xIJye8jF1i0elW3yi0KArHg4MMrikKXdgEoJkucm/bQCBZQSGJX13xQmCmWajidxq7eWG9QdgGhvlPimNeOWJbwB6zdv7c7YEfVJ9lz23ugU6jVScV6mWgt3cuEjRo1sZ2jvOqJKRfpXJlIJMeMwtAtVvZfz2mrnutb8rUXOr4L/aY6KklBcDdNq9XGPsLFDMBhGT3B2Tkmwxw1gw7GKyYO3NuMOjWnAy7OORy4Em0KV7M0wjUS+0fBt1yu4RjRgp/N1QaufX8kxrQ1i0OM+Z1OA6cX3TiVIqxF2HzyJms/WaMo0zy1Sh2nd/g1yY75LN2I3URkL0VtSEv5wfDhU6dOcf/996NWq7l69SoXL14kEolQq9Vez5DvJO7Uz+KVjBczKRkWrVaLZrNJuVQhJWdynhkntKHe6ACGRtP7g9vcPReystzUoNSqullz53grCiXdzr7DcfAHcLl7jmCrP75NwCQgyNlFMZ6nHUp1jYScXgsb377E3LQdg17NrswNC6JAMJzBaNJQvR3szkObCtjIZQYlY3OzDkq5wWxIFSqyvR5nZsFNqVjFN2ElHs2hN6jY3U6i6cMv/0RP55vazeHIwd5eCl0f6IgqBbt9uuGyPHlZp1fSbLWI9AFqdDPV9WdYMFnI5QaLZ6IgEE10smGdRkk8U2TG0suwOpmxm215+65VivjMGiKHZrXNOoxkZPmcYZRlGqBVj67WK8coDsQxLkWj5tNNOEx4tTpOaEyIt/Ps/HiPjYvB7q7Hah1dD1Gphn9eu9Ue4Pv7I5Usoh+hOc5lq11VxLEJE45aheyFLVb+5gbleH6kJM0ygnuOhzKII3YioxQrrWaL/Y3hGfdBSJLExMQE999/P0tLS2SzWXK5HGtra0cKgf/Yk7/++JkA8p34WbySYTAY7thcpNVq0Wg0qNfrNJtNWq0WKrWSz/6//xqb20ylWENUiGRjZSxOPRvXegvLAQhb3SZ2lsMoVApqtSrpvm4jvcmAy2c/8rn994hKI7G93KNw5k5OUO1rV50+7SOb6HMocxmpFmtEfnidpTknBbliPT3nIJ0qMjtpY+3HqxwPdEDTOIS7VBzyE9CbVOQyWYr5CgdD4ZRq2dDerafVaqPV9o2R6pNjGU0a6rkatVqjW+Ds/I7Y9YEGyOU7rzndeoTa4GJQLTeYspk54XWyvHx0EOnJ6V7zyeSEjVi6yI21CEteRxeMr8t6bL/TRMBtI5gZBHWFKJCK9a6ryTAa6MYt6PUx2/BsbnRGeGAWpFIqWJpwcLfHxWRJSe5inOp+kf3b8aEDC8YJnKUxrcuGEZ7JAC7XoKWpTqdiacGFS6XAVa4SemqFrZ9skAv1wC0WGu0zoRyhpW7Umzh9w7Pnwhij/b2NozvKUaHX6zl27Bh6vR673c76+vpr1przZwLId+JnceHCBSYmJvjGN77Bhz/8YU6ePPmyP29cYe8gC67Val0QFgQBhUKBSqVCo9GgUqkIzHv4j1//CI9/6E38x6/9Fv/1+5/gyz/8XUzWTiagVCvYvd3RwRodWtrtNt55J3t9HXZqrZK9tTilQw0NokIYaEOdOdErEmr0KqqlKhGZMhAUIqVStcurafQqtq53FgVREAj9dBWdPMFaJQOvplan3W6Tfn4DSSES2RvkE/V6Ndu3BltkvX4rLUGBpBTZkjPucrElv68WaBMO9q5pNNxbdCan7d0HMipTHHaXgf1I7/e9TlN3arbaoCMZP8pVupRassGjgCZCVwVxatHDrb4iXjFT4Z4ZXxeMl/x2/Fo1q/tHFSUzNj2Zvsy7NQZYR6kbAArlMVM0hOGZtcOkw6iQOGOyYtytEfxJkPVn98jItE12jG9FbJgDnhxKaTTojgudXoVLVkUsWDS0VsNs/OAma0+vjQTyfLo6cm2oj2nXNlqGU0OxUO97SZKCmWMelk56sIg1rn7vhTv/MnIIgtCdNtJfCHy16lWvRvzMCJcX87O49957X/KkkFFx4Pg2MTHRFZi3Wq2BrEehUHQnjYxSdrgDdtyBXnbbaDR48/vv4Vtf+hF2n5Hwauehz8nto2ceOsb3/vyZ7u9Pn5hgczlEcH1Qf6xUSQMURrkPJGZP+Gm3oSVPHlk8N41SqyK02bl5HV4d+/JE6JkTfpaf3+DYW9xsNFtsbsUxm7WU5dFMqe04d79xkcuXB2/I6UkbK33gajSriIYyFKpN5k94WVmO4HDqiUXzKBQiO9sJfD4rof3OOTjdRuJyg4FCEtnfTaM1a7Db9d0Zgq4JC1c3e8Bpt+qJhLOoVApEUaA8hMKphzKk0kezpqVpF8sbMfxuM7f3Ot/dpFcz47OxsZ8kejPEvNuA1GwTXUuSl5QI6kHvBYUoUMoPAnAqPaZJYszkjXhm9O7rQKEhCgJTLgv6FmT3smS306yqcjTqzaGeENFIFoVCGKp/zuXqI70kEmNal/OHEwFRYCpgwyCJqHMVMhc2GZa2GEb4KderDdwTVqL7RxeI5JjW91p9uIZZoRA5eXeAWrbI7pUN1n/Y231e+ptrtNvtO6YbDjeSHBQCp6enXzNt0/Bz1ql3EAaDgWQy2c2AD1QXh7NghUIx9o/VbrfJ5/NsbW1x8eJFLl26xNlfnMNs12OxdLZ83hknmUgBUalg8h4LrskeF1xvtPD36ZQB7B7zABg7J6zs3O5kq5PHPCw/tznQTVfIVYj2ZbjVbO+1RKSzhVz5wTXuOuVlym9lxqYlttfbWiqGTJWO7w4uEP5JJ4E5N61mu2uf6fZ16I7AtI1qpYFC6nGHzr7t7sKiG0GAcCiDo69jK18bBNyDazA146RWHL6NbCTbR3hWAcjlq6hVChq0MOs1nJp1U6nWuboWplCu4TRraJfa7GznmQ+4SKfLBByD/OnJgOtIMStTGN3skMqX0IzwiihV6liGFKT0GhXzbhtv8HtwpyD5XITdCxGykbJ8DUZPy2g0Wlitw1+r1Rq43cOnpmSzZUwjADQczqLVSizM2Dnht2BK5gn+/W1WfniLxO5o+mEcXWO2j9Apx3JII9Qnqj4bUYfXzNIZP5MeHaX1XerxJLf//gal7ODikYpk2Lh658OOxyksXksz9n4uAblQKPCJT3yCv/zLvwQ6s7nUajVKpfJFV8tarUY4HObGjRs8++yzbG9vo1arOX36NPfffz8nTi3xro/8IhHZ59gqg9Ndb1zkF9/2KL/wrrMA6Ewatm+H0PfJp0SFgHToIXfKhRdJpaBaqKLRq7oAPXfXJJJK0RXRz53wdef6eaYdxHc7PxdEgfRqkNZGhOjVHZJ9ovvtixuYzb0CjsOhJ9VHC3j8FlZvBElnSugNarZkS8+MzNdqZSWCqm9AaCHfyxDz+QqeQGcROvhuLRG2+7ajoiiwL5+31qDqUiL9Mem3Et7LMOsZHHcUcOgJxrIcn3WgU4kks0VubEapNVqoJAVnZ71kImVC4U62m5YzdIe2B1KiIJAeMpuuVKl3NdSHo90Gp3X0xGibLG/z2YzcPenhpNaEcq3I7o/3qMcqFLPDO/pc7tEKDecYA3zLmMLewTivg3C7jJxedDGtV2HNl9n50W3Wn14bAL1YMD2yHbowhj45fP92ow0291FliiAKiCKcOO3FoW0TvbDMje9eYOviOs1Gk8YYCd6F710Z+drhaDQar/kuPfg5BeQ///M/58knn2R1dZVHHnmEP/iDPxjLKWcyGdbX13n++ee5evUq5XKZQCDAG97wBk6fPo3P5xuYYvJL73+AX/nQIzj8VsJyO/QDv3wWURR5x796M2qtkokFD61mm1ioBz53P7pEtM+mUyGJXR568ewU0d0kk8d9XTVFswX6Pl1ru09fbe3LRt2zLnZvhNhdiaAz9cDXGTCTiRXw94n9vc7BTMtk0eIOWAntpZmcc9KoN7E7jQT3OhNGwsE0ggCh/d71SyY623m3V09wL93NqPIy9SKaVAMV9EmvlVK5jiB29t3DMjCLDIxGYfChqteaTFnVpDYTBBMFDt52zmfHpdZw83qIlrzNn52wEYp2lBvVvjbgEwEn0dhwemKUQQ+AdogKQykpOOZ3ENDqma6qKLyQYP2pHfaW411OWq0ZzQSWy6OpkHFOxqMUGgA6rZLZaTunZx34aZN+fpOVv7nJ7tU9bPbh8r1GvYlrYvjiENtPj+SKS2MM9e3OzvuptUqmF534/VqU+TTL371A9OY20bXwkWN2bwUHTLf648JfX6FWq91RX0G9Xn/N+1jAz5BD/lmH3+/n85//PP/+3/97vvKVr/DYY4/xC7/wC3zkIx/peiErlUpKpRJmsxm73c7U1NQd/VFVaiXv/K038/bffJhnv3ON7/3ZU9z/2BkA9CYtD739LCvX9lHrlKTkKrXVa+CX/tV5bvx0vdtCPXvCx+oL23inHaxc3AJ6j+TkcS/BjRgqGahcfkvXzEiplti5FQSFiCBJA5RGs2+ysNVhJr6dJr8eBUWneSPSZ2rvn7KzdjPE0vlpQuEcFZkq8fjMJBMF/AEb+8EMgUkbe3JHn89vISTzzwaDjqhQZHsnjihCOJzpmOmLQB+lZ5SLRFPT9u6x/SGKAkF5uGpiKwnyn2DKZaRabuLSGLixGsQ7YSStE3HoNeysJ47gl7YvQ9rfSqLxqKg2GhRSo0HEMEbe1m52qBWzTsWU3YKQbxBaSRBeD2I96SMVHg7ylcroyr4ojNYpx+OjO0yLxcFCol6vZtJnRihWUUSz7D+/NfS4kRktICqHLwDVSh2X30oseJQrTkSG+25Y7Ab0WgUzARM7lzfYWBukG9Tm4de5Wq4xd3aKjStH6YmNy9tk4jmMNn2XehhFM/68ZMg/t4B8EAaDgX/zb/4NCwsLfPnLX+bBBx/E5/PxG7/xG/yLf/Evjvgfv5SQJIk3vuNu3viOuwd+/vhvPsKFHy7jmbRx6UfLPPPEFT7yuffi8OtZvM/PtR91Hp5mvYkgdLKfZr3Z8VqWG0rUeg0zJ/2sXesUOu0uEzG5G8o762BnLY5w6AYUBMgcOKcJdJtRwjf3cb35LgxGNbsv9AonGo2EIIrs7SQxW3Rsy2Cdk7fbFpue/WAGU99cQKtdTyiYweEwsrWRYGLSxl4og9tjIBop0FKLR/SlxULn/Ww2A9vbR7lLr0tPdL0DRqlwEdtJC6l8Ga1SjaMJt291MqvYfp5z90/z3PLR4q/FqGGtz7yp0Wgx7bKgEEXWhsjoDkIxdGQ8TLusePVGEBpEl9NstwfBNzdmWx8Kpka+Fh5j9p7Lji7exWI5PG4TTrOWcjTL3o0gGzc712GUrAygMsICE8BoMjLMQB5AZxwOoIVMBb1RQzFfwTtlx2pWk96JEry2xkYiQWLEd5cYvRCph5gTSUoFJx86RiVfwerqGHk1m00ajcbQgvzrGfJrLK5cucJv//Zv8/Wvf51r167xe7/3e/zVX/0VH/nIR/iVX/mVV5T4Dyx4CCx4AHjwbXfxoc++E62cJf7zjz/Op370JYxWDZs395k/G2DtYic7mFrys3FjH8+Ug43rQebu6rSba/QqNq7tdt8/kyoMnf/mm3Gyv9Lhn6ePe9m+0VNXaNoV6EuypuacbNyOMH/Sz9palBNnA2RuBLHa9ezLBZ8DQ5lyX3ZWlbNot9dEIpHHbNOxF8rgcJi7gNwfKpWCPTnTqlSGg4NRpaZfdRqwmbAadVgFJbVmjYZMBTgdBjL7wzM0l1nNemqQDtALEukxpj7930erVjLrsqCuQnQtSXIrguQudWV8hyMSHq2KKJUaWKxaMumjoF2tNPB4zANNMQdRqzXweM1dwyiFQsBp06AXBYrBLEIyz8qQdulEOItWrzriVQKdxoyR3700JpMfom+WlAom55xYjCo2Lqyx//R1+pfGRDCFZ8ZJZEgb9d7tIFqTZkBRdBAReSHVmbWcf8sZ7nvsLHe/5fQAXQedonyr1aLVanX1xaIoIknS2Az5tRSv/W9wh/GZz3ym+//3338/X/+plOWNAAAgAElEQVT619ne3uZLX/oSn//85/nABz7ABz7wAYzG0S2zLze0fbrOudMB5s8EUKkUJLSpAdBUyjpis9tMoVBlU9YKuyaM7F7vZHmugI3oIZOfgzD1ccXawyqAWJFYo9/XV7bwlB+8vJzFev1W0qkSdqeBUDCDJInsy5SIKAoEd1NotEo25U6qg7lxzWaLdqMJhzhgu0lFJFLC7tCyunZU7K9SSexvHXJqKzaYsOkJb6cJ9XHuHruR9ZthFE7VgEeCKArEE0fBT1NpDwWp7vvZjFiUKk7pzQRXEuytDBb+YrEcOp3yyDQV6GTgNpuaVGq4JtnpMA0FZACLVTcUkAFcDgM2gxqKVYI39kltxDi4At654Zlwu93G5beys3r0+uYzJSx2PZnkUaledAglcRAHXLHepGFiykazWGbn8gZrW3ucfGiR1P5wlYbdZx0KyI16k9mzU6xe2Bz8/Qkri2+YZvo+H+cfvYvAVGDs1HlRFBFFsXv/NptNyuUy2WwWs9lMq9U6Qmm8liiLn8ui3p3G9PQ0X/jCF3jqqacAeMtb3sK/+3f/7lUXkr/1/Q8QWPTwe9/7JBNz7s4PRYGdlSB6q5b1a3u4pqwd824BSn0PvcNj7gyhG3KTHXQOigqB/dXBAopZo2DKoUVUCEzN2dnbTKDWSmyuRbE7DezKVMIBV+mWt8ETkz1T+4lJG+VynbkFF+VSDY1WyZ4MmMlkAaFSR2oPnpdKBnyzXkGjfrQRY27STuWQJlnI1wlvp9GppO72XalUsLcep15t4D9UmFyccpDND2Zekx49Ny7tock2cclqCUkhsjBh51zAzRQa8jeS3Hxqi8RWhuYQ57d2e1Didzg8nqMdmAcxrrCnONRK7HGbOL3oZs6oRgxnWP/BTdafWT+STR5ILYeFfoT0DTpSy2FRylcw24+aIbl8Fpx2E367isLKFre+d5GVp25Ska9xfH80JTOu+04URQRBYO7sFP/7v30nX3z6P/CnN3+PT//px/jVD74DjU7D1atXuXr1KolEYqz8rtlsEovFWF5e5vLly7TbbUwmE7VajVqt1ks4XkNgDP8LZcjjwmw284lPfIKPf/zjfPOb3+TXf/3XmZyc5GMf+xjnzp17xT/vkV+9hzf907tJJpM88J5TbN0M4pq0EtvLMHduks0bYQrywzh3wsfG5Q6loVCKHUmcQoHVoSXdt51WqhTsy5NEppf8bF7tURyTxzzcfnadVqvN4sMnqAsdkJw+5uX2rTBakwLSYDJru8W7g8YNfZ8szGjSIgh0TYUmpuysbcQwGNTEozmEVgtluUajzyeh1ercYg5Bw+aQAaDt6mAF3WTSorNoqWbLrPU11CzMOFm93NkgW3VqdvuOaR6STp1a9HD7WgjakEoUcQg6TjtsBNdShPYGF6p2G7xeMxsjvRNeXuvtOEP3cqnO3IwDHZBYj5J8bqObBU8tukceVxsz5mpc12FbGOe4ZiWfqTAx40CvFomtBQk/f4sw4JeVQocjtpPAGbAT3zuaJe/dDmEw6yj0UUWSSuL0LxzngXec55N/9lvYfdYjxx14U0xMTJDP5wkGg6ytreFyufD5fGi1WiqVCvF4nHg8TqPRwG63MzMzMzCA4oDSqFariKJILpfD6/W+ZrTIrwNyX0iSxPve9z7e+9738vTTT/Nf/st/IZ1O89GPfpTHHnvsH9Tx0263KRQKJBIJkskkrVYLm83Gm9/9IM984wY6sw4QOfPgMR55173cuLBJMpSnkO0Vk+ZOTbDywjaCXsc//deP8JX/89vd1/xzLrZknlnV5ysgqRRUcqVuoU1qNAld3eT0fQso1ApOHnchqSUiyhImq4pcroJWq+xae/YPTy0Vq8wvelhb6QD/wYNuMEkUY3XaKpF2ooTGpqdSrWPUqwmGMzjsBnb/fgXT6Qlyhd77adUKNvu32QJMzTm4vhIhcGiEfLlf19s3L8/jMLK505MWnlr0cvtacECBYdWoKUZyVEeApGaETzGARq0DhvPI2TH8dDg8SAcYjRoCHhMUq8Rv7JEdIcMbRyMkwqMni0TDoxs9hoGqRqciMOPArFMQLmTZ+rujhVKr20xwbXhBVGNRwt7Rn7eaLfzHfARXw5z/pdPc97Zz3P2Lp9AaR2fwh8NoNHL8+HEajQa7u7tcvHiRRqOBRqPB5/OxtLSEVjv8/QRBYH9/nyeeeILvfOc71Go1vvWtb73qZmavVLwOyENCEATe+MY38sY3vpH19XW++MUv8rnPfY5/+S//Je9///vR6UZ73vZHvV4nlUqRSCTI5XLo9XocDgenT58e4Mne9zuPcfmpFT7zpx/qbj1/+dd/gWQky79993/t/l4hVwRRwdK5AOcePg59gKyTs1JJpWD3ds+nYvGuSW4+vQJ0HrBENMfUvJvNH9+imC2xeG6Km8+toXeY8CyeI2+pY7So2NvLo1Ir2Jed65RKkb3dJE53r1kiKRf9HHYL8asR6jYNUqqM321ibTfJhMfC7WyEgEPPSrLAhE0/AMhzU05uJ3tAcPy4m3SpwkzAxvpaLzue8FnYW+uBbrbPA9ll0ROTgWoYGB+bd7NxZZ8Td00QiQ0H0HRmdNtvcox9ZCSSQ6kUuyO1+qNUbDA9aUNs1ihHMiRv7rN+vXdiLr+F2BAJYKVYw+mzDC3GZVNFTDYdudTR75FLlBEVwlDwLcgmS1aHAbfPTDWVZfvyJrdXt5lY9JJPDl8cMkO6PA9C0T66iLmnHNz7trM88CvnOX7//IAB1Z3GQV9APB4nlUqh1+uZn59Hp9MRjUYJhULUajV8Pl93KlCr1eLatWv8z//5P3nyySexWq08/vjj/MVf/AV+v/81RVu8DsgvEvPz8/zhH/4hqVSKP/7jP+bRRx/lbW97Gx/+8IfxeDwDvzsqC/b7/Zw4cWLkjXHfW89w31vPHPm53WPmo//X+/gPv/b/YPOYCK3FkPQaHv+t+zC7dGh0qq6sKRPvPFQzSz7WXuhQHN4ZB7efX++8mSAwcczL6qVt4vKUaFEhkJZHH3km7bzwzWeR1BIzv3QGJiy0hCbBHdlc3qGh1YSw7MvrcBmJy4CcCmcQaw2EVhuhDXqZRz6w09TIhTXdoW11qc+NLRCwUG7U2NnL4jlk5WjRa+jP02LBDLoZE81Gk03Zm3oYGOv1ahI7ne93MMFiWBQLo6mARKKAwaChUDiqDmg2W/j9NnblRUuSRKYCNnS0ia9F0aUKrN8YXo+wOo1DAfngtVHqCKfPMhSQm40Wnkkrkd30od83YbdpMLTrbFxcJXGo+S24FsHiNpOJHs2+g6sRzE4j2fhRwN67HUJv1uGdc3PmnxzHd8aBwa3F5/N1KIKXAMaNRoNkMkk8Hiefz2OxWHA6nSwsLAzsSg+KdvF4nCeeeIIvfOEL+P1+9vb2OHHiBO985zv59Kc/jcUyWgb4jz1eB+Q7DJvNxmc+8xl+53d+h6997Wu8733v4/jx43zgAx9gdXWVubk5lEolBoMBu93OmTNn7ni69bg4/eACb/3nD7K7EiKxn+LRd93D1OIEt27dwj5hIriaQG/SdDsGD25gQRQQ261OsUqhQABuPL028N6L56ZZfmYFQRCoylzuzAkvV//qAgCzjy50f1etliCSZnrSwfZuGpfX3LXDjB/4McuAW5KBJhrLY9Crycv0R6Vvy+20G7o+zgaDGkmlZC+e49i8m9U+GkOnVbJ1+9C2uQ1+hxG1WuLWcngoGANMuU2sXuvsFsLB0Vv6bLaCTidRKg0HZrfHRGF9eCu0zarHoJZo5ysEb+yz1zeIwDuimAZQLI02KBrXzNFi9MJicZhIRnIEZp2ohBahmztEng8RAQKnPEOPabfb+GbdQwG53W7jm/cMALJSLXHm4RPc99hZ7n3sLmzePu+Wep1wOMwLL7yAXq/H7/djsViGJiLVarXLB9dqNex2O4FAAJPJNDJxKRQK/OAHP+CJJ57gxo0b3HvvvVQqFfb393n44Ydf9jzOf0zxOiC/xFCpVLzjHe9gd3eXb3zjG3z/+99ncXGR3/iN3+A973nPq+Is9c8/83ae+h8X+ZUPPcLphxbQGjS43W6ePb1OcDWByaGhkMyj0ijZvtXJyJbOT3eoCklCGFKtllQKorJl5fTZSbZvRZhYcrJ6cRvoyOs2v3+Tpfe+ieWNJDaTkevfvoTBk0Q14aUkW1A67XrSoQwoBAQZ1MPLEfxvmiMUzHDXrJO9S533jN4KIZ7w0mq18TqMpHc6wO3zW6iKUG80SR9ye3Pa1ASHSNr0SiW5QoX7j/lY2zjatedz67pgDJDL1jDYdF3v6MPh8w/SJP2hPcQx+7xmHEY1hVCG1l6C9Rd2hx5XHdOxpxhh0wmDuu/D0R6CxwazFv+kFZ3YpB2LsbZ5lNxVqUZLyWrV0QXIRq2B0Wbgnree4b63nePsPzk5IOPsD6VSyeTkJIFAgGw2y/7+PisrK3i9XrxeL/V6nVgsRiKRQBAEnE4nx44dG0kBttttYrEY3/3ud3niiSeIxWK85S1v4eMf/zj33ntv91k7UFz8PMTrgPwyQqlUMjMzw5NPPonT6WR5eZkvfOEL/NEf/REf/OAHee9734tG8/J8aoeFVq/mrR946MjPF89O83ffegGTyUCIOPYJI+GVBA6fhbVLmyAICAoRhkx5mDzuYuPiDoIokMuUmT0zweaVHrCYLFoizRY737uA5+Fz5Fc6XG8hkmXx7DSb8pa6Fs8htNs0VRIKWa/barSYtBtx1lvoorkurVItVPA6jQSjOZKyn/LJUz5aCoFULMeZYz6uXh8sLtULw6VPUrmBrtBk66er+O+eINdXfNRoJGqZo9mux2VivTBcTTGusFerN5ifcaAF4qthEj9d54DR9kyONgQa15QRH0FXAESGqBcOIp/ufE/3hBW7TUsulGTv2ia3braxuExURyw4wZUIkkox1Mxn51YQlVZJrU9+6J11ce/bznL/L9/d4YPHeGkcDkEQsFgsmM1mkskk29vbrK+vo1Qq8Xq9nDlzZqTWuN1us7a2xne+8x2++93vIooib3/72/niF7/I4uLi0OxZoVDg9Xrv+Pz+McfrgPwyQqfT8f73v7/776WlJf7bf/tvxONxvvzlL/Pwww/zzne+kw996EM4nc5X7Tymj3duwoRcmddrdUAbQWhQq9RBUmC06anlKwMKA0mtICq3SR97wwLFXIW95VBX9+mfc7EmC/hL6SILtSLXL6x3jxczRRYn3VzfSFA68ONtgyArOZQqBfGfrFHLl1BND35/m1aF4LMQWU0wEbCiqNaJX9ymGssh/dLgUILZaQe7N49W+e0ONVvXg5glKOUqpNdi0KcsmQ/YuX35qGpAO0YbfFgLbTJq8LmMVJNZUi9skosPLwjGghlUGmmoJC2bKmJxGMgM8SwuFao4POah3hDlQg2TTUuuz4NDVIgEZh3o1CLaeomdZ28SOnRcJpZjYtF7RIMOUClWR3pG1Ct1pk75UKnVvOHtd3Pf284SOO4f+n1fLJrNJqlUilgsRi6Xw2QyMTU1hdVqpVAoEAwGuXz5Ml6vF4fDgV6vp9lscunSJZ544gn+9m//Fp/Px+OPP863vvUt3G73a6oo9w+N1wH5FQyn08lnP/tZPv3pT/PVr36Vd73rXZw7d46PfvSjHDt27BX/vMljXuxeM4ndBFqDmp1bIU7eN8uNn3RUFaJS4n/72Hme/8YyO9d6wLZ4doqbP76NQiVRKdfJJfIdAJdDpRS74CypJTJ9mZ5CKZEIZUle2mHqwWOEgrIAv69gNzVvZ/25TU6+YZ78IQpCyJWx2Y20XUbUkTTlap1MLIdnys6tH9zE/sbFrnpDLRzNyhxODYV0BZ9Lz+6VDvBkgjmm3zTP9l6KuWkHt68MH3Qwaq4hQCSa61ARhg4VEby5z2afKsJg7Xzu4Wg1W7hnnAPc8cD5es1DARlAa5RghM2G1WWkUqrj9OpRtduEb+6z+aPO91p6YGH4QYDZaRwKyABq3WBNQ6VVcdcjJzj/1tNMnvNQqOXRaDToXdqXZA5fq9VIJBLEYjEqlQp2u31oIdtisWCxWKjX6+zv7/PLv/zLVKtVcrkcDz30EO94xzv47Gc/+6p0y75W4jUJyNPT0xiNRhQKBZIkcfHiRVKpFO9973vZ3t5menqar3/961itRwXoP4vQaDR86EMf4oMf/CB//dd/zac//WkkSeK3f/u3efjhh1+xFV+rVzN/yk9iN8H0cR/H759i9h4vu6v75GJFzj66yC+961Gyu5UuIOuMGrZlvfLx+xdIx7JkE72izdRxHxuXe+2tp950gms/We3++9i9syxf6JgjJfoaNwS50212ycP6c+sgQDSYJB0drNCnN2K4NSpsySwbV/c48eACkMDmMhLZTjBh0pBMFbGYtawvDwKLz2cikygSCNjZOuxuls2hUikoxPIjXSxjh3wglEoFUxNWtLSJrURoJ7LcDg+Xenkm7KynhysmDCNGFEHHGXBUaPVHj7O5jLg9JrRSm52dILvrRymG+pCJzAeRHtGSDR2nP7PT1OODHz2B+pAf9AH3u7q62uV+hxWnS6VStyjXbrdxOBwsLCx0pWjDIpPJ8P3vf58nnniCtbU13vSmN3H8+HEuXbrEzZs3efzxx8e2Tf+vEMJLnM780kc5vwoxPT3NxYsXcTgc3Z996lOfwmaz8bu/+7v85//8n0mn03z+85////EsB+PatWv8/u//PsvLy/zmb/4m7373u//BKoxKpcIPv/kM8UgSx4yJ+ZNTOBwOGsU2f/5//A8+8ScfQqFQsPbCNp9+2/8NwMRxO3vXwijVSqbPTLJ2eXALOznnZPtGpyhkdpnQ281Etjv0hkqrRGcxUMpXmJx3sREuDSwuNpeRajxDMVdm5qSPerPJ/uogZ7twZoLV5zdYun+elcs7mDxWsskCVqeRdCyH0W0k4TIz6TMQXO1l5pNTNhLhLGq1RDtVpHAo81ZplHgfmGTjxvjizsScA4NGSTtfYf/6HtU+r4vFuwKsXh3S7QCcuGeaW3LB83BMHnOwu3LUdB9gYsHB/tr/1955h0dVpv/7nmRSSC8zk95ICEmAEEjoZekllFCUoggqYqG4CF8FF1SEVVgVhBWFdUVEVmX9gYqEACoQ0EgnoECAJQUyqZPek8nM+/tjyJAyCUEJSXTu6+K6Zs6Zc857yMxz3vcpn8fwPr9gN1KuZuLh54ydtRm5KZmk3252q/CRkX3T8HFSc1MkJiaoGwkaOrs7klurQYBHJ1d6jQ2jz7geBPUJaNaEQK1Wk5mZSXp6uj5jQiKR6FM6zc3NkcvlyOXyJv3B6enpREdHExMTQ1FREaNHj2by5Ml07969TgC8oqLivsZd2iDNmoX9YQxy586diY2Nxc3NjYyMDIYMGcK1a9dacZSGycjIYPPmzXz77bdMmzaNJ554AienxgNDtRFCUFhYqP9RSKVSZDIZMpkMKyurRn9oGo2WOcEvYWpqQmVxKZVlVYRFBnPxhxt1uhx37OrJjVq+4ohxPTl/5Ir+fZeBnUlLzMbG2hxlRgkmtndmQ6ZSEzxcbbiVoPNsdo7ww1RqypXTdcVkFK425GUUIrW0wMXbmZtJObj6OpJZS2DIa1wo+VmlekGcGmNcWaHG18WOm5fre0/B3skaJ4UVt0qrqaznz3X3cMDJ1oKSjAI6mJnqJU3r05TR9e/iTqKB6wLI3e1RpRuemdrYW1JSr3uIuYUUL385FmhJu3yTvEYkK1185WSlGA5Cega7omxEVrTLwM5oqzX0juxB78gwPDr9toCXVqslLy8PpVJJfn6+PjOiY8eOjVbKabVarl69yr59+zh06BBWVlZMmDCBqKgo/Pz8/lT+4Ho068bbpctCIpEwatQoJBIJzzzzDE8//TRZWVn6SKubm1ubTYNxc3PjjTfe4G9/+xs7duxgwoQJ9O3blwULFtCxY8cGn6+qqiI3N5ecnBxKSkqwt7dHJpPh6+vbbLlBU1MTuvQLoCSvmCtx+XSwteSFfz7NO/P+za8/1rgjBOWFd3yd3l089d1MQNeSqrKsClFRhVKZi8S1brCuc1d3rvx4FYAONhYkX0nHqV4fON8gF5Iv3MItUEZmUgHVpjo3h3W9slqHMjVltwsLaoxxeVkVXUPcSPjpBobw9Hbk8o9X8Qr1Is++A3K5DWbVGjKup5N9Jomab0NIhG+j/0+lxYZzjeEuJc2ZRZhZmKKubOheKCmswEFmg1ajxd3TEXVRCSnxiVxP1LmNOoZ6N2qQZR6OjRpkW3tbajugzSzNCP1LEP0nRhAxpjv2ssaFiJpCrVaTk5ODSqWitLQUJycnvL29CQ0Npbq6moyMDC5evIitrS12dnZ4eXlRXV3NqVOniI6O5tixY/j5+REVFUV0dHSdSZORu9MuDXJcXBzu7u76vMSgoKDWHtI9Y21tzfz583n22WfZt28fixYtws7OjgULFgBQXFys/zLXGODaIir3SviIrnz00mcARC0YhZ2TDePmDtUbZK8gF27G64yERCJB7qsgvtbsOLCnH5d/uoa6shosLZDUEmvpGOzKlZ/urEZ8QzxIS1aRebNu+lZxns4/KzUzAxMJ+VklmEpNSK/XYy/vZg65yTl0HxKE2swUZXkVfv5yrv6caPDeTE1NSLuqW+qn/pJK98GBXDxueHWU10RXjsxbuUhMJHVWDTWUFJTj7GJLblbDqjWhFbh4OaG8Udd4OsitsLOVYi015fLRK+T/akBTopGcXoDi/MaLR5TX03F0tSd8ZCi9xnbHo5sL2TlZCCGo0JRjq7Vpdk58eXm53h+s0WiQyWQNRHtAl4Pv4+ODt7c3GRkZrFy5klOnTlFZWcmwYcOYNm0a69ata3T2bOTutEuDXCMUolAomDx5MqdPn8bFxYWMjAy9y0KhULTyKJuHiYkJUVFRCCHYvn0706dPx8fHh6ioKJ5//vn79uUe8Wh/HGQ2HP4sjonzRwIQMaobTq725GcXUVFLu9cz1J3r5+8EzaTmUi4cvqx/L5FKdTJpEgn2TtaobmTWaXNRUliOzMOeoloavB7+zigvp2PjYEVGUh6+we6k3FDhG+xCypU7M72OXT1Iul1uXKYqJvlKOl6BrqhVxQYNJYB/sCvXT9x+sAQoKGnCkOVlleoWjwZOVVmuxtXbmcxGOjJb21sYNMgAdg7WmEpz8faXYyGFrKtKcuJvkAt4d3NvVEoyI8lwdgaA8mo69jLbOkFXz85u9B4bRu9xPejcy7+OwfT09qCsrAylUklSUhIKhQIPD48GvtmaEv/s7Gy960sul9OlS5dG/bhCCHJzczlw4AAxMTHcvHmT4cOH6+MiX375JatXrzYa499JuzPIpaWlaLVabG1tKS0t5bvvvuPVV19l4sSJ7Nixg+XLl7Njxw6ioqJae6j3RFZWFsuXL+err74iPT2df/7znwwdOpRHH32UOXPmYGf325agNZiYmOh+yGPD9NtMpaYMf6Q/vx5L4MpPt90NtpbIXGWkXb8zw3TrKCO1tv/UxASq1HRwtMbJ3pzkm3eMioOrDan/y8InpK6MpPS29LZ3sDsJZ1KwsrcGVA3a7hQV6IxPQHcvbvyqM8x2HaRUmTT+Va0qvvMwsbazJD2pMSlNqKpo2ug6yGwa3WfRoWHwysrGAk9fZyypRpqfz43DDf3TOTfzMZGa6PSt65GfWYhnoCvK6w39wVqtwLOzGx6BbvSODKN3ZA/c/RuX5wRdjnxgYCAajYasrCx+/fVXzM3N9UE5lUpFfn4+1tbWKBQKvL29G219JIQgJSWF6OhoDhw4QFVVFZGRkbzxxhuEhIToZ+CTJ09m+fLlf2b/8H2j3QX1kpKSmDx5MqATJXnkkUdYsWIFubm5TJs2jVu3buHt7c3/+3//r9nBsrZKcXEx27ZtY/v27QwZMoTnnnsOb2/v+3oNlTKPFWPX6X2VfSf1wkRqysn9OhUaW5kNs96IZMtTu/THSJwcsLSQsnTLTN6cvb3O+YJ6+XH13E0cFbbk35aY9O6k4OavuuwFn65epF7PxNbFAY1aQ2V5lb7Ltm+wGykJGUjNTbC01eX9KjwdUV29RfCw7lz5tWFQzdXTkcwE3bltHDpQWVKJuqoaZ185udmG83+DenhzNd5wubNviIKUK4bjD75BrqRczcTZxQ6Fqy2l2QXcupCERq3B0toCTbVG59IxQMdQb5J+MXzNkAGBXIm7k1poaW1B2NAu9B7Xg/DRodg30jn6btSI9qSnp1NQUIBEIkGhUODv799oZoQh5bSoqCiioqLanXJaG+OPGdTr2LEjFy9ebLDd2dmZw4cPt8KIWg5bW1sWL17MwoUL+frrr3nqqadwc3Nj0aJFRERE3JdryD2dmL9pDm/M+CeOLvbMWDYBe7kdl+KuU1JQxqhZAxk1eShf/f0HslJydNoYEgm9JwRg4QjObvbk3hYNMjE1IT05B7mnIyrlnSCY9Lbqm8zdkdRrmfiE6NwVQeG+XD3bsFtyYA9frpxJ0b2pKkOj1lBRYti4Ojp10Ie2vANduXJS52d2Vtg2apAlJo3/NuoXsoDOp+7pJ8PGwgRXWxPSzl2lvsmuKK0koKcfN843vB8AS5vG82szkjOxk9nQa6xOujL0LyGYN1HK3RQ1oj3Z2dmo1WqcnZ3x9/fH1taW6upqMjMziY+Px8bGBhsbG7y9vdFoNMTFxbFv3z7i4uIICgpqN8ppqampzJ49m8zMTExMTHj66af561//2uy6hB07dvD3v/8dgJUrVzJnzpwHfQt1aHcz5JZAo9EQERGBh4cH0dHRJCcnM2PGDPLy8ujZsyc7d+68L8pt94MTJ06wfv16srOzmT9/PuPGjbsv3RB+OZ5AcV4JAyb1AiD2y1P8d8N+/nHw/ygoKuA/q77hzN7LYGmJtcyGRR9OxNramhO7r3Jwx1Ir4XIAACAASURBVAngjpshuJefvnjEw0+G8nZubdf+gVw+nURw/04knLuJT2cXbt5WcvMOdOHW9SwcXewoLa6kqkJNp24eXDuu8107+ziTr65rpCw7mENZKRUllUgkujzomodDSN8ArvxiuJDDxcuRrFTDWRM6pTWBxESCs4sVFhKB6noWxSrdeRsrPwYI7tuJhJP/M7jP0dW+QdGGd7AHvSPDiBjTHVuPDqSlpWFpaYmXl1ejKmn1EUJQWlqKSqUiJycHExMTZDIZCoWiUX9uTX7wyy+/zNmzZ1Gr1YwZM4aHH36YoUOHtqvijIyMDDIyMujZsyfFxcWEh4fzzTff8Mknn9y1LiEvL4+IiAjOnj2LRCIhPDycc+fOtVRB2R9zhtwSbNq0ieDgYIqKdFkAy5Yt44UXXmDGjBk8++yzbNu2jeeee66VR6mjX79+7N69m6SkJDZt2sS6deuYPXs2s2bNwsbG5u4naITQwcGAbslaWFiIRw8nIl/oR2JyIjKZjKFTB3Bm72UkZqY8uWIivXv3pqCgAI8ud/zHkpqW7FV3lu1WVneMaE5mIRJTE9KScnBU2HHr2p1jzW5rUcjdHcm/mIqZhZScpDtpdwXpBZi6udTpfefbScbVOF3KWMeuniTWaltVUtB4YE+VXoDUzFTvKqnB3tkaNw8HzEU1l2Ivk5bYUAXNvEPjD2bl9YxGszTyMwvxDvbATmar8+VHhuHqVzfw7OHhQWFhIampqVy/fl2vLVw/vVEIUUfEvUOHDsjlcsLCwpr0B2dlZRETE8P+/fv1GUpPPvkk8fHxfPnll6xZs6ZdGWNAX00IuhVlcHAwaWlp7N27l9jYWADmzJnDkCFDGhjkQ4cOMXLkSL1rc+TIkRw8eJCZM2c+0HuozZ9+hqxUKpkzZw4rVqxgw4YN7Nu3D7lcTmZmJlKplBMnTrBq1SoOHTrU2kM1SEFBAR9++CH/+c9/GD16NM8888w9t6upyT2tnessl8txcnLSz74ry6t4rNMSHH1c+VfcyjrHPzNgDWXF5VRUVCO0AktrC8pLKnHxciLrRuZtXV0XMlJy8evqSfL/sgnp5acvGvH0l6NMVOEX4q7XPg7p6cWlw7/WuY5Hn2Ayas1sHWwkFGTo/NT+XdxJrKUUZ25phloqRTTSTs4rQEHqjWzcvJ1wdLCkIFWF8tIthBB0GdBZ32WlPlb2lpQVVTT6S/Dt6knKpTvjsLSxoOfwbvSKDCNidHdsHRsvLa5NVVUVaWlpZGZm4ujoiLu7u76nXFFRkcG/UX1qlNOio6M5ePAgEomECRMmMGnSJDp16lRnBq7RaNpN37nGSElJYfDgwVy6dAlvb28KCu6kODo6OpKfX3dV9M4771BRUcHKlbrv85o1a+jQoQP/93//1xLDM86Qm8PixYt56623KC7W/bBzc3NxcHDQz0o8PT1bvAv178HBwYGXXnqJF154gS+//JJZs2bh7+/PwoUL6d69u8Fjapa5NUZYq9Uik8nw8fHB1tbW4FLZooM5nXv7M+HZEQ329Y/szo1flVw5k4yrrxOZKbpZq62dBZm3H/iOCnsyUnKxvN2iKq/W8r2DjSUmpiZ6xTUnhR3Xfr7a4Dp2dpbUzJl9Oim4eV5n0O3lViReqpvdUFWhRhHgRHZ6XW0KU6kJ3v5ynB0tKU6sQvnzJernRWTfMlyyDFBWWIFbgJyMG4YzOaztrXByc9CnpnUbFIRZE3oWjVGTGWFubo5SqSQ9PR0zMzM8PT0JCgpq1HgaUk6Liopiz549KBSKRt0g7d0Yl5SUMHXqVDZu3NjsjCRDk9HWDlr+qQ1ydHQ0CoWC8PBw/fKmLf6RmoOZmRmPPvoojzzyCMePH2ft2rWUlpayYMECRo0aRWVlJfn5+RQVFZGfn4+VlZXB/n5NMW/tdLw7N5x994vszpkjCQA4KRzITMnDUWHDjds6GRITCWlJ2UhMTVAmqXD3lZGerDNorj7O3PhFSXAvP33ZspOTJTnXGuo0mEru/G0sTGupiMmsKcxqGMBzltmSnV6Eta0lnr5OaErLuXUhiRspSqq7eJKbatjwqlJzcfNXkJFoONvCycWxgUH2DHKj3/hw+ozviX+Yz2/+ztT2BwshkMvldO3aFWtra0pLS1Eqlfq8ewcHB5ycnKisrOTYsWNER0dz8uRJevTo0W6U05588kn97/DSpUsATJ8+XS97UFBQgIODAxcuXGhwbI3ImImJCSkpKbz22mtMmTIFoFl1CZ6envrfPehWy0OGDLn/N3kP/KldFi+//DI7d+5EKpVSUVFBUVERkydP5tChQ+3GZdEUP//8MytWrODatWtYWFiwatUqhg0bhqOj433vbHLuaALR23+kvLSS4vwyFK62XDymm+W6dZKRmVyAR5ALacn5hPTy5cpttbbAHt6kJ+egFVBWXIFfkCuJJxrOjgEC+gSSlFqEjb0lpRl5CI0Wqbkp5mamlNUre7aTWeHm60BJdhlpl26hqSfSb2IiwdrRmuJGmph2GdiZyz8ZdlvIPJ0oyCokuF8gfcaFETK4ExWUUVRUhIeHh0G/b2MIISgqKtIXaVhYWKBQKJDJZI0+KDUaDTdu3GDWrFkIIaisrGTs2LFMmTKFQYMGNepHboscP34cGxsbZs+erTfItVm6dCn29va8+uqrDfb5+vpy5swZli5dipOTExs3btTve/HFF3F2dtYH9fLy8njrrbfqHJ+Xl0d4eDjnz58HoGfPnpw7d66l0mX/uOJCLUFsbCzvvPMO0dHRPPzww0ydOlUf1AsNDWX+/PmtPcR7Ii8vj4ceeojIyEj69+/P4cOH+fLLL5kwYQLz5s3DxaXpAoPfy8Xj13h95vsABPcN4OrZFHx7eJFyJRMbhw6U5JfrcozTC+gc7svVczcxlZrgbG9GZiPawtYya8rNbQkIlnPjtkEPCvfh6plkJCYSvAIUWFubo0rJIisxC2sHK0oLyxr91gb360TCCcNZEe7+LqQn1h2HlV0Heo7Q+YPDR3bDxqGuP1itVpOWlkZGRgaOjo54eXkZlKOsEe3Jzs6msLAQOzs75HI5zs7OTfqDa5TT9u/fT3FxMaNGjaJjx458//335OfnExMT0y5Wc/VJSUlh/PjxDQyyEAJvb2+OHDlCp04NNaB9fX3ZvHkzEyZMoFu3bvpJxptvvkmfPn0M1iWcPXuWrVu38tFHHwHw8ccf8+abbwKwYsUKnnjiiZa6TaNBvhdqG+SkpCR92luPHj34z3/+0+6iz4aorKzk888/54MPPqBr164sWLCAkJCQFrvelpd2Ebv7NGYdLKisUGPlbEfvEcHc+OUWt65l07G7G4PH9yCkVwAxn8aRk5bHL981zDGvjX2IH6YVFfou2wFd3DBFoLx0iyID7ez9unmR/KthSc2mUtgAnD2ckEi4nRXRg66DOiM1u/vMVwiBSqVCqdR5p728vPTtjLKzsykvL8fR0RGFQoG9vX2jqxWtVktCQoI+KGdtba0Pyvn6+tYxvu1ZvrIxg3z8+HGWLFnC2bNnDR7n5+eHo6NjHZGxNozRIBsxjBCCw4cPs379eoQQLFiwgGHDht332VV5SQUbF+3kYtz/COkbwISn/kKPwbrOKVfPp2BmLaG4PB+NRoOLiwvRW2I59O+4OyfQ3k6RqGWwwif3If4HXW6yg4M5JWk5mHcwpzjPcJpbSP9Arvx83eA+UzNTzDuYUV5U193RMdSbXpFh9B3fE79uv70ysry8HKVSSUZGBmq1GkdHR/z8/JrMMa6urubkyZPs37+fY8eO0bFjRyZOnMiECRNwdnb+zWNpyzRmkJ977jkCAgJYunSpwePS09PriIy99957DB48+EEM+bdgNMitQUVFBYMHD6ayspLq6moeeughXn/99TZbbHL58mU2bNjAL7/8wlNPPcW0adPu62pAXVmNmUXDWWWN77RGZay6uprC7BI+XrDvTh5vda0yZBMTvILccXS15/LFdDRl5VBSglajJWRAZxJOGpbldFDYUZBtuAsIgGdXVzKuqeg6sDN9xvWg19geyD1/mw9RCEFxcbE+KGdmZqYXcZdKpaSnp5Oeno6dnR0eHh7Y29sDuu4bR44cITo6mvPnz9O7d28mTZrEyJEj24VYj6HA3KpVq/j3v/+t7yn55ptvEhkZ2eDYgwcPMn/+fJRKJatXr2b58uWA7sHk4eHBuXPn8PT0vOsYVq1ahY2NTUulrN0PjAa5NahJKbOxsUGtVjNw4EA2bdrEhg0bmDJlit4v3b179zZTbAI6caP333+fr7/+mqlTpzJ37tz7PiPTaDR6befCwkJsbW31vlOpVIparebVqe9y9WSyTj1OUzcQ5+7vgoOLHWXFFSSduyPF6d/Dl6RfDLslQFcRdyuhbuqitb0V4aNCCR/TFXtfayqqy3F3d8fd3b3ZATnQuRXy8/P1oj02NjbI5XJkMpnB89Sopm3bto2vvvqKDh06UFlZyYgRI4iKiqJ///73dP22gKHAXHMMpEajITAwkO3bt/Pcc88hlUr54osvCAkJ4eDBg6xdu5Zjx44ZPLa+yNjIkSN59dVXGTNmTIvc433AmIfcGkgkEn3FnFqtRq1WI5FIOHLkCJ9//jmgqxxatWpVmzLILi4urF69mpdffplPP/2UqKgoIiIiWLBggcGASnOp0VZQqVRUVlbi7OyMu7s7wcHBDZbtZmZmTJw3gqsn/613V1hYmVNZpquYS0/MQmICecq6amw3L6ViZduBslqqb7WxddIF1uTezvSJ7EGvsWF0GRBYxx+sVqtJT0/n7NmzODg4NBqQA93srUbEvaSkBEdHR+RyOYGBgY36gw0pp40ZMwaVSkV8fDxjx45ty8vtJhk8eDApKSn3fNzp06cpKytj+vTp5OTkYGVlxSuvvMKePXvYtWtXg4q59PR0nnrqKWJiYsjKymogMtaGjXGzMc6QWwCNRkN4eDg3btxgwYIFvPjii/Tt25cbN3TL6tTUVMaOHWswzaetoNVqiYmJYePGjVhaWrJo0SIGDhx4Vz9zjdZuzbLd1NQUmUyGXC7HysrqrtetVmuYG7qcYlUhQ6b3Y/Ybk1k2fB3Zybm4+DlTlFmoy5yoR3D/QK6eaihg7x/mw8CpvQkb2qVZ/mAhBDk5Ody6dQsTExO8vLxwdnau82BRq9X6e2qskAZ0/4cXL14kOjqaH374AScnJyZOnNhAOa20tJTKysp2rU5Y3w+8atUqPvnkE+zs7IiIiGD9+vUNNCJ2797NwYMH9RkPO3fu5NSpU2zevPmBj/8BYJwhtxampqZcuHCBgoICJk+eTEJCQoPPtPX0JBMTE8aPH8/48eO5cOEC69ev57XXXuOZZ55hypQpdXJd6y/bra2tkcvl9OjR455zYqVmpjy8ZCzBvf3x7+4DwPh5Izi47Sg5qSoqSioNHld9Wz9Dai6l26AgfT85Z/d7E4qp6Rsnk8lQqVQkJiZy8eJFLC0t8fDwIDg4uEm/blVVFXFxcURHRxMXF0dwcDBRUVFNKqdZW1s32a25PfLcc8/xyiuvIJFIeOWVV1i6dCkff/xxnc+01yKslsRokFsQBwcHhgwZwsmTJykoKKC6uhqpVIpSqbxnvYnWJCwsjJ07d5KWlsZ7773HoEGDmDBhAlZWVmi1Wvr374+TkxMKhaLJZXtzGT9vWJ33Q2f044cdsY0aYxtHa7yDPJj810h6juhKB9vfFgirEVbKzs7WVzN6e3tjb29PdnY2GRkZVFZW4uXlVWe2X1xczA8//EB0dDSXLl1iwIABTJo0iY0bN7b5dElDAbkXX3yRffv2YW5ujr+/P9u3bzf4MKmplDM1NUVTz99fO8993rx5jB8/vsHxnp6epKbe8f23t99FS2B0WTQDIUSzn9wqla4LhoODA+Xl5YwaNYply5axY8eOdl9sArBlyxY+++wzMjIy9GW9L7zwAn5+fi16XZUyl1fG/YOM28UaLr5yQocH4x4qx7ubOz5+PshksnueYdUEGrOzsykuLsbe3h6FQoGTk1ODB4tWq0WlUnHp0iXeeustwsPDSUhIICcnh1GjRjFp0iR69ep136sgWxJDAbnvvvuOYcOGIZVKWbZsGUADpTSo2/29vsuipmwZ4N133+XUqVPs2rWrzvHV1dUEBgZy+PBhPDw86NWrF59//jldunRpyVtuLYwui/vF/PnzcXZ25tVXX71rqlpGRgZz5sxBo9Gg1WqZNm0a48ePJyQkhBkzZrBy5Up69OjB3LlzH9Do7y9BQUF89dVXKBQKNBoNe/fuZf78+Tg5OfH888/Tp0+fFrmu3NOZNw/9jaOfxxExujs+Xe6kQpWWlnLr1i0SExPx8PDA3d29SbGcqqoqvYh7TaDRy8sLOzu7Rg26EIIbN27og3KWlpacOHGCsrIyXn75ZR555JH7fs8PAkMBuVGjRulf9+3bl927dzd5jpkzZxIbG0tOTg6enp68/vrrxMbGcuHCBSQSCb6+vvzrX/8C6gbmpFIpmzdvZvTo0Wg0Gp588sk/qjFuNsYZ8l0oLy/H2tqaQYMGERsb+6f3cTXG6dOnWb9+PUqlkvnz5zNhwoQHnr5Vu3RZJpPh5eWlr16rEe1RqVRIJBK9iHtTgcbGlNMmTpyoV05TqVRcvny51UVpfg+NFWYATJgwgenTpzNr1qwG+9pZpVxrY5wh3w+2bt2Kv78/Dz30kN4YnzlzhgULFrBhwwYGDhzYotf/vS1qHhS9e/fmv//9Lzdv3mTTpk289dZbPPbYYzz22GMPTHHMzMwMX19fvL29ycrKIj4+Hu3t9LmaQGP37t2bXOVUVFRw7Ngx9u/f3yzlNLlc3q6NcVO88cYbSKVSHn30UYP74+Li6lTKBQUFtdvUvbZC+3F2tQLV1dWsWbOGDz74gIoKXXnt119/zfvvv8+FCxcMZk/cb6RSKevXrychIYGTJ0/y/vvvc+XKFdatW8fw4cP53//+x/Dhw1m3bl2Lj6U5+Pj4sGHDBn788UckEgkjR45kxYoVel2HlkSj0aBSqbh69SopKSnY2dnh7u6uL9IxNTU1OGsvKChg165dPPbYYwwZMoSjR4/yyCOPEB8fz86dO3nooYfarIzlk08+iUKhoGvXrvpteXl5jBw5kk6dOjFy5MgGwuw17Nixg06dOjFkyJA6Yu41+6Kjo/nss88aXRXWBOAUCgWTJ0/m9OnT9+mu/rwYXRZNsGvXLs6fP09UVBQ7d+7ktddeY8aMGYwfP57Y2Fg+/fRTnJ2d7yno93uJiopi4cKFLFy4kNjYWL3e65AhQ/Qasm2J6upq9uzZwz//+U88PT15/vnn6dGjx307f1VVlb5Io7y8HCcnJ+RyeQO9iIqKClJTU8nKyuLo0aNMnDiREydOsH//fkpKShgzZgyTJk2ie/fu7T4o99JLL91TP7nU1FR69epFVlYWjo6OHDx4kCVLlnDs2DF96XN92mGlXGtjLJ3+vfj4+HDq1Ck+/fRTEhISCA4Opry8HGdnZ9LS0li7du0DNcb32qKmLSGE4Oeff+add94hPz+fBQsWMHbs2N9k/MrLy8nOzq7T7UQul2Ntbd1kkUZCQgLffvstsbGxXLlyhaCgIF5//XWGDx/ermMD9X3AnTt3vuvD+osvviA2NpaioiJiY2P1xvitt95i7dq1+mAn6AJ7W7durROQS0pKalApt2LFigd74+0Low/59/Djjz/i4eGBq6srHTp04PDhwzzyyCNYWVmxYcMGVq1aBTSdElfjv7wfM67f0qKmLSGRSBgwYAADBgwgMTGRd999lzfeeIPHH3+cWbNmNRlcqxHtqRFxNzMz0y/Tm8rzrVFOi46O5vjx43Ts2JGoqCgWL16Mo6Mjhw4d4scff2TEiIZtqdozWVlZ+pQzNzc3srMbdj5JS0vDy8urQT+5uXPnNpoB5O7uTkxMDAAdO3bk4sWmpVKN3DtGg9wIRUVFvP++TmB90KBB9OzZkwEDBvD2228jl8vp1q0bcMfY1hjmmJgYzMzMGDhw4H1T6lKr1UydOpVHH330nlrUtFX8/f3ZvHkz+fn5/Otf/2LYsGGMHTuWp59+Wm9Iaqr/srOzKSgo0AsR+fr6Npm9YUg5bfLkyfzjH/9o8PcYO3YsY8eObdF7basYq+TaJkaD3Ajjxo3Tvw4LCwPg0qVL/PTTTyxZsgTQGY0agyyRSKioqODo0aMcPXoUhUKBq6trgxr+2sc0ByEEc+fOJTg4WH9dgIkTJ7Jjxw6WL1/Ojh07iIqK+l332xo4OjqyfPlylixZwn//+1+mTZuGvb09Wq2Wfv368fDDD+Pi4kLnzp2bFO3Jzc3lwIED7N+/n9TUVIYNG8a8efPo379/u2reee3aNaZPn65/n5SUxOrVq1m8eLF+W2xsLFFRUfpCnClTpjB79uw652mv/eSMGLMsmo0Qgry8PKytrfnLX/4C1J0dgy4NKCMjg6ioKGJiYhBCsGfPnjrnqTmmxp1xN+Li4ti5cydHjhwhLCyMsLAwYmJiWL58Od9//z2dOnXi+++/1+vItkdMTU3ZtWsXZmZmeHt7Y2Njw5kzZ7h165bB/n9CCJKTk3nvvfeIjIxkxowZqFQq1q5dy/nz51m/fj2DBg1qV8YYdL7fCxcucOHCBc6dO4eVlZXeT1ubQYMG6T9nqNdczcMaaPRhPXr0aL777jvy8/PJz8/nu+++Y/To0fc8ZiEEGo3G4IzbyG9ACHEv//70VFZWCiGEqK6urrNdq9WKN998U6xZs0akp6cLIYRYuHChWLp0qRBCiPj4ePH222+LnTt3NjinVqtt4VG3fbKysuq8T0hIEE8//bQIDw8XH3zwgVCpVOKnn34Sy5YtExEREWLUqFFi8+bNQqlU/iH//w4dOiT69+/fYPvRo0fFuHHj9O9nzJghXF1dhVQqFR4eHuKjjz4SOTk5YtiwYSIgIEAMGzZM5ObmCiGEOHPmjJg7d67+2G3btgl/f3/h7+8vPv7442aPLS8vT8ybN6/BdqVSKVJTU+/lNv9MNMvGGrMs7oH67oYaQRVTU1N+/fVXli1bRmhoKOvWraOoqIhXXnmFoUOHUlpaykcffcTQoUM5deoUpqambNiwgYCAgDrnhvsTAPwjoVKp+OCDD9i0aRPDhw9n8uTJREZGNqqc9kfhySefpGfPnixcuLDO9tjYWKZOnYqnpyfu7u688847D6zcWKPR6FcdZmZmnD9/nm7durFr1y62bNlCQUEBffr0YejQocycOfOe3XN/cJrnoG+u5RbGGbJQKpVizZo14ttvv22wb9OmTWLOnDni5ZdfFkII8fe//13Mnz9ffPLJJ2LRokVi9+7d+s/Gx8eL8vJy8fPPP4u1a9eKxMTEB3YPNTzxxBNCLpeLLl266Lfl5uaKESNGiICAADFixAiRl5f3wMfVGBqNprWH8MCorKwUzs7OIjMzs8G+wsJCUVxcLIQQYv/+/SIgIKDFxnHt2jWxevVqoVQq62wvKSkRw4cPF6+88ooQQohjx46JCxcuCCGEWLlypQgNDW2xMbVjmmVjjY+ve8DDw4PHH3+cffv2MWzYMD788EMAbt26xbVr1+jXrx9lZWUEBgZy6tQpnnjiCYKCgqisrNQXQ1RXVxMWFoalpSXe3t7Y2dkxffp0li9f3kDCsCV5/PHHOXjwYJ1tbbX6D/5cK4cDBw7Qs2fPOhKWNdjZ2ek70kRGRqJWq8nJyWmRcRQWFnLz5k3Onz8PwIkTJ+jcuTN//etfsbCwIDo6GtD5tE+ePElYWBgJCQnk5uYSHx/fImP6o/Pn+ZbfJzw9Pfnwww/5+uuvKSoq4rHHHiMxMRGFQsGwYcPYuHEjJ06cYMeOHURERODs7MyVK1fo2LEjcCe1KD4+nm+++YaePXuyfft21Gp1swN994PBgwc36FCxd+9e5syZA+jaTH3zzTcPbDx/BHx9fenWrRthYWFEREQ02C+E4PnnnycgIIDQ0FC9oavPF1980aB9UQ2ZmZn6ANrp06fRarW/qfdhzTkOHz6slwCo//3z9/cnICCACxcuADpZzsWLF/PRRx/x2muvcePGDdLS0qiqquLIkSNs3bqV3bt36xUBjfwGmjuVFkaXhUFqL6UNLauLiorEvHnzxNChQ/Vui7S0NCGXy8W7774rIiMjhb+/v3j44YdFQkLCAxu3EEIkJyfXcVnY29vX2e/g4PBAx9Pe8fHxESqVqtH9+/fvF2PGjBFarVacOHFC9O7du8FnSktLhZOTkygoKNBv27Jli9iyZYsQQoj33ntPhISEiNDQUNGnTx8RFxd3T2Os/R3du3evkEgkYsmSJUIIw8HlL774Qh/AGzRokDhw4IB+3+jRo8V7770nlEqlmD59ujh06JBITk4WAwcOFIMHDxZqtfqexvYHp1k21piH/DupvZQ2tKy2tbXlww8/5KuvvuL48eP06tWLo0ePMnDgQBYvXszixYtZtWoV+fn5BAUFPcihG3nA7N27l9mzZyORSOjbty8FBQV1hNwBrKysyM2t28T12Wef1b+u0TG5VyoqKhg4cCDjx4/XV5mam5vrXVRlZWUGqyX9/PwQQpCYmMigQYP45ptv9HoV3t7efPvttyxcuJDx48ezaNEifHx8mDdvHhEREe2ue3ZbwOiyaGFqloFTpkxh48aNeHt7ExYWxrVr11izZg2LFy/mxx9/ZOjQoa080jsFBUC7q/5rC0gkEkaNGkV4eLg+vlCbmnLlGjw9PUlLS3sgY7O0tCQ5OZlPPvmEy5cvA3Dx4kWGDh2KTCbj2LFjQEO3hY+PDy4uLhw9epS5c+dy9uxZvv32Ww4ePIiDgwOHDx+moqKCWbNmcfToUb777jtmz55NSEjIA7mvPxpGg9zCGCoE6d69OzExMVhZWWFiYoKtre1vSsq/3zSnoMBI48TFxXH+/HkOHDjA+++/z/Hjx+vsF61crOQbXQAAA+xJREFUrrxmzRrkcjnHjx8nPj4eDw8PkpKSGDhwIEeOHDF4jIuLC4GBgZw9exYfHx+2bt3Kxx9/zKZNm5g+fTrZ2dlYWlqi0Wj0cpxardZYKPIbMRrkB0Rtd4ZWq8XHx4elS5fy9ttv64VdHiQzZ86kX79+XLt2DU9PT7Zt2/aHqv67F1JTUxk6dCjBwcF06dKFTZs2NfhMbGws9vb2+mrJ1atXN/jM3fSBW7up5/jx46murqZ379787W9/w9XVlc6dO+Pu7s7169d1aVf13G4SiQQPDw+kUimJiYlERESwZ88eDhw4QHh4uF5+tnZVpImJiVEX47fSXGezMAb17itarfYPWWHWHklPTxfnzp0TQuiCsJ06dRKXL1+u85n6FXL1KSkpEUVFRfrX/fr1qxMAE0KI6OjoOkG9Xr163ec7uTtOTk5CCCFmz54tPD09xYEDB8TNmzfFww8/LE6cOCGEuBP4q/l+lpeX64+v2abRaP5UueH3AWNQry1jnEG0Hdzc3PSBNVtbW4KDg0lLS7snP2hWVlYDfeAxY8awdetWQBeYi4yMJCYmhoCAAKysrNi+ffv9v5m7MHPmTPbs2cOWLVtYuXIljo6OuLq6IpfLiYmJoW/fvvp8+JrZck1fQlFLavbPlBf+QGmu5RbGGbKRe+TAgQMiMDBQ+Pv7i7Vr17b2cJpFcnKy8PLyEoWFhXW2Hz16VDg5OYnQ0FAxZswYcenSpVYa4e/j4sWLYsSIEUIIoU+tq66uFtevXxfZ2dl1PltdXS2io6PF6tWrRUZGxgMf6x+MFtGyMGKkWUgkElPgOjASUAJngJlCiCutOrAmkEgkNsAx4A0hxFf19tkBWiFEiUQiiQQ2CSE6tcY4fw+3/y4qIYRTI/ulwATgMcAd+B74DLgmjMaixTG6LIy0FL2BG0KIJACJRLILiALapEGWSCRmwB7gs/rGGEAIUVTrdYxEIvlAIpHIhBAtU7fcQgghNBKJRNbER9wBDfACcMtohB8sRoNspKXwAFJrvVcCfVppLE0i0TlGtwEJQogNjXzGFcgSQgiJRNIbXYZSrqHPtnWEEFqJRGIihGhQqy+EuAXcaoVhGcFokI20HIailm11tjUA3RL9V4lEcuH2tr8B3gBCiK3AQ8BzEomkGigHZrTn2aMhY2yk9TEaZCMthRLwqvXeE0hvpbE0iRDiJ+6iVyuE2AxsfjAjMvJnxZi7YqSlOAN0kkgkfhKJxByYAXzbymMyYqRNY5whG2kRhBDVEolkIXAIMAU+FkJcbuVhGTHSpjGmvRkxYsRIG8HosjBixIiRNoLRIBsxYsRIG8FokI0YMWKkjfD/AR4RGjSkblDMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1f099eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "x = np.linspace(1, 20, 20)\n",
    "y = np.linspace(1, 50, 50)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "print(acc_array.shape)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, acc_array, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Accuracy varying M_pca & M_lda');\n",
    "ax.set_xlabel('M_lda')\n",
    "ax.set_ylabel('M_pca')\n",
    "ax.set_zlabel('Accuracy');\n",
    "\n",
    "ax.view_init(30, 230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=90, size=6)\n",
    "        plt.yticks(tick_marks, target_names, size=6)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     size=3,\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     size=3,\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'n_comps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c7961976aa51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_comps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mM_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstandard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mW_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'n_comps'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "_card = 52\n",
    "D, N = X_train.shape\n",
    "\n",
    "M_pca = 147\n",
    "M_lda = 46\n",
    "\n",
    "standard = False\n",
    "\n",
    "pca = PCA(n_comps=M_pca, standard=standard)\n",
    "W_train = pca.fit(X_train)\n",
    "        \n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=M_lda)\n",
    "W_train_2 = lda.fit_transform(W_train.T, y_train.T.ravel())\n",
    "\n",
    "nn = KNeighborsClassifier(n_neighbors=1)\n",
    "nn.fit(W_train_2, y_train.T.ravel())\n",
    "\n",
    "W_test = pca.transform(X_test)\n",
    "\n",
    "W_test_2 = lda.transform(W_test.T)\n",
    "\n",
    "acc = nn.score(W_test_2, y_test.T.ravel())\n",
    "\n",
    "print('M_pca = ', M_pca, ', M_lda = ', M_lda,' --->  Accuracy = %.2f%%' % (acc * 100))\n",
    "\n",
    "y_hat = nn.predict(W_test_2)\n",
    "\n",
    "cfn_matrix = confusion_matrix(y_test.T, y_hat)\n",
    "\n",
    "class_names = np.arange(1,53)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plot_confusion_matrix(cm           = cfn_matrix, \n",
    "                      normalize    = False,\n",
    "                      target_names = class_names,\n",
    "                      title        = \"Confusion Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a234b0c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prettify plots\n",
    "plt.rcParams['figure.figsize'] = [12.0, 9.0]\n",
    "sns.set_palette(sns.color_palette(\"muted\"))\n",
    "_palette = sns.color_palette(\"muted\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "\n",
    "done = {'success': False, 'failure': False}\n",
    "first_failure = True\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "\n",
    "for y, t, w, x in zip(y_hat, y_test.T.ravel(), W_test.T, X_test.T):\n",
    "    if y == t and done['success'] is False:\n",
    "        x_hat = pca.reconstruct(w)\n",
    "        axes[0].imshow(x.reshape(46,56).T,\n",
    "                       cmap=plt.get_cmap('gray'))\n",
    "        axes[0].set_title(\n",
    "            'Successful NN Classification\\nPredicted Class: %d, True Class: %d' % (y, t), color=_palette[1])\n",
    "        done['success'] = True\n",
    "    elif y != t and done['failure'] is False and first_failure is True:\n",
    "        first_failure = False\n",
    "    elif y != t and done['failure'] is False and first_failure is False:\n",
    "        x_hat = pca.reconstruct(w)\n",
    "        axes[1].imshow(x.reshape(46,56).T,\n",
    "                       cmap=plt.get_cmap('gray'))\n",
    "        axes[1].set_title(\n",
    "            'Failed NN Classification\\nPredicted Class: %d, True Class: %d' % (y, t), color=_palette[2])\n",
    "        done['failure'] = True\n",
    "    #elif done['failure'] is True and done['success'] is True:\n",
    "     #break\n",
    "\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  1 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  2  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  3  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  4  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  5  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  6  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  7  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  8  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  9  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  10  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  11  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  12  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  13  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  14  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  15  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  16  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  17  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  18  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  19  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  20  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  21  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  22  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  23  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  24  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  25  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  26  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  27  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  28  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  29  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  30  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  31  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  32  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  33  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  34  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  35  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  36  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  37  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  38  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  39  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  40  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  41  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  42  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  43  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  44  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  45  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  46  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  47  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  48  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  49  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  50  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  51  --->  Accuracy = 4.81%\n",
      "M_pca =  2 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  2 , M_lda =  2  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  3  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  4  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  5  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  6  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  7  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  8  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  9  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  10  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  11  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  12  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  13  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  14  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  15  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  16  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  17  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  18  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  19  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  20  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  21  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  22  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  23  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  24  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  25  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  26  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  27  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  28  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  29  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  30  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  31  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  32  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  33  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  34  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  35  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  36  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  37  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  38  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  39  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  40  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  41  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  42  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  43  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  44  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  45  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  46  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  47  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  48  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  49  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  50  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  51  --->  Accuracy = 9.62%\n",
      "M_pca =  3 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  3 , M_lda =  2  --->  Accuracy = 5.77%\n",
      "M_pca =  3 , M_lda =  3  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  4  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  5  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  6  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  7  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  8  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  9  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  10  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  11  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  12  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  13  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  14  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  15  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  16  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  17  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  18  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  19  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  20  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  21  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  22  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  23  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  24  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  25  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  26  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  27  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  28  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  29  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  30  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  31  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  32  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  33  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  34  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  35  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  36  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  37  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  38  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  39  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  40  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  41  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  42  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  43  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  44  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  45  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  46  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  47  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  48  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  49  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  50  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  51  --->  Accuracy = 17.31%\n",
      "M_pca =  4 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  4 , M_lda =  2  --->  Accuracy = 6.73%\n",
      "M_pca =  4 , M_lda =  3  --->  Accuracy = 18.27%\n",
      "M_pca =  4 , M_lda =  4  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  5  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  6  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  7  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  8  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  9  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  10  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  11  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  12  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  13  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  14  --->  Accuracy = 25.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  4 , M_lda =  15  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  16  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  17  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  18  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  19  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  20  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  21  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  22  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  23  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  24  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  25  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  26  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  27  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  28  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  29  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  30  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  31  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  32  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  33  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  34  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  35  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  36  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  37  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  38  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  39  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  40  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  41  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  42  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  43  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  44  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  45  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  46  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  47  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  48  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  49  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  50  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  51  --->  Accuracy = 25.00%\n",
      "M_pca =  5 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  5 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  5 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  5 , M_lda =  4  --->  Accuracy = 25.96%\n",
      "M_pca =  5 , M_lda =  5  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  7  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  8  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  9  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  10  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  11  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  12  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  13  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  14  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  15  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  16  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  17  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  18  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  19  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  20  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  21  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  22  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  23  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  24  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  25  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  26  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  27  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  28  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  29  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  30  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  31  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  32  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  33  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  34  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  35  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  36  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  37  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  38  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  39  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  40  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  41  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  42  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  43  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  44  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  45  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  46  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  47  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  48  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  49  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  50  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  51  --->  Accuracy = 35.58%\n",
      "M_pca =  6 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  6 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  6 , M_lda =  3  --->  Accuracy = 20.19%\n",
      "M_pca =  6 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  6 , M_lda =  5  --->  Accuracy = 35.58%\n",
      "M_pca =  6 , M_lda =  6  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  7  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  8  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  9  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  10  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  11  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  12  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  13  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  14  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  15  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  16  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  17  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  18  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  19  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  20  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  21  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  22  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  23  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  24  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  25  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  26  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  27  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  28  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  29  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  30  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  31  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  32  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  33  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  34  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  35  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  36  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  37  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  38  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  39  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  40  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  41  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  42  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  43  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  44  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  45  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  46  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  47  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  48  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  49  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  50  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  51  --->  Accuracy = 41.35%\n",
      "M_pca =  7 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  7 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  7 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  7 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  7 , M_lda =  5  --->  Accuracy = 32.69%\n",
      "M_pca =  7 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  7 , M_lda =  7  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  8  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  9  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  10  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  11  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  12  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  13  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  14  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  15  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  16  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  17  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  18  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  19  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  20  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  21  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  22  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  23  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  24  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  25  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  26  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  27  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  28  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  29  --->  Accuracy = 37.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  7 , M_lda =  30  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  31  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  32  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  33  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  34  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  35  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  36  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  37  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  38  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  39  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  40  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  41  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  42  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  43  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  44  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  45  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  46  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  47  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  48  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  49  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  50  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  51  --->  Accuracy = 37.50%\n",
      "M_pca =  8 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  8 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  8 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  8 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  8 , M_lda =  5  --->  Accuracy = 33.65%\n",
      "M_pca =  8 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  8 , M_lda =  7  --->  Accuracy = 44.23%\n",
      "M_pca =  8 , M_lda =  8  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  9  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  10  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  11  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  12  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  13  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  14  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  15  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  16  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  17  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  18  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  19  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  20  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  21  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  22  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  23  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  24  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  25  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  26  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  27  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  28  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  29  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  30  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  31  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  32  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  33  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  34  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  35  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  36  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  37  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  38  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  39  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  40  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  41  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  42  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  43  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  44  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  45  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  46  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  47  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  48  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  49  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  50  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  51  --->  Accuracy = 42.31%\n",
      "M_pca =  9 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  9 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  9 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  9 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  9 , M_lda =  5  --->  Accuracy = 36.54%\n",
      "M_pca =  9 , M_lda =  6  --->  Accuracy = 38.46%\n",
      "M_pca =  9 , M_lda =  7  --->  Accuracy = 40.38%\n",
      "M_pca =  9 , M_lda =  8  --->  Accuracy = 44.23%\n",
      "M_pca =  9 , M_lda =  9  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  10  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  11  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  12  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  13  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  14  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  15  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  16  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  17  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  18  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  19  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  20  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  21  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  22  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  23  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  24  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  25  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  26  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  27  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  28  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  29  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  30  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  31  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  32  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  33  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  34  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  35  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  36  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  37  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  38  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  39  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  40  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  41  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  42  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  43  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  44  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  45  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  46  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  47  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  48  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  49  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  50  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  51  --->  Accuracy = 48.08%\n",
      "M_pca =  10 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  10 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  10 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  10 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  10 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  10 , M_lda =  6  --->  Accuracy = 42.31%\n",
      "M_pca =  10 , M_lda =  7  --->  Accuracy = 45.19%\n",
      "M_pca =  10 , M_lda =  8  --->  Accuracy = 50.96%\n",
      "M_pca =  10 , M_lda =  9  --->  Accuracy = 49.04%\n",
      "M_pca =  10 , M_lda =  10  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  11  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  12  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  13  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  14  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  15  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  16  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  17  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  18  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  19  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  20  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  21  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  22  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  23  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  24  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  25  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  26  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  27  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  28  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  29  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  30  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  31  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  32  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  33  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  34  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  35  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  36  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  37  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  38  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  39  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  40  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  41  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  42  --->  Accuracy = 52.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  10 , M_lda =  43  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  44  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  45  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  46  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  47  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  48  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  49  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  50  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  51  --->  Accuracy = 52.88%\n",
      "M_pca =  11 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  11 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  11 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  11 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  11 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  11 , M_lda =  6  --->  Accuracy = 39.42%\n",
      "M_pca =  11 , M_lda =  7  --->  Accuracy = 47.12%\n",
      "M_pca =  11 , M_lda =  8  --->  Accuracy = 51.92%\n",
      "M_pca =  11 , M_lda =  9  --->  Accuracy = 50.96%\n",
      "M_pca =  11 , M_lda =  10  --->  Accuracy = 51.92%\n",
      "M_pca =  11 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  12  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  13  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  14  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  15  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  16  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  17  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  18  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  19  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  20  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  21  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  22  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  23  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  24  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  25  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  26  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  27  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  28  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  29  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  30  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  31  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  32  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  33  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  34  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  35  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  36  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  37  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  38  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  39  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  40  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  41  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  42  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  43  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  44  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  45  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  46  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  47  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  48  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  49  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  50  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  51  --->  Accuracy = 56.73%\n",
      "M_pca =  12 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  12 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  12 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  12 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  12 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  12 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  12 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  12 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  12 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  12 , M_lda =  10  --->  Accuracy = 60.58%\n",
      "M_pca =  12 , M_lda =  11  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  12  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  13  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  14  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  15  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  16  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  17  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  18  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  19  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  20  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  21  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  22  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  23  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  24  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  25  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  26  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  27  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  28  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  29  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  30  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  31  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  32  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  33  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  34  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  35  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  36  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  37  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  38  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  39  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  40  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  41  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  42  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  43  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  44  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  45  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  46  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  47  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  48  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  49  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  50  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  51  --->  Accuracy = 59.62%\n",
      "M_pca =  13 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  13 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  13 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  13 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  13 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  13 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  13 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  13 , M_lda =  8  --->  Accuracy = 53.85%\n",
      "M_pca =  13 , M_lda =  9  --->  Accuracy = 53.85%\n",
      "M_pca =  13 , M_lda =  10  --->  Accuracy = 55.77%\n",
      "M_pca =  13 , M_lda =  11  --->  Accuracy = 57.69%\n",
      "M_pca =  13 , M_lda =  12  --->  Accuracy = 58.65%\n",
      "M_pca =  13 , M_lda =  13  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  14  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  15  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  16  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  17  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  18  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  19  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  20  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  21  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  22  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  23  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  24  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  25  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  26  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  27  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  28  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  29  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  30  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  31  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  32  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  33  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  34  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  35  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  36  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  37  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  38  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  39  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  40  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  41  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  42  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  43  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  44  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  45  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  46  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  47  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  48  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  49  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  50  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  51  --->  Accuracy = 63.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  14 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  14 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  14 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  14 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  14 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  14 , M_lda =  6  --->  Accuracy = 43.27%\n",
      "M_pca =  14 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  14 , M_lda =  8  --->  Accuracy = 48.08%\n",
      "M_pca =  14 , M_lda =  9  --->  Accuracy = 51.92%\n",
      "M_pca =  14 , M_lda =  10  --->  Accuracy = 57.69%\n",
      "M_pca =  14 , M_lda =  11  --->  Accuracy = 61.54%\n",
      "M_pca =  14 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  14 , M_lda =  13  --->  Accuracy = 63.46%\n",
      "M_pca =  14 , M_lda =  14  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  15  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  16  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  17  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  18  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  19  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  20  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  21  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  22  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  23  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  24  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  25  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  26  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  27  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  28  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  29  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  30  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  31  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  32  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  33  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  34  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  35  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  36  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  37  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  38  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  39  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  40  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  41  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  42  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  43  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  44  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  45  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  46  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  47  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  48  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  49  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  50  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  51  --->  Accuracy = 64.42%\n",
      "M_pca =  15 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  15 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  15 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  15 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  15 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  15 , M_lda =  6  --->  Accuracy = 46.15%\n",
      "M_pca =  15 , M_lda =  7  --->  Accuracy = 50.96%\n",
      "M_pca =  15 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  15 , M_lda =  9  --->  Accuracy = 52.88%\n",
      "M_pca =  15 , M_lda =  10  --->  Accuracy = 55.77%\n",
      "M_pca =  15 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  15 , M_lda =  12  --->  Accuracy = 60.58%\n",
      "M_pca =  15 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  15 , M_lda =  14  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  15  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  16  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  17  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  18  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  19  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  20  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  21  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  22  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  23  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  24  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  25  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  26  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  27  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  28  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  29  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  30  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  31  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  32  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  33  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  34  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  35  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  36  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  37  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  38  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  39  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  40  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  41  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  42  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  43  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  44  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  45  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  46  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  47  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  48  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  49  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  50  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  51  --->  Accuracy = 65.38%\n",
      "M_pca =  16 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  16 , M_lda =  2  --->  Accuracy = 10.58%\n",
      "M_pca =  16 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  16 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  16 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  16 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  16 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  16 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  16 , M_lda =  9  --->  Accuracy = 54.81%\n",
      "M_pca =  16 , M_lda =  10  --->  Accuracy = 53.85%\n",
      "M_pca =  16 , M_lda =  11  --->  Accuracy = 58.65%\n",
      "M_pca =  16 , M_lda =  12  --->  Accuracy = 57.69%\n",
      "M_pca =  16 , M_lda =  13  --->  Accuracy = 61.54%\n",
      "M_pca =  16 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  15  --->  Accuracy = 67.31%\n",
      "M_pca =  16 , M_lda =  16  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  17  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  18  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  19  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  20  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  21  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  22  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  23  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  24  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  25  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  26  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  27  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  28  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  29  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  30  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  31  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  32  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  33  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  34  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  35  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  36  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  37  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  38  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  39  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  40  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  41  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  42  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  43  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  44  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  45  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  46  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  47  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  48  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  49  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  50  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  51  --->  Accuracy = 66.35%\n",
      "M_pca =  17 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  17 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  17 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  17 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  17 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  17 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  17 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  17 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  17 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  17 , M_lda =  10  --->  Accuracy = 55.77%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  17 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  17 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  17 , M_lda =  13  --->  Accuracy = 62.50%\n",
      "M_pca =  17 , M_lda =  14  --->  Accuracy = 65.38%\n",
      "M_pca =  17 , M_lda =  15  --->  Accuracy = 66.35%\n",
      "M_pca =  17 , M_lda =  16  --->  Accuracy = 65.38%\n",
      "M_pca =  17 , M_lda =  17  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  18  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  19  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  20  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  21  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  22  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  23  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  24  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  25  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  26  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  27  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  28  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  29  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  30  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  31  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  32  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  33  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  34  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  35  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  36  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  37  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  38  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  39  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  40  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  41  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  42  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  43  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  44  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  45  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  46  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  47  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  48  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  49  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  50  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  51  --->  Accuracy = 67.31%\n",
      "M_pca =  18 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  18 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  18 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  18 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  18 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  18 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  18 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  18 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  18 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  18 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  11  --->  Accuracy = 57.69%\n",
      "M_pca =  18 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  13  --->  Accuracy = 64.42%\n",
      "M_pca =  18 , M_lda =  14  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  15  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  16  --->  Accuracy = 67.31%\n",
      "M_pca =  18 , M_lda =  17  --->  Accuracy = 67.31%\n",
      "M_pca =  18 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  20  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  21  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  22  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  23  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  24  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  25  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  26  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  27  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  28  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  29  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  30  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  31  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  32  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  33  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  34  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  35  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  36  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  37  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  38  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  39  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  40  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  41  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  42  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  43  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  44  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  45  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  46  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  47  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  48  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  49  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  50  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  51  --->  Accuracy = 70.19%\n",
      "M_pca =  19 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  19 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  19 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  19 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  19 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  19 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  19 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  19 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  19 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  19 , M_lda =  10  --->  Accuracy = 62.50%\n",
      "M_pca =  19 , M_lda =  11  --->  Accuracy = 61.54%\n",
      "M_pca =  19 , M_lda =  12  --->  Accuracy = 64.42%\n",
      "M_pca =  19 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  19 , M_lda =  14  --->  Accuracy = 65.38%\n",
      "M_pca =  19 , M_lda =  15  --->  Accuracy = 65.38%\n",
      "M_pca =  19 , M_lda =  16  --->  Accuracy = 66.35%\n",
      "M_pca =  19 , M_lda =  17  --->  Accuracy = 68.27%\n",
      "M_pca =  19 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  19 , M_lda =  19  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  21  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  23  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  24  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  25  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  26  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  27  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  28  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  29  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  30  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  31  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  32  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  33  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  34  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  36  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  37  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  38  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  39  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  40  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  41  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  43  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  44  --->  Accuracy = 71.15%\n",
      "M_pca =  19 , M_lda =  45  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  46  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  47  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  48  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  49  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  50  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  51  --->  Accuracy = 72.12%\n",
      "M_pca =  20 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  20 , M_lda =  2  --->  Accuracy = 10.58%\n",
      "M_pca =  20 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  20 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  20 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  20 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  20 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  20 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  20 , M_lda =  9  --->  Accuracy = 60.58%\n",
      "M_pca =  20 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  20 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  20 , M_lda =  12  --->  Accuracy = 66.35%\n",
      "M_pca =  20 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  20 , M_lda =  15  --->  Accuracy = 69.23%\n",
      "M_pca =  20 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  20 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  20 , M_lda =  20  --->  Accuracy = 75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  20 , M_lda =  21  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  22  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  24  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  25  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  26  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  27  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  28  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  29  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  30  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  31  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  32  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  33  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  34  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  35  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  36  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  37  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  38  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  39  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  40  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  41  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  43  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  44  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  45  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  46  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  47  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  48  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  49  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  50  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  51  --->  Accuracy = 75.00%\n",
      "M_pca =  21 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  21 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  21 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  21 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  21 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  21 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  21 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  21 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  21 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  21 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  21 , M_lda =  14  --->  Accuracy = 64.42%\n",
      "M_pca =  21 , M_lda =  15  --->  Accuracy = 66.35%\n",
      "M_pca =  21 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  21 , M_lda =  17  --->  Accuracy = 68.27%\n",
      "M_pca =  21 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  21 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  21 , M_lda =  20  --->  Accuracy = 71.15%\n",
      "M_pca =  21 , M_lda =  21  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  23  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  24  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  25  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  26  --->  Accuracy = 73.08%\n",
      "M_pca =  21 , M_lda =  27  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  28  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  29  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  30  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  31  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  32  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  33  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  36  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  37  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  38  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  39  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  40  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  41  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  43  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  44  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  45  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  46  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  47  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  48  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  49  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  50  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  51  --->  Accuracy = 72.12%\n",
      "M_pca =  22 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  22 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  22 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  22 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  22 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  22 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  22 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  22 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  22 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  22 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  22 , M_lda =  12  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  22 , M_lda =  15  --->  Accuracy = 67.31%\n",
      "M_pca =  22 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  22 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  22 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  22 , M_lda =  19  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  20  --->  Accuracy = 71.15%\n",
      "M_pca =  22 , M_lda =  21  --->  Accuracy = 72.12%\n",
      "M_pca =  22 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  23  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  24  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  25  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  26  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  27  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  28  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  29  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  31  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  33  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  34  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  35  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  36  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  37  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  38  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  39  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  40  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  41  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  42  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  43  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  44  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  45  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  46  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  47  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  48  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  49  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  50  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  51  --->  Accuracy = 73.08%\n",
      "M_pca =  23 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  23 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  23 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  23 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  23 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  23 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  23 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  23 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  23 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  23 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  23 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  14  --->  Accuracy = 70.19%\n",
      "M_pca =  23 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  23 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  23 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  23  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  24  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  25  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  26  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  27  --->  Accuracy = 73.08%\n",
      "M_pca =  23 , M_lda =  28  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  29  --->  Accuracy = 74.04%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  23 , M_lda =  30  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  32  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  33  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  34  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  35  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  36  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  37  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  38  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  39  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  40  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  41  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  42  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  43  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  44  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  45  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  46  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  47  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  49  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  50  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  51  --->  Accuracy = 74.04%\n",
      "M_pca =  24 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  24 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  24 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  24 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  24 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  24 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  24 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  24 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  24 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  14  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  24 , M_lda =  16  --->  Accuracy = 69.23%\n",
      "M_pca =  24 , M_lda =  17  --->  Accuracy = 69.23%\n",
      "M_pca =  24 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  20  --->  Accuracy = 69.23%\n",
      "M_pca =  24 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  22  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  23  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  24  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  25  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  26  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  27  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  28  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  29  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  31  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  33  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  34  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  35  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  36  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  37  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  38  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  39  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  40  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  41  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  42  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  43  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  44  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  45  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  46  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  47  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  48  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  49  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  50  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  51  --->  Accuracy = 73.08%\n",
      "M_pca =  25 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  25 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  25 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  25 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  25 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  25 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  25 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  25 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  25 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  25 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  25 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  25 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  18  --->  Accuracy = 69.23%\n",
      "M_pca =  25 , M_lda =  19  --->  Accuracy = 69.23%\n",
      "M_pca =  25 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  25 , M_lda =  21  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  22  --->  Accuracy = 69.23%\n",
      "M_pca =  25 , M_lda =  23  --->  Accuracy = 68.27%\n",
      "M_pca =  25 , M_lda =  24  --->  Accuracy = 66.35%\n",
      "M_pca =  25 , M_lda =  25  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  26  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  27  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  28  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  29  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  30  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  31  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  32  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  33  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  34  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  35  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  36  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  37  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  38  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  39  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  40  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  41  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  42  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  43  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  44  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  45  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  46  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  47  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  48  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  49  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  50  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  51  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  26 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  26 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  26 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  26 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  26 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  26 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  26 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  26 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  26 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  26 , M_lda =  13  --->  Accuracy = 68.27%\n",
      "M_pca =  26 , M_lda =  14  --->  Accuracy = 67.31%\n",
      "M_pca =  26 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  26 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  26 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  26 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  26 , M_lda =  22  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  23  --->  Accuracy = 69.23%\n",
      "M_pca =  26 , M_lda =  24  --->  Accuracy = 69.23%\n",
      "M_pca =  26 , M_lda =  25  --->  Accuracy = 69.23%\n",
      "M_pca =  26 , M_lda =  26  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  27  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  28  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  29  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  30  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  31  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  32  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  33  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  36  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  37  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  38  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  39  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  40  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  41  --->  Accuracy = 72.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  26 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  43  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  44  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  45  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  46  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  47  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  48  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  49  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  50  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  51  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  27 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  27 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  27 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  27 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  27 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  27 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  27 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  27 , M_lda =  9  --->  Accuracy = 65.38%\n",
      "M_pca =  27 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  27 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  27 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  27 , M_lda =  14  --->  Accuracy = 70.19%\n",
      "M_pca =  27 , M_lda =  15  --->  Accuracy = 70.19%\n",
      "M_pca =  27 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  27 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  27 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  27 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  27 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  27 , M_lda =  21  --->  Accuracy = 75.00%\n",
      "M_pca =  27 , M_lda =  22  --->  Accuracy = 74.04%\n",
      "M_pca =  27 , M_lda =  23  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  24  --->  Accuracy = 70.19%\n",
      "M_pca =  27 , M_lda =  25  --->  Accuracy = 70.19%\n",
      "M_pca =  27 , M_lda =  26  --->  Accuracy = 71.15%\n",
      "M_pca =  27 , M_lda =  27  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  28  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  29  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  30  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  31  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  32  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  33  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  36  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  37  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  38  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  39  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  40  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  41  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  43  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  44  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  45  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  46  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  47  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  48  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  49  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  50  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  51  --->  Accuracy = 72.12%\n",
      "M_pca =  28 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  28 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  28 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  28 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  28 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  28 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  28 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  28 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  28 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  28 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  28 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  28 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  28 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  28 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  19  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  28 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  23  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  24  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  25  --->  Accuracy = 67.31%\n",
      "M_pca =  28 , M_lda =  26  --->  Accuracy = 67.31%\n",
      "M_pca =  28 , M_lda =  27  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  28  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  29  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  30  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  31  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  32  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  33  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  34  --->  Accuracy = 68.27%\n",
      "M_pca =  28 , M_lda =  35  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  36  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  37  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  38  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  39  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  40  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  41  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  42  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  43  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  44  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  45  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  46  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  47  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  48  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  49  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  50  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  51  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  29 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  29 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  29 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  29 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  29 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  29 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  29 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  29 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  29 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  29 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  29 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  29 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  29 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  29 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  29 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  22  --->  Accuracy = 71.15%\n",
      "M_pca =  29 , M_lda =  23  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  24  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  25  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  26  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  27  --->  Accuracy = 68.27%\n",
      "M_pca =  29 , M_lda =  28  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  29  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  30  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  31  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  32  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  33  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  34  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  35  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  36  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  37  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  38  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  39  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  40  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  41  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  42  --->  Accuracy = 68.27%\n",
      "M_pca =  29 , M_lda =  43  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  44  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  45  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  46  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  47  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  48  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  49  --->  Accuracy = 68.27%\n",
      "M_pca =  29 , M_lda =  50  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  51  --->  Accuracy = 70.19%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  30 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  30 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  30 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  30 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  30 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  30 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  30 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  30 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  30 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  30 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  30 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  30 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  30 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  30 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  30 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  30 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  30 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  30 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  30 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  30 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  30 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  30 , M_lda =  23  --->  Accuracy = 72.12%\n",
      "M_pca =  30 , M_lda =  24  --->  Accuracy = 71.15%\n",
      "M_pca =  30 , M_lda =  25  --->  Accuracy = 72.12%\n",
      "M_pca =  30 , M_lda =  26  --->  Accuracy = 69.23%\n",
      "M_pca =  30 , M_lda =  27  --->  Accuracy = 70.19%\n",
      "M_pca =  30 , M_lda =  28  --->  Accuracy = 66.35%\n",
      "M_pca =  30 , M_lda =  29  --->  Accuracy = 66.35%\n",
      "M_pca =  30 , M_lda =  30  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  31  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  32  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  33  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  34  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  35  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  36  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  37  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  38  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  39  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  40  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  41  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  42  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  43  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  44  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  45  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  46  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  47  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  48  --->  Accuracy = 69.23%\n",
      "M_pca =  30 , M_lda =  49  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  50  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  51  --->  Accuracy = 67.31%\n",
      "M_pca =  31 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  31 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  31 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  31 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  31 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  31 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  31 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  31 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  31 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  31 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  31 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  31 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  23  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  24  --->  Accuracy = 75.00%\n",
      "M_pca =  31 , M_lda =  25  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  26  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  27  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  28  --->  Accuracy = 71.15%\n",
      "M_pca =  31 , M_lda =  29  --->  Accuracy = 71.15%\n",
      "M_pca =  31 , M_lda =  30  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  31  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  32  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  33  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  34  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  35  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  36  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  37  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  38  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  39  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  40  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  41  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  42  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  43  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  44  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  45  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  46  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  47  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  48  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  49  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  50  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  51  --->  Accuracy = 70.19%\n",
      "M_pca =  32 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  32 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  32 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  32 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  32 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  32 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  32 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  32 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  32 , M_lda =  9  --->  Accuracy = 65.38%\n",
      "M_pca =  32 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  32 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  32 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  32 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  32 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  32 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  32 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  32 , M_lda =  24  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  25  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  26  --->  Accuracy = 75.00%\n",
      "M_pca =  32 , M_lda =  27  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  28  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  29  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  31  --->  Accuracy = 71.15%\n",
      "M_pca =  32 , M_lda =  32  --->  Accuracy = 70.19%\n",
      "M_pca =  32 , M_lda =  33  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  34  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  35  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  36  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  37  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  38  --->  Accuracy = 75.00%\n",
      "M_pca =  32 , M_lda =  39  --->  Accuracy = 71.15%\n",
      "M_pca =  32 , M_lda =  40  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  41  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  42  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  43  --->  Accuracy = 69.23%\n",
      "M_pca =  32 , M_lda =  44  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  45  --->  Accuracy = 71.15%\n",
      "M_pca =  32 , M_lda =  46  --->  Accuracy = 69.23%\n",
      "M_pca =  32 , M_lda =  47  --->  Accuracy = 70.19%\n",
      "M_pca =  32 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  49  --->  Accuracy = 70.19%\n",
      "M_pca =  32 , M_lda =  50  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  51  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  33 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  33 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  33 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  33 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  33 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  33 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  33 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  33 , M_lda =  9  --->  Accuracy = 66.35%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  33 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  33 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  33 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  21  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  33 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  24  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  25  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  26  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  27  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  28  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  29  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  30  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  31  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  32  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  33  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  35  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  36  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  37  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  38  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  39  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  40  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  41  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  42  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  43  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  44  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  45  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  46  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  47  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  49  --->  Accuracy = 73.08%\n",
      "M_pca =  33 , M_lda =  50  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  51  --->  Accuracy = 71.15%\n",
      "M_pca =  34 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  34 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  34 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  34 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  34 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  34 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  34 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  34 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  34 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  34 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  34 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  34 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  34 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  34 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  22  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  34 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  34 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  34 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  34 , M_lda =  27  --->  Accuracy = 76.92%\n",
      "M_pca =  34 , M_lda =  28  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  29  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  30  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  32  --->  Accuracy = 72.12%\n",
      "M_pca =  34 , M_lda =  33  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  34  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  35  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  36  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  37  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  38  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  39  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  40  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  41  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  43  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  44  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  45  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  46  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  47  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  48  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  49  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  50  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  35 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  35 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  35 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  35 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  35 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  35 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  35 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  35 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  35 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  35 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  35 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  35 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  35 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  35 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  35 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  35 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  35 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  35 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  35 , M_lda =  27  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  28  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  29  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  30  --->  Accuracy = 74.04%\n",
      "M_pca =  35 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  35 , M_lda =  32  --->  Accuracy = 74.04%\n",
      "M_pca =  35 , M_lda =  33  --->  Accuracy = 73.08%\n",
      "M_pca =  35 , M_lda =  34  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  35  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  36  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  37  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  38  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  39  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  40  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  41  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  42  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  44  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  46  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  48  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  49  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  50  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  36 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  36 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  36 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  36 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  36 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  36 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  36 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  36 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  36 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  36 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  36 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  36 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  36 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  36 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  19  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  36 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  36 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  36 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  36 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  36 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  36 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  36 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  36 , M_lda =  28  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  29  --->  Accuracy = 75.00%\n",
      "M_pca =  36 , M_lda =  30  --->  Accuracy = 74.04%\n",
      "M_pca =  36 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  36 , M_lda =  32  --->  Accuracy = 75.00%\n",
      "M_pca =  36 , M_lda =  33  --->  Accuracy = 74.04%\n",
      "M_pca =  36 , M_lda =  34  --->  Accuracy = 74.04%\n",
      "M_pca =  36 , M_lda =  35  --->  Accuracy = 75.00%\n",
      "M_pca =  36 , M_lda =  36  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  37  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  38  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  39  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  40  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  41  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  42  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  44  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  46  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  47  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  48  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  37 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  37 , M_lda =  3  --->  Accuracy = 19.23%\n",
      "M_pca =  37 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  37 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  37 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  37 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  37 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  37 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  37 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  24  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  37 , M_lda =  26  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  37 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  37 , M_lda =  29  --->  Accuracy = 78.85%\n",
      "M_pca =  37 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  31  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  33  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  34  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  37 , M_lda =  36  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  37  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  38  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  37 , M_lda =  40  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  41  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  43  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  44  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  45  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  46  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  48  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  49  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  50  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  38 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  38 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  38 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  38 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  38 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  38 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  38 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  38 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  38 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  38 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  38 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  38 , M_lda =  29  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  38 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  38 , M_lda =  33  --->  Accuracy = 71.15%\n",
      "M_pca =  38 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  38 , M_lda =  35  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  36  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  37  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  38  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  39  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  40  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  41  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  44  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  45  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  46  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  48  --->  Accuracy = 73.08%\n",
      "M_pca =  38 , M_lda =  49  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  50  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  39 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  39 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  39 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  39 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  39 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  39 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  39 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  39 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  39 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  39 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  39 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  39 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  39 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  39 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  39 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  39 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  39 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  39 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  39 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  39 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  39 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  39 , M_lda =  29  --->  Accuracy = 75.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  39 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  39 , M_lda =  31  --->  Accuracy = 73.08%\n",
      "M_pca =  39 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  39 , M_lda =  33  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  39 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  39 , M_lda =  36  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  37  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  38  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  39  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  40  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  41  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  43  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  44  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  45  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  46  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  48  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  49  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  50  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  51  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  40 , M_lda =  2  --->  Accuracy = 9.62%\n",
      "M_pca =  40 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  40 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  40 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  40 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  40 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  40 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  40 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  40 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  40 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  40 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  24  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  25  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  40 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  40 , M_lda =  28  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  29  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  30  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  33  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  34  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  35  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  36  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  37  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  38  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  39  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  40  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  41  --->  Accuracy = 72.12%\n",
      "M_pca =  40 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  40 , M_lda =  43  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  44  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  45  --->  Accuracy = 72.12%\n",
      "M_pca =  40 , M_lda =  46  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  47  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  49  --->  Accuracy = 72.12%\n",
      "M_pca =  40 , M_lda =  50  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  51  --->  Accuracy = 72.12%\n",
      "M_pca =  41 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  41 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  41 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  41 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  41 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  41 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  41 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  41 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  41 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  41 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  41 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  41 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  41 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  41 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  41 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  41 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  41 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  41 , M_lda =  33  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  35  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  36  --->  Accuracy = 75.96%\n",
      "M_pca =  41 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  39  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  40  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  41  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  43  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  44  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  45  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  46  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  47  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  49  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  50  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  51  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  42 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  42 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  42 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  42 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  42 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  42 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  42 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  42 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  42 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  42 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  42 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  42 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  42 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  33  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  36  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  39  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  40  --->  Accuracy = 73.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  42 , M_lda =  41  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  42 , M_lda =  43  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  44  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  46  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  47  --->  Accuracy = 75.00%\n",
      "M_pca =  42 , M_lda =  48  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  49  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  43 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  43 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  43 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  43 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  43 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  43 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  43 , M_lda =  8  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  43 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  43 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  43 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  43 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  43 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  43 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  43 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  43 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  43 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  32  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  33  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  34  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  39  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  40  --->  Accuracy = 74.04%\n",
      "M_pca =  43 , M_lda =  41  --->  Accuracy = 74.04%\n",
      "M_pca =  43 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  43 , M_lda =  43  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  44  --->  Accuracy = 75.00%\n",
      "M_pca =  43 , M_lda =  45  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  46  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  48  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  49  --->  Accuracy = 74.04%\n",
      "M_pca =  43 , M_lda =  50  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  44 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  44 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  44 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  44 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  44 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  44 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  44 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  44 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  44 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  44 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  44 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  44 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  44 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  44 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  44 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  44 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  44 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  44 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  44 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  35  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  40  --->  Accuracy = 76.92%\n",
      "M_pca =  44 , M_lda =  41  --->  Accuracy = 76.92%\n",
      "M_pca =  44 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  44 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  44 , M_lda =  44  --->  Accuracy = 75.00%\n",
      "M_pca =  44 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  44 , M_lda =  46  --->  Accuracy = 73.08%\n",
      "M_pca =  44 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  44 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  44 , M_lda =  49  --->  Accuracy = 75.00%\n",
      "M_pca =  44 , M_lda =  50  --->  Accuracy = 75.00%\n",
      "M_pca =  44 , M_lda =  51  --->  Accuracy = 74.04%\n",
      "M_pca =  45 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  45 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  45 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  45 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  45 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  45 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  45 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  45 , M_lda =  8  --->  Accuracy = 75.96%\n",
      "M_pca =  45 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  45 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  45 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  45 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  45 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  45 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  45 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  45 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  25  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  45 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  45 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  45 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  33  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  45 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  38  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  40  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  41  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  42  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  44  --->  Accuracy = 75.00%\n",
      "M_pca =  45 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  47  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  51  --->  Accuracy = 75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  46 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  46 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  46 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  46 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  46 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  46 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  46 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  46 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  46 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  46 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  46 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  46 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  46 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  46 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  46 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  46 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  46 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  46 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  46 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  46 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  31  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  46 , M_lda =  33  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  34  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  35  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  39  --->  Accuracy = 75.96%\n",
      "M_pca =  46 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  46 , M_lda =  41  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  42  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  43  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  44  --->  Accuracy = 75.00%\n",
      "M_pca =  46 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  46  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  46 , M_lda =  48  --->  Accuracy = 75.96%\n",
      "M_pca =  46 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  47 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  47 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  47 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  47 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  47 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  47 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  47 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  47 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  47 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  47 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  47 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  47 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  47 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  47 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  47 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  32  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  47 , M_lda =  34  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  36  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  38  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  47 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  41  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  42  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  45  --->  Accuracy = 75.96%\n",
      "M_pca =  47 , M_lda =  46  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  47  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  49  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  48 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  48 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  48 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  48 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  48 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  48 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  48 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  48 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  48 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  48 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  48 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  48 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  48 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  48 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  48 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  48 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  32  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  33  --->  Accuracy = 76.92%\n",
      "M_pca =  48 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  35  --->  Accuracy = 76.92%\n",
      "M_pca =  48 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  48 , M_lda =  40  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  48 , M_lda =  42  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  43  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  48 , M_lda =  46  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  47  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  49  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  48 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  49 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  49 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  49 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  49 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  49 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  49 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  49 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  49 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  49 , M_lda =  11  --->  Accuracy = 77.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  49 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  49 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  49 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  49 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  49 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  49 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  49 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  49 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  49 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  49 , M_lda =  31  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  36  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  40  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  48  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  50  --->  Accuracy = 75.96%\n",
      "M_pca =  49 , M_lda =  51  --->  Accuracy = 75.00%\n",
      "M_pca =  50 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  50 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  50 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  50 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  50 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  50 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  50 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  50 , M_lda =  8  --->  Accuracy = 78.85%\n",
      "M_pca =  50 , M_lda =  9  --->  Accuracy = 78.85%\n",
      "M_pca =  50 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  50 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  50 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  50 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  50 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  50 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  50 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  50 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  50 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  50 , M_lda =  33  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  40  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  42  --->  Accuracy = 78.85%\n",
      "M_pca =  50 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  50 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  46  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  47  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  50  --->  Accuracy = 75.00%\n",
      "M_pca =  50 , M_lda =  51  --->  Accuracy = 77.88%\n",
      "M_pca =  51 , M_lda =  1  --->  Accuracy = 12.50%\n",
      "M_pca =  51 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  51 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  51 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  51 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  51 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  51 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  51 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  51 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  51 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  51 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  51 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  51 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  51 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  51 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  51 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  51 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  51 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  51 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  51 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  51 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  51 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  51 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  51 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  51 , M_lda =  33  --->  Accuracy = 77.88%\n",
      "M_pca =  51 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  51 , M_lda =  35  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  36  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  51 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  51 , M_lda =  41  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  42  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  47  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  51 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  52 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  52 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  52 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  52 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  52 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  52 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  52 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  52 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  52 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  52 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  52 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  52 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  52 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  22  --->  Accuracy = 81.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  52 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  52 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  52 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  52 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  52 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  52 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  34  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  52 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  52 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  52 , M_lda =  48  --->  Accuracy = 76.92%\n",
      "M_pca =  52 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  52 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  52 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  53 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  53 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  53 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  53 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  53 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  53 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  53 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  53 , M_lda =  8  --->  Accuracy = 75.00%\n",
      "M_pca =  53 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  53 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  53 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  53 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  53 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  53 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  53 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  53 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  53 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  53 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  53 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  53 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  43  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  53 , M_lda =  47  --->  Accuracy = 76.92%\n",
      "M_pca =  53 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  53 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  53 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  53 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  54 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  54 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  54 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  54 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  54 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  54 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  54 , M_lda =  7  --->  Accuracy = 71.15%\n",
      "M_pca =  54 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  54 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  54 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  54 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  54 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  54 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  54 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  54 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  54 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  54 , M_lda =  41  --->  Accuracy = 77.88%\n",
      "M_pca =  54 , M_lda =  42  --->  Accuracy = 79.81%\n",
      "M_pca =  54 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  54 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  55 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  55 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  55 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  55 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  55 , M_lda =  6  --->  Accuracy = 66.35%\n",
      "M_pca =  55 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  55 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  55 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  55 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  55 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  55 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  55 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  55 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  55 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  55 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  55 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  55 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  55 , M_lda =  32  --->  Accuracy = 81.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  55 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  34  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  55 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  55 , M_lda =  41  --->  Accuracy = 77.88%\n",
      "M_pca =  55 , M_lda =  42  --->  Accuracy = 76.92%\n",
      "M_pca =  55 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  55 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  55 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  55 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  55 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  56 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  56 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  56 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  56 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  56 , M_lda =  6  --->  Accuracy = 67.31%\n",
      "M_pca =  56 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  56 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  56 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  56 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  56 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  56 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  56 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  56 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  56 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  56 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  56 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  56 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  41  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  42  --->  Accuracy = 76.92%\n",
      "M_pca =  56 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  56 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  50  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  57 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  57 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  57 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  57 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  57 , M_lda =  6  --->  Accuracy = 65.38%\n",
      "M_pca =  57 , M_lda =  7  --->  Accuracy = 71.15%\n",
      "M_pca =  57 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  57 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  57 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  57 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  57 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  57 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  57 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  57 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  57 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  57 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  57 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  57 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  57 , M_lda =  26  --->  Accuracy = 76.92%\n",
      "M_pca =  57 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  57 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  57 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  57 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  57 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  40  --->  Accuracy = 76.92%\n",
      "M_pca =  57 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  57 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  57 , M_lda =  46  --->  Accuracy = 75.96%\n",
      "M_pca =  57 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  48  --->  Accuracy = 79.81%\n",
      "M_pca =  57 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  57 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  57 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  58 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  58 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  58 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  58 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  58 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  58 , M_lda =  6  --->  Accuracy = 64.42%\n",
      "M_pca =  58 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  58 , M_lda =  8  --->  Accuracy = 75.00%\n",
      "M_pca =  58 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  58 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  58 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  58 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  58 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  58 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  58 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  58 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  58 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  58 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  58 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  58 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  58 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  58 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  58 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  58 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  58 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  41  --->  Accuracy = 77.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  58 , M_lda =  42  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  43  --->  Accuracy = 77.88%\n",
      "M_pca =  58 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  58 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  58 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  58 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  58 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  50  --->  Accuracy = 78.85%\n",
      "M_pca =  58 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  59 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  59 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  59 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  59 , M_lda =  5  --->  Accuracy = 60.58%\n",
      "M_pca =  59 , M_lda =  6  --->  Accuracy = 64.42%\n",
      "M_pca =  59 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  59 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  59 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  59 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  59 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  59 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  59 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  59 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  59 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  59 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  59 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  59 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  59 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  59 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  59 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  59 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  59 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  59 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  41  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  42  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  59 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  46  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  60 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  60 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  60 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  60 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  60 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  60 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  60 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  60 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  60 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  60 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  60 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  60 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  60 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  60 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  60 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  60 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  60 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  60 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  60 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  60 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  60 , M_lda =  40  --->  Accuracy = 77.88%\n",
      "M_pca =  60 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  60 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  47  --->  Accuracy = 76.92%\n",
      "M_pca =  60 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  60 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  60 , M_lda =  50  --->  Accuracy = 78.85%\n",
      "M_pca =  60 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  61 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  61 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  61 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  61 , M_lda =  4  --->  Accuracy = 50.96%\n",
      "M_pca =  61 , M_lda =  5  --->  Accuracy = 66.35%\n",
      "M_pca =  61 , M_lda =  6  --->  Accuracy = 67.31%\n",
      "M_pca =  61 , M_lda =  7  --->  Accuracy = 72.12%\n",
      "M_pca =  61 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  61 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  61 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  61 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  61 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  61 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  61 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  61 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  61 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  61 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  61 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  61 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  61 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  61 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  61 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  61 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  61 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  61 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  44  --->  Accuracy = 78.85%\n",
      "M_pca =  61 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  47  --->  Accuracy = 77.88%\n",
      "M_pca =  61 , M_lda =  48  --->  Accuracy = 79.81%\n",
      "M_pca =  61 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  51  --->  Accuracy = 77.88%\n",
      "M_pca =  62 , M_lda =  1  --->  Accuracy = 4.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  62 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  62 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  62 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  62 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  62 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  62 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  62 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  62 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  62 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  62 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  62 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  62 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  62 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  62 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  62 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  62 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  62 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  62 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  62 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  62 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  62 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  62 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  62 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  63 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  63 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  63 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  63 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  63 , M_lda =  5  --->  Accuracy = 61.54%\n",
      "M_pca =  63 , M_lda =  6  --->  Accuracy = 66.35%\n",
      "M_pca =  63 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  63 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  63 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  63 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  63 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  63 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  63 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  63 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  63 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  63 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  63 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  63 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  63 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  63 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  63 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  63 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  63 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  63 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  63 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  64 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  64 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  64 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  64 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  64 , M_lda =  5  --->  Accuracy = 60.58%\n",
      "M_pca =  64 , M_lda =  6  --->  Accuracy = 70.19%\n",
      "M_pca =  64 , M_lda =  7  --->  Accuracy = 71.15%\n",
      "M_pca =  64 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  64 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  64 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  64 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  64 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  64 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  64 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  64 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  64 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  64 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  64 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  64 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  64 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  64 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  64 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  64 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  64 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  64 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  64 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  65 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  65 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  65 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  65 , M_lda =  5  --->  Accuracy = 58.65%\n",
      "M_pca =  65 , M_lda =  6  --->  Accuracy = 67.31%\n",
      "M_pca =  65 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  65 , M_lda =  8  --->  Accuracy = 75.00%\n",
      "M_pca =  65 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  65 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  65 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  12  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  65 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  65 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  65 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  65 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  65 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  65 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  65 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  65 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  65 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  65 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  65 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  65 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  65 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  47  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  66 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  66 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  66 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  66 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  66 , M_lda =  5  --->  Accuracy = 59.62%\n",
      "M_pca =  66 , M_lda =  6  --->  Accuracy = 66.35%\n",
      "M_pca =  66 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  66 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  66 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  66 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  66 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  66 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  66 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  66 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  66 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  66 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  66 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  66 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  66 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  66 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  66 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  66 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  66 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  66 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  66 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  66 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  66 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  66 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  51  --->  Accuracy = 77.88%\n",
      "M_pca =  67 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  67 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  67 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  67 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  67 , M_lda =  5  --->  Accuracy = 59.62%\n",
      "M_pca =  67 , M_lda =  6  --->  Accuracy = 66.35%\n",
      "M_pca =  67 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  67 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  67 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  67 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  67 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  67 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  67 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  67 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  67 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  67 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  67 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  67 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  67 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  67 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  67 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  67 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  67 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  67 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  67 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  67 , M_lda =  43  --->  Accuracy = 79.81%\n",
      "M_pca =  67 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  67 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  67 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  51  --->  Accuracy = 77.88%\n",
      "M_pca =  68 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  68 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  68 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  68 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  68 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  68 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  68 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  68 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  68 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  68 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  68 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  68 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  68 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  68 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  68 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  68 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  68 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  23  --->  Accuracy = 79.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  68 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  68 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  68 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  68 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  68 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  68 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  68 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  68 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  68 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  68 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  68 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  68 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  68 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  68 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  68 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  68 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  68 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  68 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  68 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  68 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  69 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  69 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  69 , M_lda =  3  --->  Accuracy = 46.15%\n",
      "M_pca =  69 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  69 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  69 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  69 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  69 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  69 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  69 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  69 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  69 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  69 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  69 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  69 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  69 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  69 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  69 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  69 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  69 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  69 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  69 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  69 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  69 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  48  --->  Accuracy = 79.81%\n",
      "M_pca =  69 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  50  --->  Accuracy = 78.85%\n",
      "M_pca =  69 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  70 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  70 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  70 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  70 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  70 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  70 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  70 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  70 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  70 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  70 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  70 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  70 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  70 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  70 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  70 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  70 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  70 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  70 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  70 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  70 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  70 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  70 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  70 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  70 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  70 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  70 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  70 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  70 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  70 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  70 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  70 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  71 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  71 , M_lda =  3  --->  Accuracy = 39.42%\n",
      "M_pca =  71 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  71 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  71 , M_lda =  6  --->  Accuracy = 66.35%\n",
      "M_pca =  71 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  71 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  71 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  71 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  71 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  71 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  71 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  71 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  71 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  71 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  71 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  71 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  71 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  71 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  71 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  34  --->  Accuracy = 85.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  71 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  71 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  71 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  72 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  72 , M_lda =  3  --->  Accuracy = 39.42%\n",
      "M_pca =  72 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  72 , M_lda =  5  --->  Accuracy = 56.73%\n",
      "M_pca =  72 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  72 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  72 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  72 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  72 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  72 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  72 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  72 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  72 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  72 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  72 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  72 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  72 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  72 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  72 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  72 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  72 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  72 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  72 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  72 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  72 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  73 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  73 , M_lda =  3  --->  Accuracy = 41.35%\n",
      "M_pca =  73 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  73 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  73 , M_lda =  6  --->  Accuracy = 65.38%\n",
      "M_pca =  73 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  73 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  73 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  73 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  73 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  73 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  73 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  73 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  73 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  73 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  73 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  73 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  73 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  73 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  73 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  73 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  73 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  74 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  74 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  74 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  74 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  74 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  74 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  74 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  74 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  74 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  74 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  74 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  74 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  74 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  74 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  74 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  74 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  74 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  74 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  74 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  74 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  74 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  74 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  74 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  74 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  74 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  43  --->  Accuracy = 82.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  74 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  74 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  74 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  74 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  74 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  74 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  74 , M_lda =  50  --->  Accuracy = 78.85%\n",
      "M_pca =  74 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  75 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  75 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  75 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  75 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  75 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  75 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  75 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  75 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  75 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  75 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  75 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  75 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  75 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  75 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  75 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  75 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  75 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  75 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  75 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  75 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  75 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  75 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  75 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  75 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  75 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  75 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  75 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  75 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  75 , M_lda =  32  --->  Accuracy = 77.88%\n",
      "M_pca =  75 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  75 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  75 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  75 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  75 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  75 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  75 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  75 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  75 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  75 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  75 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  75 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  75 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  75 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  75 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  76 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  76 , M_lda =  3  --->  Accuracy = 39.42%\n",
      "M_pca =  76 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  76 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  76 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  76 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  76 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  76 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  76 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  76 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  76 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  76 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  76 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  76 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  76 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  76 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  76 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  76 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  76 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  49  --->  Accuracy = 77.88%\n",
      "M_pca =  76 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  76 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  77 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  77 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  77 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  77 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  77 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  77 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  77 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  77 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  77 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  77 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  77 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  77 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  77 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  77 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  77 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  77 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  77 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  77 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  77 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  77 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  77 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  77 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  44  --->  Accuracy = 76.92%\n",
      "M_pca =  77 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  77 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  48  --->  Accuracy = 76.92%\n",
      "M_pca =  77 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  77 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  77 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  78 , M_lda =  1  --->  Accuracy = 6.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  78 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  78 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  78 , M_lda =  4  --->  Accuracy = 52.88%\n",
      "M_pca =  78 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  78 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  78 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  78 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  78 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  78 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  78 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  78 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  78 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  78 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  78 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  78 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  78 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  78 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  78 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  78 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  78 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  78 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  78 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  78 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  78 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  78 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  78 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  78 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  78 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  78 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  79 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  79 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  79 , M_lda =  3  --->  Accuracy = 38.46%\n",
      "M_pca =  79 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  79 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  79 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  79 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  79 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  79 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  79 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  79 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  79 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  79 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  79 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  79 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  79 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  79 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  79 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  79 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  79 , M_lda =  43  --->  Accuracy = 79.81%\n",
      "M_pca =  79 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  79 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  79 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  79 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  79 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  80 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  80 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  80 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  80 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  80 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  80 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  80 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  80 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  80 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  80 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  80 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  80 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  80 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  80 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  80 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  80 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  80 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  80 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  80 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  80 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  80 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  80 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  80 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  80 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  80 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  80 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  80 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  80 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  80 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  80 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  80 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  80 , M_lda =  44  --->  Accuracy = 78.85%\n",
      "M_pca =  80 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  80 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  80 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  80 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  80 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  80 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  81 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  81 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  81 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  81 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  81 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  81 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  81 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  81 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  81 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  81 , M_lda =  10  --->  Accuracy = 75.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  81 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  81 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  81 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  81 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  81 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  81 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  81 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  81 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  81 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  81 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  81 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  81 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  81 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  81 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  81 , M_lda =  42  --->  Accuracy = 79.81%\n",
      "M_pca =  81 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  81 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  81 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  81 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  81 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  82 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  82 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  82 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  82 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  82 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  82 , M_lda =  6  --->  Accuracy = 64.42%\n",
      "M_pca =  82 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  82 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  82 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  82 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  82 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  82 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  82 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  82 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  82 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  82 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  82 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  82 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  82 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  82 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  82 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  82 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  82 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  82 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  82 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  82 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  82 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  51  --->  Accuracy = 77.88%\n",
      "M_pca =  83 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  83 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  83 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  83 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  83 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  83 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  83 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  83 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  83 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  83 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  83 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  83 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  83 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  83 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  83 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  83 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  83 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  83 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  83 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  83 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  83 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  83 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  83 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  83 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  84 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  84 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  84 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  84 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  84 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  84 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  84 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  84 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  84 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  84 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  84 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  84 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  84 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  84 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  84 , M_lda =  19  --->  Accuracy = 84.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  84 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  84 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  84 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  84 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  84 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  84 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  84 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  84 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  84 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  84 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  84 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  84 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  84 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  84 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  84 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  84 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  84 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  84 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  84 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  84 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  84 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  85 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  85 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  85 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  85 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  85 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  85 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  85 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  85 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  85 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  85 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  85 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  85 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  85 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  85 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  85 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  85 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  85 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  85 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  85 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  85 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  85 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  85 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  85 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  85 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  85 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  85 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  85 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  85 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  85 , M_lda =  43  --->  Accuracy = 79.81%\n",
      "M_pca =  85 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  85 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  85 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  85 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  86 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  86 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  86 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  86 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  86 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  86 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  86 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  86 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  86 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  86 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  86 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  86 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  86 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  86 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  86 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  86 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  86 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  86 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  86 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  86 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  86 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  86 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  86 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  86 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  86 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  86 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  86 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  86 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  86 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  86 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  86 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  86 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  86 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  86 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  86 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  86 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  86 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  86 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  86 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  86 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  86 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  87 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  87 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  87 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  87 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  87 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  87 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  87 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  87 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  87 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  87 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  87 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  87 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  87 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  87 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  87 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  87 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  87 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  87 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  87 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  87 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  87 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  87 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  28  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  87 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  87 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  87 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  87 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  87 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  87 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  87 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  87 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  87 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  87 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  87 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  87 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  87 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  87 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  88 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  88 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  88 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  88 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  88 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  88 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  88 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  88 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  88 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  88 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  88 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  88 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  88 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  88 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  88 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  88 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  88 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  88 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  88 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  88 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  88 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  88 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  88 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  88 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  88 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  88 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  88 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  88 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  88 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  88 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  89 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  89 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  89 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  89 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  89 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  89 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  89 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  89 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  89 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  89 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  89 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  89 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  89 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  89 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  89 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  89 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  89 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  89 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  89 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  89 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  89 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  89 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  89 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  89 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  89 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  89 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  89 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  89 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  89 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  89 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  90 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  90 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  90 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  90 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  90 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  90 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  90 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  90 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  90 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  90 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  90 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  90 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  90 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  90 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  90 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  90 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  90 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  90 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  37  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  90 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  90 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  90 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  90 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  90 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  90 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  90 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  90 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  91 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  91 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  91 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  91 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  91 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  91 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  91 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  91 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  91 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  91 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  91 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  91 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  91 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  91 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  91 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  91 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  91 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  91 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  91 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  91 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  91 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  91 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  91 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  92 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  92 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  92 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  92 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  92 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  92 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  92 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  92 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  92 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  92 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  92 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  92 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  92 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  92 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  92 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  92 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  92 , M_lda =  18  --->  Accuracy = 89.42%\n",
      "M_pca =  92 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  92 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  92 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  92 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  92 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  92 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  92 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  92 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  92 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  92 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  92 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  92 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  92 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  92 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  92 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  92 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  92 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  93 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  93 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  93 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  93 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  93 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  93 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  93 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  93 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  93 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  93 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  93 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  93 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  93 , M_lda =  13  --->  Accuracy = 87.50%\n",
      "M_pca =  93 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  93 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  93 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  93 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  93 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  93 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  93 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  93 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  93 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  93 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  93 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  93 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  93 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  93 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  93 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  93 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  93 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  93 , M_lda =  47  --->  Accuracy = 81.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  93 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  93 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  93 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  94 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  94 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  94 , M_lda =  3  --->  Accuracy = 41.35%\n",
      "M_pca =  94 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  94 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  94 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  94 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  94 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  94 , M_lda =  9  --->  Accuracy = 78.85%\n",
      "M_pca =  94 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  94 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  94 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  94 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  94 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  94 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  94 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  94 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  94 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  94 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  94 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  94 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  94 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  94 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  94 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  94 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  94 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  94 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  94 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  94 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  94 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  94 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  94 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  94 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  95 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  95 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  95 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  95 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  95 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  95 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  95 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  95 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  95 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  95 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  95 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  95 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  95 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  95 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  95 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  95 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  95 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  95 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  95 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  95 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  95 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  95 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  95 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  95 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  95 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  95 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  96 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  96 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  96 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  96 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  96 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  96 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  96 , M_lda =  8  --->  Accuracy = 75.00%\n",
      "M_pca =  96 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  96 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  96 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  96 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  96 , M_lda =  13  --->  Accuracy = 87.50%\n",
      "M_pca =  96 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  96 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  96 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  96 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  96 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  96 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  96 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  96 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  96 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  96 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  96 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  96 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  97 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  97 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  97 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  97 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  97 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  97 , M_lda =  6  --->  Accuracy = 65.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  97 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  97 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  97 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  97 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  97 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  97 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  97 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  97 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  97 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  97 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  97 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  97 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  97 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  97 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  97 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  97 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  97 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  97 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  97 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  97 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  97 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  97 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  97 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  97 , M_lda =  48  --->  Accuracy = 79.81%\n",
      "M_pca =  97 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  97 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  98 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  98 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  98 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  98 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  98 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  98 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  98 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  98 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  98 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  98 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  98 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  98 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  98 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  98 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  98 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  98 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  98 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  98 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  98 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  98 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  98 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  98 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  98 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  98 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  98 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  98 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  98 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  98 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  99 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  99 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  99 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  99 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  99 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  99 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  99 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  99 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  99 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  99 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  99 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  99 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  99 , M_lda =  13  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  99 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  99 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  99 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  99 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  99 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  99 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  99 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  99 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  99 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  99 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  99 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  99 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  99 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  99 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  99 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  99 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  99 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  99 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  100 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  100 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  100 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  100 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  100 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  100 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  100 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  100 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  100 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  100 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  100 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  100 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  100 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  14  --->  Accuracy = 88.46%\n",
      "M_pca =  100 , M_lda =  15  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  100 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  100 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  100 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  100 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  100 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  100 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  100 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  100 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  100 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  100 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  100 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  100 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  101 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  101 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  101 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  101 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  101 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  101 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  101 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  101 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  101 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  101 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  101 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  101 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  101 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  101 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  101 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  101 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  101 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  101 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  101 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  101 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  101 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  101 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  101 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  101 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  101 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  101 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  101 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  101 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  101 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  101 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  101 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  101 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  101 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  102 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  102 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  102 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  102 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  102 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  102 , M_lda =  7  --->  Accuracy = 72.12%\n",
      "M_pca =  102 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  102 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  102 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  102 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  102 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  102 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  102 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  102 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  102 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  102 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  102 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  102 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  102 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  102 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  102 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  102 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  102 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  102 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  103 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  103 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  103 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  103 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  103 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  103 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  103 , M_lda =  7  --->  Accuracy = 71.15%\n",
      "M_pca =  103 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  103 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  103 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  103 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  103 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  103 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  22  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  103 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  103 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  103 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  103 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  103 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  103 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  103 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  103 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  103 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  104 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  104 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  104 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  104 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  104 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  104 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  104 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  104 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  104 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  104 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  104 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  12  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  104 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  104 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  104 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  104 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  104 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  104 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  104 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  105 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  105 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  105 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  105 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  105 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  105 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  105 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  105 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  105 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  105 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  105 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  13  --->  Accuracy = 87.50%\n",
      "M_pca =  105 , M_lda =  14  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  105 , M_lda =  19  --->  Accuracy = 89.42%\n",
      "M_pca =  105 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  105 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  105 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  24  --->  Accuracy = 90.38%\n",
      "M_pca =  105 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  105 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  105 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  105 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  105 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  105 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  105 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  105 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  105 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  105 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  105 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  105 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  106 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  106 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  106 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  106 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  106 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  106 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  106 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  106 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  106 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  106 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  106 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  106 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  106 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  106 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  16  --->  Accuracy = 91.35%\n",
      "M_pca =  106 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  106 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  106 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  106 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  22  --->  Accuracy = 90.38%\n",
      "M_pca =  106 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  24  --->  Accuracy = 90.38%\n",
      "M_pca =  106 , M_lda =  25  --->  Accuracy = 92.31%\n",
      "M_pca =  106 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  27  --->  Accuracy = 91.35%\n",
      "M_pca =  106 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  106 , M_lda =  29  --->  Accuracy = 90.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  106 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  106 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  106 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  106 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  106 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  106 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  106 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  106 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  106 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  106 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  106 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  106 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  106 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  107 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  107 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  107 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  107 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  107 , M_lda =  5  --->  Accuracy = 56.73%\n",
      "M_pca =  107 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  107 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  107 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  107 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  107 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  107 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  107 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  107 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  107 , M_lda =  15  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  107 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  107 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  107 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  107 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  107 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  26  --->  Accuracy = 91.35%\n",
      "M_pca =  107 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  107 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  107 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  107 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  107 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  107 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  107 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  107 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  107 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  107 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  107 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  107 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  107 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  107 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  107 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  107 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  108 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  108 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  108 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  108 , M_lda =  5  --->  Accuracy = 56.73%\n",
      "M_pca =  108 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  108 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  108 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  108 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  108 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  108 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  108 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  13  --->  Accuracy = 87.50%\n",
      "M_pca =  108 , M_lda =  14  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  15  --->  Accuracy = 91.35%\n",
      "M_pca =  108 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  108 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  108 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  108 , M_lda =  24  --->  Accuracy = 91.35%\n",
      "M_pca =  108 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  108 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  108 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  108 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  108 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  108 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  108 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  108 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  108 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  108 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  108 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  108 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  108 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  108 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  108 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  108 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  108 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  109 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  109 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  109 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  109 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  109 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  109 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  109 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  109 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  109 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  109 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  109 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  109 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  109 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  109 , M_lda =  14  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  15  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  16  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  109 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  19  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  109 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  109 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  36  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  109 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  109 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  109 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  109 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  109 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  109 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  110 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  110 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  110 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  110 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  110 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  110 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  110 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  110 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  110 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  110 , M_lda =  11  --->  Accuracy = 85.58%\n",
      "M_pca =  110 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  110 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  110 , M_lda =  14  --->  Accuracy = 89.42%\n",
      "M_pca =  110 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  110 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  110 , M_lda =  21  --->  Accuracy = 90.38%\n",
      "M_pca =  110 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  110 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  110 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  110 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  110 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  110 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  110 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  110 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  110 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  111 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  111 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  111 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  111 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  111 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  111 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  111 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  111 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  111 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  111 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  111 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  111 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  111 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  111 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  111 , M_lda =  15  --->  Accuracy = 90.38%\n",
      "M_pca =  111 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  111 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  111 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  111 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  111 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  111 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  111 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  111 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  111 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  111 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  111 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  111 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  111 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  111 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  112 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  112 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  112 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  112 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  112 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  112 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  112 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  112 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  112 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  112 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  112 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  112 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  112 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  14  --->  Accuracy = 90.38%\n",
      "M_pca =  112 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  17  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  112 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  112 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  112 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  112 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  112 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  43  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  112 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  113 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  113 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  113 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  113 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  113 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  113 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  113 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  113 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  113 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  113 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  113 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  113 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  113 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  17  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  113 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  114 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  114 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  114 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  114 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  114 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  114 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  114 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  114 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  114 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  114 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  114 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  114 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  114 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  14  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  15  --->  Accuracy = 90.38%\n",
      "M_pca =  114 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  114 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  114 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  114 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  114 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  114 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  114 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  114 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  114 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  114 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  114 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  114 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  114 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  114 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  114 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  115 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  115 , M_lda =  3  --->  Accuracy = 38.46%\n",
      "M_pca =  115 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  115 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  115 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  115 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  115 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  115 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  115 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  115 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  115 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  115 , M_lda =  13  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  115 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  115 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  115 , M_lda =  26  --->  Accuracy = 91.35%\n",
      "M_pca =  115 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  115 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  115 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  115 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  115 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  115 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  115 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  115 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  115 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  115 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  115 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  50  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  115 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  116 , M_lda =  2  --->  Accuracy = 26.92%\n",
      "M_pca =  116 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  116 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  116 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  116 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  116 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  116 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  116 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  116 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  116 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  116 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  116 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  116 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  116 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  116 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  116 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  116 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  116 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  116 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  116 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  116 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  116 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  116 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  116 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  116 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  116 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  116 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  116 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  116 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  116 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  117 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  117 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  117 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  117 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  117 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  117 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  117 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  117 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  117 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  117 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  117 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  117 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  117 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  117 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  17  --->  Accuracy = 89.42%\n",
      "M_pca =  117 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  117 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  117 , M_lda =  27  --->  Accuracy = 91.35%\n",
      "M_pca =  117 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  117 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  117 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  117 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  117 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  117 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  117 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  117 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  118 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  118 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  118 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  118 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  118 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  118 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  118 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  118 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  118 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  118 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  118 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  118 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  16  --->  Accuracy = 89.42%\n",
      "M_pca =  118 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  118 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  118 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  118 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  118 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  118 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  118 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  118 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  118 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  118 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  118 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  119 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  119 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  119 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  119 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  119 , M_lda =  6  --->  Accuracy = 54.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  119 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  119 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  119 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  119 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  119 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  119 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  119 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  119 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  119 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  119 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  119 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  119 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  21  --->  Accuracy = 90.38%\n",
      "M_pca =  119 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  119 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  119 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  119 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  119 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  119 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  119 , M_lda =  32  --->  Accuracy = 92.31%\n",
      "M_pca =  119 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  119 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  119 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  119 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  119 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  119 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  119 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  120 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  120 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  120 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  120 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  120 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  120 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  120 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  120 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  120 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  120 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  120 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  120 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  120 , M_lda =  13  --->  Accuracy = 88.46%\n",
      "M_pca =  120 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  120 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  120 , M_lda =  20  --->  Accuracy = 90.38%\n",
      "M_pca =  120 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  26  --->  Accuracy = 91.35%\n",
      "M_pca =  120 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  120 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  120 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  120 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  120 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  120 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  120 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  120 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  120 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  120 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  120 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  120 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  1  --->  Accuracy = 13.46%\n",
      "M_pca =  121 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  121 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  121 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  121 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  121 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  121 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  121 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  121 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  121 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  121 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  121 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  121 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  121 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  121 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  121 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  121 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  121 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  121 , M_lda =  20  --->  Accuracy = 91.35%\n",
      "M_pca =  121 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  121 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  121 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  121 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  121 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  121 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  121 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  121 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  121 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  121 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  121 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  121 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  121 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  121 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  121 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  121 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  121 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  121 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  121 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  121 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  122 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  122 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  122 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  122 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  122 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  122 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  122 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  122 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  122 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  122 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  122 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  122 , M_lda =  13  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  122 , M_lda =  14  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  122 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  122 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  122 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  122 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  122 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  122 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  122 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  122 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  122 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  122 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  122 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  122 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  122 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  122 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  122 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  122 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  122 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  123 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  123 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  123 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  123 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  123 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  123 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  123 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  123 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  123 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  123 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  123 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  123 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  123 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  123 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  123 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  123 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  123 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  123 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  30  --->  Accuracy = 93.27%\n",
      "M_pca =  123 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  123 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  123 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  123 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  123 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  123 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  123 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  123 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  124 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  124 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  124 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  124 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  124 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  124 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  124 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  124 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  124 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  124 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  124 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  124 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  124 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  124 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  124 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  124 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  124 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  124 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  26  --->  Accuracy = 91.35%\n",
      "M_pca =  124 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  124 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  124 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  124 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  124 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  124 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  124 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  124 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  125 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  125 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  125 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  125 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  125 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  125 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  125 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  125 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  125 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  125 , M_lda =  10  --->  Accuracy = 81.73%\n",
      "M_pca =  125 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  125 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  125 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  125 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  125 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  20  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  125 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  125 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  125 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  125 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  125 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  125 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  125 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  125 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  125 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  126 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  126 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  126 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  126 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  126 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  126 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  126 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  126 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  126 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  126 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  126 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  126 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  126 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  126 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  126 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  126 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  126 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  126 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  32  --->  Accuracy = 92.31%\n",
      "M_pca =  126 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  126 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  126 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  126 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  126 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  126 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  126 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  127 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  127 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  127 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  127 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  127 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  127 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  127 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  127 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  127 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  127 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  127 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  127 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  127 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  127 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  127 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  127 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  127 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  127 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  127 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  127 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  127 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  127 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  127 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  127 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  127 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  127 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  127 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  127 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  127 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  127 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  127 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  128 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  128 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  128 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  128 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  128 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  128 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  128 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  128 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  128 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  128 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  128 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  128 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  128 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  128 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  128 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  128 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  22  --->  Accuracy = 90.38%\n",
      "M_pca =  128 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  27  --->  Accuracy = 90.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  128 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  128 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  128 , M_lda =  30  --->  Accuracy = 92.31%\n",
      "M_pca =  128 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  128 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  128 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  128 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  128 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  128 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  128 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  128 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  128 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  128 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  128 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  129 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  129 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  129 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  129 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  129 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  129 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  129 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  129 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  129 , M_lda =  10  --->  Accuracy = 82.69%\n",
      "M_pca =  129 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  129 , M_lda =  12  --->  Accuracy = 85.58%\n",
      "M_pca =  129 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  129 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  129 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  129 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  129 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  129 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  129 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  129 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  130 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  130 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  130 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  130 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  130 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  130 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  130 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  130 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  130 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  130 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  130 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  130 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  130 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  130 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  130 , M_lda =  16  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  130 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  130 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  130 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  130 , M_lda =  21  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  130 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  130 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  130 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  130 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  130 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  130 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  130 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  130 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  130 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  130 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  130 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  130 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  130 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  131 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  131 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  131 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  131 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  131 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  131 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  131 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  131 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  131 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  131 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  131 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  131 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  131 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  18  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  131 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  131 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  131 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  131 , M_lda =  27  --->  Accuracy = 90.38%\n",
      "M_pca =  131 , M_lda =  28  --->  Accuracy = 91.35%\n",
      "M_pca =  131 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  131 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  131 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  34  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  131 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  131 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  131 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  131 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  131 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  131 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  132 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  132 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  132 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  132 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  132 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  132 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  132 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  132 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  132 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  132 , M_lda =  11  --->  Accuracy = 84.62%\n",
      "M_pca =  132 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  132 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  132 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  132 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  132 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  132 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  132 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  132 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  132 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  132 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  132 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  132 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  132 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  132 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  133 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  133 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  133 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  133 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  133 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  133 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  133 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  133 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  133 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  133 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  133 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  133 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  133 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  133 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  16  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  133 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  133 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  133 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  133 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  133 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  133 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  133 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  133 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  133 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  133 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  133 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  133 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  133 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  133 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  133 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  133 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  133 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  133 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  134 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  134 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  134 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  134 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  134 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  134 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  134 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  134 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  134 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  134 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  134 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  134 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  134 , M_lda =  14  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  134 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  134 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  134 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  134 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  134 , M_lda =  39  --->  Accuracy = 92.31%\n",
      "M_pca =  134 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  41  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  134 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  134 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  135 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  135 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  135 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  135 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  135 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  135 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  135 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  135 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  135 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  135 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  135 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  135 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  135 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  135 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  135 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  135 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  135 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  135 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  135 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  135 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  135 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  135 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  135 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  135 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  136 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  136 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  136 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  136 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  136 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  136 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  136 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  136 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  136 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  136 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  136 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  136 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  136 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  136 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  136 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  136 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  136 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  136 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  136 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  136 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  136 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  136 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  34  --->  Accuracy = 91.35%\n",
      "M_pca =  136 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  136 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  136 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  137 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  137 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  137 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  137 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  137 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  137 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  137 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  137 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  137 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  137 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  137 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  137 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  137 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  137 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  137 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  137 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  137 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  137 , M_lda =  19  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  137 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  137 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  137 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  137 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  137 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  137 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  137 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  137 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  137 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  137 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  37  --->  Accuracy = 91.35%\n",
      "M_pca =  137 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  137 , M_lda =  40  --->  Accuracy = 91.35%\n",
      "M_pca =  137 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  137 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  137 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  137 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  137 , M_lda =  48  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  137 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  137 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  138 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  138 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  138 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  138 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  138 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  138 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  138 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  138 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  138 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  138 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  138 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  138 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  138 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  138 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  138 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  138 , M_lda =  18  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  138 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  138 , M_lda =  22  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  138 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  25  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  138 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  138 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  138 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  40  --->  Accuracy = 91.35%\n",
      "M_pca =  138 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  138 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  138 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  138 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  139 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  139 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  139 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  139 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  139 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  139 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  139 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  139 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  139 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  139 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  139 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  139 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  139 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  139 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  139 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  139 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  139 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  139 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  139 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  139 , M_lda =  21  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  139 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  139 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  139 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  139 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  139 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  139 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  35  --->  Accuracy = 92.31%\n",
      "M_pca =  139 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  37  --->  Accuracy = 91.35%\n",
      "M_pca =  139 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  139 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  139 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  139 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  140 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  140 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  140 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  140 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  140 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  140 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  140 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  140 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  140 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  140 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  140 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  140 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  140 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  140 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  140 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  140 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  140 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  140 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  140 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  140 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  140 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  140 , M_lda =  25  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  140 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  140 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  140 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  140 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  39  --->  Accuracy = 92.31%\n",
      "M_pca =  140 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  140 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  140 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  140 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  141 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  141 , M_lda =  3  --->  Accuracy = 25.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  141 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  141 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  141 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  141 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  141 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  141 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  141 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  141 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  141 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  141 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  141 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  141 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  141 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  141 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  141 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  141 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  141 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  141 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  141 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  141 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  141 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  141 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  141 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  35  --->  Accuracy = 93.27%\n",
      "M_pca =  141 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  141 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  141 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  141 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  141 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  141 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  141 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  141 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  142 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  142 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  142 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  142 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  142 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  142 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  142 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  142 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  142 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  142 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  142 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  142 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  142 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  142 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  142 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  142 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  142 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  142 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  142 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  142 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  142 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  142 , M_lda =  23  --->  Accuracy = 90.38%\n",
      "M_pca =  142 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  142 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  142 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  142 , M_lda =  27  --->  Accuracy = 92.31%\n",
      "M_pca =  142 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  142 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  142 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  142 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  142 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  142 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  37  --->  Accuracy = 91.35%\n",
      "M_pca =  142 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  142 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  142 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  142 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  142 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  142 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  142 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  142 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  142 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  142 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  142 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  143 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  143 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  143 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  143 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  143 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  143 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  143 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  143 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  143 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  143 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  143 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  143 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  143 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  143 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  143 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  143 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  143 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  143 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  143 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  143 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  143 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  143 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  143 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  143 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  143 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  143 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  143 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  143 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  143 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  144 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  144 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  144 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  144 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  144 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  144 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  144 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  144 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  144 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  144 , M_lda =  10  --->  Accuracy = 75.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  144 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  144 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  144 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  144 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  144 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  144 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  144 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  144 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  144 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  144 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  144 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  144 , M_lda =  38  --->  Accuracy = 92.31%\n",
      "M_pca =  144 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  144 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  144 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  144 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  144 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  144 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  144 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  144 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  145 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  145 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  145 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  145 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  145 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  145 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  145 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  145 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  145 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  145 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  145 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  145 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  145 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  145 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  145 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  145 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  145 , M_lda =  18  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  145 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  145 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  145 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  145 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  145 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  145 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  27  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  145 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  145 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  145 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  145 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  146 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  146 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  146 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  146 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  146 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  146 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  146 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  146 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  146 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  146 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  146 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  146 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  146 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  146 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  146 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  146 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  146 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  146 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  146 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  146 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  146 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  146 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  146 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  146 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  146 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  146 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  146 , M_lda =  28  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  146 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  146 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  34  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  146 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  146 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  146 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  146 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  146 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  146 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  146 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  146 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  50  --->  Accuracy = 93.27%\n",
      "M_pca =  146 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  147 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  147 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  147 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  147 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  147 , M_lda =  6  --->  Accuracy = 64.42%\n",
      "M_pca =  147 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  147 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  147 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  147 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  147 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  147 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  147 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  147 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  147 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  147 , M_lda =  16  --->  Accuracy = 85.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  147 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  147 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  147 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  147 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  147 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  147 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  147 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  147 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  147 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  147 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  147 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  147 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  147 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  147 , M_lda =  48  --->  Accuracy = 93.27%\n",
      "M_pca =  147 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  148 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  148 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  148 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  148 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  148 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  148 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  148 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  148 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  148 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  148 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  148 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  148 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  148 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  148 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  148 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  148 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  148 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  148 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  148 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  148 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  148 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  148 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  148 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  148 , M_lda =  25  --->  Accuracy = 90.38%\n",
      "M_pca =  148 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  148 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  148 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  148 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  148 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  148 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  148 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  148 , M_lda =  33  --->  Accuracy = 91.35%\n",
      "M_pca =  148 , M_lda =  34  --->  Accuracy = 91.35%\n",
      "M_pca =  148 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  148 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  148 , M_lda =  37  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  148 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  148 , M_lda =  40  --->  Accuracy = 93.27%\n",
      "M_pca =  148 , M_lda =  41  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  148 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  148 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  46  --->  Accuracy = 93.27%\n",
      "M_pca =  148 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  48  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  148 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  148 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  149 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  149 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  149 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  149 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  149 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  149 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  149 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  149 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  149 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  149 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  149 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  149 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  149 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  149 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  149 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  149 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  149 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  149 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  149 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  149 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  149 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  149 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  149 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  149 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  149 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  149 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  149 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  149 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  149 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  149 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  149 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  149 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  149 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  149 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  149 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  149 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  149 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  149 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  149 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  149 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  149 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  149 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  149 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  149 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  150 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  150 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  150 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  150 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  150 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  150 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  150 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  150 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  150 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  150 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  150 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  150 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  150 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  150 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  150 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  150 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  150 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  150 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  150 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  150 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  150 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  150 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  150 , M_lda =  23  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  150 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  150 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  150 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  150 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  150 , M_lda =  31  --->  Accuracy = 92.31%\n",
      "M_pca =  150 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  150 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  150 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  150 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  150 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  150 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  150 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  150 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  150 , M_lda =  43  --->  Accuracy = 93.27%\n",
      "M_pca =  150 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  150 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  150 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  150 , M_lda =  47  --->  Accuracy = 94.23%\n",
      "M_pca =  150 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  150 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  150 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  151 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  151 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  151 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  151 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  151 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  151 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  151 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  151 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  151 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  151 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  151 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  151 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  151 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  151 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  151 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  151 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  151 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  151 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  151 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  151 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  151 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  151 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  151 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  151 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  151 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  151 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  151 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  151 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  151 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  151 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  151 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  151 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  35  --->  Accuracy = 92.31%\n",
      "M_pca =  151 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  151 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  151 , M_lda =  38  --->  Accuracy = 92.31%\n",
      "M_pca =  151 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  41  --->  Accuracy = 93.27%\n",
      "M_pca =  151 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  151 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  151 , M_lda =  44  --->  Accuracy = 93.27%\n",
      "M_pca =  151 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  151 , M_lda =  46  --->  Accuracy = 94.23%\n",
      "M_pca =  151 , M_lda =  47  --->  Accuracy = 93.27%\n",
      "M_pca =  151 , M_lda =  48  --->  Accuracy = 94.23%\n",
      "M_pca =  151 , M_lda =  49  --->  Accuracy = 93.27%\n",
      "M_pca =  151 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  152 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  152 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  152 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  152 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  152 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  152 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  152 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  152 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  152 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  152 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  152 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  152 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  152 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  152 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  152 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  152 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  152 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  152 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  152 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  152 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  152 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  152 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  152 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  152 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  152 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  31  --->  Accuracy = 92.31%\n",
      "M_pca =  152 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  152 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  152 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  38  --->  Accuracy = 92.31%\n",
      "M_pca =  152 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  152 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  45  --->  Accuracy = 93.27%\n",
      "M_pca =  152 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  152 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  152 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  153 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  153 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  153 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  153 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  153 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  153 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  153 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  153 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  153 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  153 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  153 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  153 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  153 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  153 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  153 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  153 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  153 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  153 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  153 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  153 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  153 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  153 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  28  --->  Accuracy = 91.35%\n",
      "M_pca =  153 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  153 , M_lda =  30  --->  Accuracy = 90.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  153 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  153 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  153 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  153 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  153 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  153 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  153 , M_lda =  37  --->  Accuracy = 92.31%\n",
      "M_pca =  153 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  153 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  153 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  153 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  153 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  153 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  153 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  153 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  153 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  153 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  153 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  153 , M_lda =  49  --->  Accuracy = 93.27%\n",
      "M_pca =  153 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  153 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  154 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  154 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  154 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  154 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  154 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  154 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  154 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  154 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  154 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  154 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  154 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  154 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  154 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  154 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  154 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  154 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  154 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  154 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  154 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  154 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  154 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  154 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  154 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  154 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  154 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  154 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  154 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  154 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  154 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  31  --->  Accuracy = 93.27%\n",
      "M_pca =  154 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  154 , M_lda =  33  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  34  --->  Accuracy = 92.31%\n",
      "M_pca =  154 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  154 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  154 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  154 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  154 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  48  --->  Accuracy = 94.23%\n",
      "M_pca =  154 , M_lda =  49  --->  Accuracy = 93.27%\n",
      "M_pca =  154 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  155 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  155 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  155 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  155 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  155 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  155 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  155 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  155 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  155 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  155 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  155 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  155 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  155 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  155 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  155 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  155 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  155 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  155 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  155 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  155 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  27  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  28  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  155 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  155 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  155 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  155 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  155 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  155 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  155 , M_lda =  42  --->  Accuracy = 93.27%\n",
      "M_pca =  155 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  155 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  155 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  1  --->  Accuracy = 12.50%\n",
      "M_pca =  156 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  156 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  156 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  156 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  156 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  156 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  156 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  156 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  156 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  156 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  156 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  156 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  156 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  156 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  156 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  156 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  156 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  156 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  156 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  156 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  156 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  156 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  156 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  156 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  156 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  156 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  156 , M_lda =  28  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  29  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  156 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  156 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  156 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  156 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  156 , M_lda =  37  --->  Accuracy = 91.35%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  156 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  156 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  156 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  156 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  156 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  45  --->  Accuracy = 93.27%\n",
      "M_pca =  156 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  156 , M_lda =  48  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  51  --->  Accuracy = 93.27%\n",
      "M_pca =  157 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  157 , M_lda =  2  --->  Accuracy = 24.04%\n",
      "M_pca =  157 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  157 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  157 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  157 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  157 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  157 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  157 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  157 , M_lda =  10  --->  Accuracy = 82.69%\n",
      "M_pca =  157 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  157 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  157 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  157 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  157 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  157 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  157 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  157 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  157 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  157 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  157 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  157 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  157 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  157 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  157 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  157 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  157 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  157 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  157 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  157 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  157 , M_lda =  33  --->  Accuracy = 92.31%\n",
      "M_pca =  157 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  157 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  157 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  157 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  157 , M_lda =  41  --->  Accuracy = 92.31%\n",
      "M_pca =  157 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  157 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  157 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  157 , M_lda =  46  --->  Accuracy = 94.23%\n",
      "M_pca =  157 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  157 , M_lda =  48  --->  Accuracy = 92.31%\n",
      "M_pca =  157 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  157 , M_lda =  50  --->  Accuracy = 93.27%\n",
      "M_pca =  157 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  158 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  158 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  158 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  158 , M_lda =  4  --->  Accuracy = 52.88%\n",
      "M_pca =  158 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  158 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  158 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  158 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  158 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  158 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  158 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  158 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  158 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  158 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  158 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  158 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  158 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  158 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  158 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  158 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  158 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  158 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  158 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  158 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  158 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  158 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  158 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  158 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  158 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  158 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  158 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  158 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  158 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  158 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  158 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  158 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  158 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  158 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  158 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  158 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  158 , M_lda =  47  --->  Accuracy = 93.27%\n",
      "M_pca =  158 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  158 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  159 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  159 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  159 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  159 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  159 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  159 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  159 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  159 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  159 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  159 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  159 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  159 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  159 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  159 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  159 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  159 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  159 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  159 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  159 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  159 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  159 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  159 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  159 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  159 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  159 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  159 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  159 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  159 , M_lda =  30  --->  Accuracy = 93.27%\n",
      "M_pca =  159 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  159 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  159 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  159 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  159 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  159 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  159 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  159 , M_lda =  43  --->  Accuracy = 89.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  159 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  159 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  159 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  159 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  159 , M_lda =  48  --->  Accuracy = 93.27%\n",
      "M_pca =  159 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  159 , M_lda =  50  --->  Accuracy = 93.27%\n",
      "M_pca =  159 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  160 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  160 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  160 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  160 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  160 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  160 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  160 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  160 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  160 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  160 , M_lda =  10  --->  Accuracy = 81.73%\n",
      "M_pca =  160 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  160 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  160 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  160 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  160 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  160 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  160 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  160 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  160 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  160 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  160 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  160 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  160 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  160 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  160 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  160 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  160 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  160 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  160 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  160 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  160 , M_lda =  31  --->  Accuracy = 92.31%\n",
      "M_pca =  160 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  160 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  160 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  160 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  160 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  160 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  160 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  160 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  160 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  160 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  160 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  160 , M_lda =  43  --->  Accuracy = 93.27%\n",
      "M_pca =  160 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  160 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  160 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  160 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  160 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  160 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  160 , M_lda =  50  --->  Accuracy = 93.27%\n",
      "M_pca =  160 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  161 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  161 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  161 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  161 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  161 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  161 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  161 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  161 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  161 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  161 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  161 , M_lda =  11  --->  Accuracy = 85.58%\n",
      "M_pca =  161 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  161 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  161 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  161 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  161 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  161 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  161 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  161 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  161 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  161 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  161 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  161 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  161 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  28  --->  Accuracy = 93.27%\n",
      "M_pca =  161 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  161 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  161 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  161 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  161 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  161 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  161 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  161 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  161 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  161 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  161 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  161 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  161 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  161 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  161 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  161 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  161 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  161 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  161 , M_lda =  51  --->  Accuracy = 93.27%\n",
      "M_pca =  162 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  162 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  162 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  162 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  162 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  162 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  162 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  162 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  162 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  162 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  162 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  162 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  162 , M_lda =  13  --->  Accuracy = 89.42%\n",
      "M_pca =  162 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  162 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  162 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  162 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  162 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  162 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  162 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  162 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  162 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  162 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  162 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  162 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  162 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  162 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  162 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  162 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  162 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  162 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  162 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  162 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  162 , M_lda =  34  --->  Accuracy = 91.35%\n",
      "M_pca =  162 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  162 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  162 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  162 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  162 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  162 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  162 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  162 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  162 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  162 , M_lda =  44  --->  Accuracy = 94.23%\n",
      "M_pca =  162 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  162 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  162 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  162 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  162 , M_lda =  49  --->  Accuracy = 90.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  162 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  162 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  163 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  163 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  163 , M_lda =  3  --->  Accuracy = 38.46%\n",
      "M_pca =  163 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  163 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  163 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  163 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  163 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  163 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  163 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  163 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  163 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  163 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  163 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  163 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  163 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  163 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  163 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  163 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  163 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  163 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  163 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  163 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  163 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  163 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  163 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  163 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  163 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  163 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  41  --->  Accuracy = 92.31%\n",
      "M_pca =  163 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  163 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  163 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  163 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  163 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  163 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  163 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  163 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  163 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  163 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  164 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  164 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  164 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  164 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  164 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  164 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  164 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  164 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  164 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  164 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  164 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  164 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  164 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  164 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  164 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  164 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  164 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  164 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  164 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  164 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  164 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  164 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  164 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  164 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  164 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  164 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  164 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  164 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  164 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  164 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  164 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  164 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  164 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  43  --->  Accuracy = 93.27%\n",
      "M_pca =  164 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  164 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  164 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  164 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  165 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  165 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  165 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  165 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  165 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  165 , M_lda =  7  --->  Accuracy = 75.00%\n",
      "M_pca =  165 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  165 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  165 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  165 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  165 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  165 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  165 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  165 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  165 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  165 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  165 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  165 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  165 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  165 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  165 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  165 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  165 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  165 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  165 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  165 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  165 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  165 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  165 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  165 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  165 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  165 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  165 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  165 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  165 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  165 , M_lda =  40  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  165 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  165 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  166 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  166 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  166 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  166 , M_lda =  4  --->  Accuracy = 39.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  166 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  166 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  166 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  166 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  166 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  166 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  166 , M_lda =  11  --->  Accuracy = 84.62%\n",
      "M_pca =  166 , M_lda =  12  --->  Accuracy = 85.58%\n",
      "M_pca =  166 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  166 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  166 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  166 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  166 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  166 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  166 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  166 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  166 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  166 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  166 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  166 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  166 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  166 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  166 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  166 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  166 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  166 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  166 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  166 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  166 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  166 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  166 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  166 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  166 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  166 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  167 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  167 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  167 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  167 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  167 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  167 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  167 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  167 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  167 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  167 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  167 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  167 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  167 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  167 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  167 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  167 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  167 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  167 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  167 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  167 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  167 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  167 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  167 , M_lda =  23  --->  Accuracy = 91.35%\n",
      "M_pca =  167 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  167 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  167 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  167 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  167 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  167 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  167 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  167 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  167 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  34  --->  Accuracy = 92.31%\n",
      "M_pca =  167 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  167 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  167 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  167 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  167 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  167 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  167 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  167 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  167 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  167 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  167 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  167 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  167 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  168 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  168 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  168 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  168 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  168 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  168 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  168 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  168 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  168 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  168 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  168 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  168 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  168 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  168 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  168 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  168 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  168 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  168 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  168 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  168 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  168 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  168 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  168 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  168 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  168 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  168 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  168 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  168 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  168 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  168 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  168 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  168 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  168 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  168 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  168 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  168 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  168 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  168 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  169 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  169 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  169 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  169 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  169 , M_lda =  5  --->  Accuracy = 56.73%\n",
      "M_pca =  169 , M_lda =  6  --->  Accuracy = 67.31%\n",
      "M_pca =  169 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  169 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  169 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  169 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  169 , M_lda =  11  --->  Accuracy = 79.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  169 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  169 , M_lda =  13  --->  Accuracy = 88.46%\n",
      "M_pca =  169 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  169 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  169 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  169 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  169 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  169 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  169 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  169 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  169 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  169 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  169 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  169 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  169 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  169 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  169 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  169 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  169 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  169 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  170 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  170 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  170 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  170 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  170 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  170 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  170 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  170 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  170 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  170 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  170 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  170 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  170 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  170 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  170 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  170 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  170 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  170 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  170 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  170 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  170 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  170 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  170 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  170 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  170 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  170 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  170 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  170 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  170 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  170 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  170 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  171 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  171 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  171 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  171 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  171 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  171 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  171 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  171 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  171 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  171 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  171 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  171 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  171 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  171 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  171 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  171 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  171 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  171 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  171 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  171 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  171 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  171 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  171 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  171 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  171 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  171 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  171 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  171 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  171 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  171 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  171 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  171 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  171 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  171 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  171 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  171 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  171 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  171 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  171 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  171 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  171 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  171 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  171 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  171 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  171 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  171 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  171 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  171 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  171 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  171 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  171 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  172 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  172 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  172 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  172 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  172 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  172 , M_lda =  7  --->  Accuracy = 72.12%\n",
      "M_pca =  172 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  172 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  172 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  172 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  172 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  172 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  172 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  172 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  172 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  172 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  172 , M_lda =  18  --->  Accuracy = 85.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  172 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  172 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  172 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  172 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  172 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  172 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  172 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  172 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  172 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  172 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  172 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  172 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  172 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  172 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  172 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  172 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  172 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  172 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  172 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  172 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  172 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  172 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  173 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  173 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  173 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  173 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  173 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  173 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  173 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  173 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  173 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  173 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  173 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  173 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  173 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  173 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  173 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  173 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  173 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  173 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  173 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  173 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  173 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  173 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  173 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  173 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  173 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  173 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  173 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  173 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  173 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  173 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  173 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  173 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  173 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  173 , M_lda =  43  --->  Accuracy = 93.27%\n",
      "M_pca =  173 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  173 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  173 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  174 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  174 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  174 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  174 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  174 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  174 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  174 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  174 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  174 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  174 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  174 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  174 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  174 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  174 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  174 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  174 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  174 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  174 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  174 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  174 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  174 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  174 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  174 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  25  --->  Accuracy = 90.38%\n",
      "M_pca =  174 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  174 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  174 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  174 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  174 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  174 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  174 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  174 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  174 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  174 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  174 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  174 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  175 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  175 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  175 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  175 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  175 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  175 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  175 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  175 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  175 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  175 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  175 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  175 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  175 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  175 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  175 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  175 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  175 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  175 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  175 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  175 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  175 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  175 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  175 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  175 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  25  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  175 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  175 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  175 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  175 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  175 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  175 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  175 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  175 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  175 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  45  --->  Accuracy = 94.23%\n",
      "M_pca =  175 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  175 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  175 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  176 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  176 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  176 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  176 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  176 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  176 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  176 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  176 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  176 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  176 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  176 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  176 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  176 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  176 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  176 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  176 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  176 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  176 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  176 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  176 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  176 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  176 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  176 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  176 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  176 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  176 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  176 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  176 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  176 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  176 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  176 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  176 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  176 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  176 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  176 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  176 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  176 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  176 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  176 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  176 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  176 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  176 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  176 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  177 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  177 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  177 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  177 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  177 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  177 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  177 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  177 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  177 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  177 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  177 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  177 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  177 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  177 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  177 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  177 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  177 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  177 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  177 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  177 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  177 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  177 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  177 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  177 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  177 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  177 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  177 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  177 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  177 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  177 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  177 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  177 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  177 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  177 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  177 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  177 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  177 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  177 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  178 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  178 , M_lda =  2  --->  Accuracy = 25.96%\n",
      "M_pca =  178 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  178 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  178 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  178 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  178 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  178 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  178 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  178 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  178 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  178 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  178 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  178 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  178 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  178 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  178 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  178 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  178 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  178 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  178 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  178 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  32  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  178 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  178 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  178 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  178 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  178 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  178 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  178 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  179 , M_lda =  1  --->  Accuracy = 13.46%\n",
      "M_pca =  179 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  179 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  179 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  179 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  179 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  179 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  179 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  179 , M_lda =  9  --->  Accuracy = 78.85%\n",
      "M_pca =  179 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  179 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  179 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  179 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  179 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  179 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  179 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  179 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  179 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  179 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  179 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  179 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  179 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  179 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  179 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  179 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  179 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  179 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  179 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  179 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  179 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  179 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  179 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  179 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  179 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  179 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  179 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  179 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  179 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  179 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  179 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  179 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  179 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  179 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  179 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  179 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  179 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  179 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  179 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  179 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  179 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  179 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  180 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  180 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  180 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  180 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  180 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  180 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  180 , M_lda =  8  --->  Accuracy = 75.96%\n",
      "M_pca =  180 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  180 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  180 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  180 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  180 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  180 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  180 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  180 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  180 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  180 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  180 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  180 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  180 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  180 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  180 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  180 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  180 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  180 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  180 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  180 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  180 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  180 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  180 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  180 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  180 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  180 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  180 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  180 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  180 , M_lda =  48  --->  Accuracy = 93.27%\n",
      "M_pca =  180 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  180 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  181 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  181 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  181 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  181 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  181 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  181 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  181 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  181 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  181 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  181 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  181 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  181 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  181 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  181 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  181 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  181 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  181 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  181 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  181 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  181 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  181 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  181 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  181 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  181 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  181 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  181 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  181 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  181 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  181 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  181 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  181 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  181 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  181 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  181 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  181 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  181 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  181 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  181 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  181 , M_lda =  39  --->  Accuracy = 89.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  181 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  181 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  181 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  181 , M_lda =  43  --->  Accuracy = 93.27%\n",
      "M_pca =  181 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  181 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  181 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  181 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  181 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  181 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  181 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  181 , M_lda =  51  --->  Accuracy = 93.27%\n",
      "M_pca =  182 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  182 , M_lda =  2  --->  Accuracy = 24.04%\n",
      "M_pca =  182 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  182 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  182 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  182 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  182 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  182 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  182 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  182 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  182 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  182 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  182 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  182 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  182 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  182 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  182 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  182 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  182 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  182 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  182 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  182 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  182 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  182 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  182 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  182 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  182 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  182 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  182 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  182 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  182 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  183 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  183 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  183 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  183 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  183 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  183 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  183 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  183 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  183 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  183 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  183 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  183 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  183 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  183 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  183 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  183 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  183 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  183 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  183 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  183 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  183 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  183 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  183 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  183 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  183 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  183 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  183 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  183 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  183 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  183 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  183 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  183 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  184 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  184 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  184 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  184 , M_lda =  4  --->  Accuracy = 53.85%\n",
      "M_pca =  184 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  184 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  184 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  184 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  184 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  184 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  184 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  184 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  184 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  184 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  184 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  184 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  184 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  184 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  184 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  184 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  184 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  184 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  184 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  184 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  184 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  184 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  184 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  184 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  184 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  184 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  184 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  184 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  184 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  184 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  184 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  184 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  184 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  184 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  184 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  184 , M_lda =  46  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  184 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  184 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  184 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  184 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  184 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  185 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  185 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  185 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  185 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  185 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  185 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  185 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  185 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  185 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  185 , M_lda =  10  --->  Accuracy = 81.73%\n",
      "M_pca =  185 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  185 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  185 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  185 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  185 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  185 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  185 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  19  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  185 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  185 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  185 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  185 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  185 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  185 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  185 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  185 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  185 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  185 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  185 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  185 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  186 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  186 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  186 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  186 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  186 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  186 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  186 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  186 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  186 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  186 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  186 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  186 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  186 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  186 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  186 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  186 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  186 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  186 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  186 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  186 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  186 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  186 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  186 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  186 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  186 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  186 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  186 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  186 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  186 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  186 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  186 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  186 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  186 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  187 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  187 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  187 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  187 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  187 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  187 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  187 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  187 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  187 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  187 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  187 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  187 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  187 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  187 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  187 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  187 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  187 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  187 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  187 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  187 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  187 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  187 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  187 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  187 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  187 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  187 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  187 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  187 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  187 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  187 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  187 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  187 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  187 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  187 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  187 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  187 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  187 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  188 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  188 , M_lda =  2  --->  Accuracy = 15.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  188 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  188 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  188 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  188 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  188 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  188 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  188 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  188 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  188 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  188 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  188 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  188 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  188 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  188 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  188 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  188 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  188 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  188 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  188 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  188 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  188 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  188 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  188 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  188 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  188 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  188 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  188 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  188 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  188 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  188 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  189 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  189 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  189 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  189 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  189 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  189 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  189 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  189 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  189 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  189 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  189 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  189 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  189 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  189 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  189 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  189 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  189 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  189 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  189 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  189 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  189 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  189 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  189 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  189 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  189 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  189 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  189 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  189 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  189 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  189 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  189 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  189 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  189 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  189 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  189 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  189 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  189 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  189 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  189 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  190 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  190 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  190 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  190 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  190 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  190 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  190 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  190 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  190 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  190 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  190 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  190 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  190 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  190 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  190 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  190 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  190 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  190 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  190 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  190 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  190 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  190 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  190 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  190 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  190 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  190 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  190 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  190 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  190 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  190 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  190 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  190 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  191 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  191 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  191 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  191 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  191 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  191 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  191 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  191 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  191 , M_lda =  9  --->  Accuracy = 73.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  191 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  191 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  191 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  191 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  191 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  191 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  191 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  191 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  191 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  191 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  191 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  191 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  191 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  191 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  191 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  191 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  191 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  191 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  191 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  191 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  191 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  191 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  191 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  191 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  191 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  191 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  191 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  192 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  192 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  192 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  192 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  192 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  192 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  192 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  192 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  192 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  192 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  192 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  192 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  192 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  192 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  192 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  192 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  192 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  192 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  192 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  192 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  192 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  192 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  192 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  192 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  192 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  192 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  192 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  192 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  192 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  192 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  192 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  192 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  192 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  192 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  192 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  192 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  192 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  192 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  192 , M_lda =  44  --->  Accuracy = 93.27%\n",
      "M_pca =  192 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  192 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  192 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  192 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  192 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  192 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  193 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  193 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  193 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  193 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  193 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  193 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  193 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  193 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  193 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  193 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  193 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  193 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  193 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  193 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  193 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  193 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  193 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  193 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  193 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  193 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  193 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  193 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  193 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  193 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  193 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  193 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  193 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  193 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  193 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  193 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  193 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  193 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  193 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  193 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  193 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  193 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  193 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  194 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  194 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  194 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  194 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  194 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  194 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  194 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  194 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  194 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  194 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  194 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  194 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  194 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  194 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  194 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  194 , M_lda =  16  --->  Accuracy = 84.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  194 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  194 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  194 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  194 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  194 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  194 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  194 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  194 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  194 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  194 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  194 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  194 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  194 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  194 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  194 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  194 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  194 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  194 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  194 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  194 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  194 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  194 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  194 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  194 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  194 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  194 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  194 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  194 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  195 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  195 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  195 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  195 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  195 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  195 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  195 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  195 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  195 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  195 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  195 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  195 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  195 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  195 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  195 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  195 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  195 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  195 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  195 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  195 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  195 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  195 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  195 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  195 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  195 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  195 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  195 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  195 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  195 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  195 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  195 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  195 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  195 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  195 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  196 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  196 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  196 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  196 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  196 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  196 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  196 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  196 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  196 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  196 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  196 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  196 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  196 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  196 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  196 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  196 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  196 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  196 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  196 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  196 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  196 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  196 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  196 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  196 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  196 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  196 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  196 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  196 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  196 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  196 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  196 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  197 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  197 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  197 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  197 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  197 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  197 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  197 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  197 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  197 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  197 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  197 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  197 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  197 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  197 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  197 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  197 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  197 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  197 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  197 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  23  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  197 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  197 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  197 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  41  --->  Accuracy = 92.31%\n",
      "M_pca =  197 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  197 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  197 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  197 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  197 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  197 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  197 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  197 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  197 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  198 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  198 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  198 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  198 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  198 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  198 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  198 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  198 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  198 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  198 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  198 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  198 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  198 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  198 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  198 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  198 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  198 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  198 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  198 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  198 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  198 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  198 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  198 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  198 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  198 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  198 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  198 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  198 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  198 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  198 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  198 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  198 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  198 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  198 , M_lda =  37  --->  Accuracy = 91.35%\n",
      "M_pca =  198 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  198 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  198 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  198 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  198 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  198 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  198 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  198 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  198 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  198 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  199 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  199 , M_lda =  2  --->  Accuracy = 25.00%\n",
      "M_pca =  199 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  199 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  199 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  199 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  199 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  199 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  199 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  199 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  199 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  199 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  199 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  199 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  199 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  199 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  199 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  199 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  199 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  199 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  199 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  199 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  199 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  199 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  199 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  199 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  199 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  199 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  199 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  199 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  199 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  199 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  199 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  199 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  199 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  199 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  199 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  199 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  199 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  199 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  199 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  199 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  200 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  200 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  200 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  200 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  200 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  200 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  200 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  200 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  200 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  200 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  200 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  200 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  200 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  200 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  200 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  200 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  200 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  200 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  200 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  200 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  200 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  200 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  200 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  200 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  200 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  200 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  200 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  200 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  29  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  200 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  200 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  200 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  200 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  200 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  200 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  200 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  200 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  200 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  200 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  200 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  200 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  200 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  200 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  200 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  200 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  200 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  201 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  201 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  201 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  201 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  201 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  201 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  201 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  201 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  201 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  201 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  201 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  201 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  201 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  201 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  201 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  201 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  201 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  201 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  201 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  201 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  201 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  201 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  201 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  201 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  201 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  201 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  201 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  201 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  201 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  201 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  201 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  201 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  201 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  201 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  201 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  201 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  201 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  201 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  201 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  201 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  202 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  202 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  202 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  202 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  202 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  202 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  202 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  202 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  202 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  202 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  202 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  202 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  202 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  202 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  202 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  202 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  202 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  202 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  202 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  202 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  202 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  202 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  202 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  202 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  202 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  202 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  202 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  202 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  202 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  202 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  202 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  202 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  202 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  202 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  202 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  202 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  203 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  203 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  203 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  203 , M_lda =  4  --->  Accuracy = 30.77%\n",
      "M_pca =  203 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  203 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  203 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  203 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  203 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  203 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  203 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  203 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  203 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  203 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  203 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  203 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  203 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  203 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  203 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  203 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  203 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  203 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  203 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  203 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  203 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  35  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  203 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  203 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  203 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  203 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  203 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  203 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  203 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  203 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  204 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  204 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  204 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  204 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  204 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  204 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  204 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  204 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  204 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  204 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  204 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  204 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  204 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  204 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  204 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  204 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  204 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  204 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  204 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  204 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  204 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  204 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  204 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  204 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  204 , M_lda =  47  --->  Accuracy = 93.27%\n",
      "M_pca =  204 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  204 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  205 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  205 , M_lda =  2  --->  Accuracy = 24.04%\n",
      "M_pca =  205 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  205 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  205 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  205 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  205 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  205 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  205 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  205 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  205 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  205 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  205 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  205 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  205 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  205 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  205 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  205 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  205 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  205 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  205 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  205 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  205 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  205 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  205 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  205 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  205 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  205 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  205 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  205 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  205 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  205 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  205 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  205 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  205 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  205 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  205 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  206 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  206 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  206 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  206 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  206 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  206 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  206 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  206 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  206 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  206 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  206 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  206 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  206 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  206 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  206 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  206 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  206 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  206 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  206 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  206 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  206 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  206 , M_lda =  23  --->  Accuracy = 90.38%\n",
      "M_pca =  206 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  206 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  206 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  206 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  206 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  206 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  206 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  206 , M_lda =  42  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  206 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  206 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  206 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  206 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  207 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  207 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  207 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  207 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  207 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  207 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  207 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  207 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  207 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  207 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  207 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  207 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  207 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  207 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  207 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  207 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  207 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  207 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  207 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  207 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  207 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  207 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  207 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  207 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  207 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  207 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  207 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  207 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  207 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  207 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  207 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  207 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  207 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  207 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  207 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  208 , M_lda =  1  --->  Accuracy = 14.42%\n",
      "M_pca =  208 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  208 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  208 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  208 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  208 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  208 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  208 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  208 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  208 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  208 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  208 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  208 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  208 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  208 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  208 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  208 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  208 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  208 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  208 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  208 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  208 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  208 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  208 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  208 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  208 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  208 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  208 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  208 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  208 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  208 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  208 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  208 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  208 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  208 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  208 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  209 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  209 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  209 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  209 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  209 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  209 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  209 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  209 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  209 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  209 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  209 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  209 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  209 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  209 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  209 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  209 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  209 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  209 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  209 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  209 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  209 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  209 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  209 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  209 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  209 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  209 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  209 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  209 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  209 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  209 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  209 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  209 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  209 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  209 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  209 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  209 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  209 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  209 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  209 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  209 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  209 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  209 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  209 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  209 , M_lda =  49  --->  Accuracy = 89.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  209 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  210 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  210 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  210 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  210 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  210 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  210 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  210 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  210 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  210 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  210 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  210 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  210 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  210 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  210 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  210 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  210 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  210 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  210 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  210 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  210 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  210 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  210 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  210 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  210 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  210 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  210 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  210 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  210 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  210 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  210 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  210 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  210 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  210 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  210 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  210 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  210 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  210 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  210 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  211 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  211 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  211 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  211 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  211 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  211 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  211 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  211 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  211 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  211 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  211 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  211 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  211 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  211 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  211 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  211 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  211 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  211 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  211 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  211 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  211 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  211 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  211 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  211 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  211 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  211 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  211 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  211 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  211 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  211 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  211 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  211 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  211 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  211 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  211 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  211 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  211 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  211 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  211 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  211 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  211 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  211 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  211 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  212 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  212 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  212 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  212 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  212 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  212 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  212 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  212 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  212 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  212 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  212 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  212 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  212 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  212 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  212 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  212 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  212 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  212 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  212 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  212 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  212 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  212 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  212 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  212 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  212 , M_lda =  32  --->  Accuracy = 92.31%\n",
      "M_pca =  212 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  212 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  212 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  212 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  212 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  212 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  212 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  212 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  212 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  212 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  212 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  212 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  212 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  213 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  213 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  213 , M_lda =  4  --->  Accuracy = 29.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  213 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  213 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  213 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  213 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  213 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  213 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  213 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  213 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  213 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  213 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  213 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  213 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  213 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  213 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  213 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  213 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  213 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  213 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  213 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  213 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  213 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  213 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  213 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  213 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  213 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  213 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  213 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  213 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  213 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  213 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  213 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  213 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  213 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  213 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  213 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  214 , M_lda =  2  --->  Accuracy = 24.04%\n",
      "M_pca =  214 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  214 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  214 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  214 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  214 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  214 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  214 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  214 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  214 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  214 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  214 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  214 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  214 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  214 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  214 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  214 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  214 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  214 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  214 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  214 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  214 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  214 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  214 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  214 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  214 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  214 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  214 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  214 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  214 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  214 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  215 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  215 , M_lda =  2  --->  Accuracy = 24.04%\n",
      "M_pca =  215 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  215 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  215 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  215 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  215 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  215 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  215 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  215 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  215 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  215 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  215 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  215 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  215 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  215 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  215 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  215 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  215 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  215 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  215 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  215 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  215 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  215 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  215 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  215 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  215 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  215 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  215 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  215 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  215 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  215 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  215 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  215 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  215 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  215 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  215 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  215 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  215 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  215 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  215 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  216 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  216 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  216 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  216 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  216 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  216 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  216 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  216 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  216 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  216 , M_lda =  10  --->  Accuracy = 72.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  216 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  216 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  216 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  216 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  216 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  216 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  216 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  216 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  216 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  216 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  216 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  216 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  216 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  216 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  216 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  216 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  216 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  216 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  216 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  216 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  216 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  216 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  216 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  216 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  216 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  216 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  216 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  217 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  217 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  217 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  217 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  217 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  217 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  217 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  217 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  217 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  217 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  217 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  217 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  217 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  217 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  217 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  217 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  217 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  217 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  217 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  217 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  217 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  217 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  217 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  217 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  217 , M_lda =  27  --->  Accuracy = 92.31%\n",
      "M_pca =  217 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  217 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  217 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  217 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  217 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  217 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  217 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  217 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  217 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  217 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  217 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  217 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  217 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  218 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  218 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  218 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  218 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  218 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  218 , M_lda =  7  --->  Accuracy = 56.73%\n",
      "M_pca =  218 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  218 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  218 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  218 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  218 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  218 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  218 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  218 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  218 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  218 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  218 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  218 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  218 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  218 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  218 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  218 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  218 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  218 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  218 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  218 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  37  --->  Accuracy = 92.31%\n",
      "M_pca =  218 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  218 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  218 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  218 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  218 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  218 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  49  --->  Accuracy = 93.27%\n",
      "M_pca =  218 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  219 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  219 , M_lda =  2  --->  Accuracy = 26.92%\n",
      "M_pca =  219 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  219 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  219 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  219 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  219 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  219 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  219 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  219 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  219 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  219 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  219 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  219 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  219 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  219 , M_lda =  16  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  219 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  219 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  219 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  219 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  219 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  219 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  219 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  219 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  219 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  219 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  219 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  219 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  219 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  219 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  219 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  219 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  220 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  220 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  220 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  220 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  220 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  220 , M_lda =  6  --->  Accuracy = 45.19%\n",
      "M_pca =  220 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  220 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  220 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  220 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  220 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  220 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  220 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  220 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  220 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  220 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  220 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  220 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  220 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  220 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  220 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  220 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  220 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  220 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  220 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  220 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  220 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  220 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  220 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  220 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  220 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  220 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  220 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  221 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  221 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  221 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  221 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  221 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  221 , M_lda =  7  --->  Accuracy = 56.73%\n",
      "M_pca =  221 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  221 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  221 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  221 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  221 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  221 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  221 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  221 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  221 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  221 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  221 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  221 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  221 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  221 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  221 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  221 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  221 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  221 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  221 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  221 , M_lda =  37  --->  Accuracy = 92.31%\n",
      "M_pca =  221 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  221 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  221 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  221 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  221 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  221 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  221 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  221 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  221 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  221 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  222 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  222 , M_lda =  2  --->  Accuracy = 25.00%\n",
      "M_pca =  222 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  222 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  222 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  222 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  222 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  222 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  222 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  222 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  222 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  222 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  222 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  222 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  222 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  222 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  222 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  222 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  222 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  222 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  222 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  222 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  222 , M_lda =  23  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  222 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  222 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  222 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  222 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  222 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  222 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  222 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  222 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  222 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  222 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  222 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  222 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  222 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  223 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  223 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  223 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  223 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  223 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  223 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  223 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  223 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  223 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  223 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  223 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  223 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  223 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  223 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  223 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  223 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  223 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  223 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  223 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  223 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  223 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  223 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  223 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  223 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  223 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  223 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  223 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  223 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  223 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  223 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  223 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  223 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  223 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  223 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  224 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  224 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  224 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  224 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  224 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  224 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  224 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  224 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  224 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  224 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  224 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  224 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  224 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  224 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  224 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  224 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  224 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  224 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  224 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  224 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  224 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  224 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  224 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  224 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  224 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  224 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  224 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  224 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  224 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  224 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  224 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  224 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  224 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  224 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  224 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  224 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  224 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  224 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  224 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  224 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  224 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  224 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  225 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  225 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  225 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  225 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  225 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  225 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  225 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  225 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  225 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  225 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  225 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  225 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  225 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  225 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  225 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  225 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  225 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  225 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  225 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  225 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  225 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  225 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  225 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  225 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  225 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  225 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  225 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  225 , M_lda =  29  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  225 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  225 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  225 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  225 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  225 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  225 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  225 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  225 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  225 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  225 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  225 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  225 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  225 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  225 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  225 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  225 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  226 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  226 , M_lda =  2  --->  Accuracy = 26.92%\n",
      "M_pca =  226 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  226 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  226 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  226 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  226 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  226 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  226 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  226 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  226 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  226 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  226 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  226 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  226 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  226 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  226 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  226 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  226 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  226 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  226 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  226 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  226 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  226 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  226 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  226 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  226 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  226 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  226 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  226 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  226 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  226 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  226 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  226 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  226 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  226 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  226 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  226 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  226 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  226 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  226 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  226 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  226 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  226 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  226 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  227 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  227 , M_lda =  2  --->  Accuracy = 26.92%\n",
      "M_pca =  227 , M_lda =  3  --->  Accuracy = 38.46%\n",
      "M_pca =  227 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  227 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  227 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  227 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  227 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  227 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  227 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  227 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  227 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  227 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  227 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  227 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  227 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  227 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  227 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  227 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  227 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  227 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  227 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  227 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  227 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  227 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  227 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  227 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  227 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  227 , M_lda =  37  --->  Accuracy = 91.35%\n",
      "M_pca =  227 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  227 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  227 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  227 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  227 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  227 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  227 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  228 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  228 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  228 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  228 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  228 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  228 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  228 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  228 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  228 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  228 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  228 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  228 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  228 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  228 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  228 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  228 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  228 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  228 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  228 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  228 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  228 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  228 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  228 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  228 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  228 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  228 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  228 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  228 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  228 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  228 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  228 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  228 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  228 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  228 , M_lda =  35  --->  Accuracy = 89.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  228 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  228 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  228 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  228 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  228 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  228 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  228 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  228 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  228 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  228 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  228 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  229 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  229 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  229 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  229 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  229 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  229 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  229 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  229 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  229 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  229 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  229 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  229 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  229 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  229 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  229 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  229 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  229 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  229 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  229 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  229 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  229 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  229 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  229 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  229 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  229 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  229 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  229 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  229 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  229 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  229 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  229 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  229 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  229 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  229 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  229 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  229 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  229 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  229 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  230 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  230 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  230 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  230 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  230 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  230 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  230 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  230 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  230 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  230 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  230 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  230 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  230 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  230 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  230 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  230 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  230 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  230 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  230 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  230 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  230 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  230 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  230 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  230 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  230 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  230 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  230 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  230 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  230 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  230 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  230 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  230 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  230 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  230 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  230 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  230 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  230 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  230 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  230 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  230 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  230 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  230 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  230 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  230 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  231 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  231 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  231 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  231 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  231 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  231 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  231 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  231 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  231 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  231 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  231 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  231 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  231 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  231 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  231 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  231 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  231 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  231 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  231 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  231 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  231 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  231 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  231 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  231 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  231 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  231 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  231 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  231 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  231 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  231 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  231 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  231 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  231 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  231 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  231 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  41  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  231 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  231 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  231 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  231 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  231 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  231 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  232 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  232 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  232 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  232 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  232 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  232 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  232 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  232 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  232 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  232 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  232 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  232 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  232 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  232 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  232 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  232 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  232 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  232 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  232 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  232 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  232 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  232 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  232 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  232 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  232 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  232 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  232 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  232 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  232 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  232 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  232 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  232 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  232 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  232 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  232 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  232 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  232 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  232 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  233 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  233 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  233 , M_lda =  3  --->  Accuracy = 19.23%\n",
      "M_pca =  233 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  233 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  233 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  233 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  233 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  233 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  233 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  233 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  233 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  233 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  233 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  233 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  233 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  233 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  233 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  233 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  233 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  233 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  233 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  233 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  233 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  233 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  233 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  233 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  233 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  233 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  233 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  233 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  233 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  233 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  233 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  233 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  233 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  233 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  233 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  233 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  233 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  233 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  233 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  233 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  233 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  233 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  233 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  233 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  233 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  233 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  233 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  233 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  234 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  234 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  234 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  234 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  234 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  234 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  234 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  234 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  234 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  234 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  234 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  234 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  234 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  234 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  234 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  234 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  234 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  234 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  234 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  234 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  234 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  234 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  234 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  234 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  234 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  234 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  234 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  234 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  234 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  234 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  234 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  234 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  234 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  234 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  47  --->  Accuracy = 89.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  234 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  234 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  234 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  234 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  235 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  235 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  235 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  235 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  235 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  235 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  235 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  235 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  235 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  235 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  235 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  235 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  235 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  235 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  235 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  235 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  235 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  235 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  235 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  235 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  235 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  235 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  235 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  235 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  235 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  235 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  235 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  235 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  235 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  235 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  235 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  235 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  235 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  235 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  235 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  235 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  235 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  235 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  235 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  235 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  235 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  235 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  235 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  235 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  235 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  235 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  235 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  235 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  235 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  235 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  235 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  236 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  236 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  236 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  236 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  236 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  236 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  236 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  236 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  236 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  236 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  236 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  236 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  236 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  236 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  236 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  236 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  236 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  236 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  236 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  236 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  236 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  236 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  236 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  236 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  236 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  236 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  236 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  236 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  236 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  236 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  236 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  236 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  236 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  236 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  236 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  236 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  236 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  236 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  236 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  236 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  236 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  236 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  237 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  237 , M_lda =  2  --->  Accuracy = 28.85%\n",
      "M_pca =  237 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  237 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  237 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  237 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  237 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  237 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  237 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  237 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  237 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  237 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  237 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  237 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  237 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  237 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  237 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  237 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  237 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  237 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  237 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  237 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  237 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  237 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  237 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  237 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  237 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  237 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  237 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  237 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  237 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  237 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  237 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  237 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  237 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  237 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  237 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  238 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  238 , M_lda =  2  --->  Accuracy = 17.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  238 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  238 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  238 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  238 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  238 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  238 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  238 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  238 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  238 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  238 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  238 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  238 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  238 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  238 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  238 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  238 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  238 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  238 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  238 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  238 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  238 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  238 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  238 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  238 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  238 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  238 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  238 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  238 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  238 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  238 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  238 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  238 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  238 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  238 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  238 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  238 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  238 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  238 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  238 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  238 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  238 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  238 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  238 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  238 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  238 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  238 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  238 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  238 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  238 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  239 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  239 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  239 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  239 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  239 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  239 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  239 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  239 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  239 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  239 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  239 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  239 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  239 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  239 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  239 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  239 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  239 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  239 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  239 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  239 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  239 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  239 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  239 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  239 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  239 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  239 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  239 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  239 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  239 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  239 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  239 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  239 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  239 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  239 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  239 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  239 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  239 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  239 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  239 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  239 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  239 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  239 , M_lda =  48  --->  Accuracy = 92.31%\n",
      "M_pca =  239 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  239 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  239 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  240 , M_lda =  1  --->  Accuracy = 13.46%\n",
      "M_pca =  240 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  240 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  240 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  240 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  240 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  240 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  240 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  240 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  240 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  240 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  240 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  240 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  240 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  240 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  240 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  240 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  240 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  240 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  240 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  240 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  240 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  240 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  240 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  240 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  240 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  240 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  240 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  240 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  240 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  240 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  240 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  240 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  240 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  240 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  240 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  240 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  240 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  240 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  240 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  240 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  240 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  240 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  240 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  240 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  241 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  241 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  241 , M_lda =  4  --->  Accuracy = 28.85%\n",
      "M_pca =  241 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  241 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  241 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  241 , M_lda =  8  --->  Accuracy = 64.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  241 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  241 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  241 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  241 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  241 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  241 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  241 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  241 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  241 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  241 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  241 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  241 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  241 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  241 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  241 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  241 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  241 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  241 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  241 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  241 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  241 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  241 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  241 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  241 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  241 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  241 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  241 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  241 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  241 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  241 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  241 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  241 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  241 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  241 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  241 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  241 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  241 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  241 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  242 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  242 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  242 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  242 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  242 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  242 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  242 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  242 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  242 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  242 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  242 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  242 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  242 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  242 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  242 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  242 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  242 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  242 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  242 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  242 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  242 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  242 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  242 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  242 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  242 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  242 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  242 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  242 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  242 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  242 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  242 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  242 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  242 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  242 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  242 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  242 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  242 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  242 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  242 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  242 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  242 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  242 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  242 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  242 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  243 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  243 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  243 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  243 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  243 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  243 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  243 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  243 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  243 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  243 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  243 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  243 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  243 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  243 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  243 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  243 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  243 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  243 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  243 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  243 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  243 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  243 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  243 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  243 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  243 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  243 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  243 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  243 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  243 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  243 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  243 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  243 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  243 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  243 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  243 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  243 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  243 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  244 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  244 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  244 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  244 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  244 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  244 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  244 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  244 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  244 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  244 , M_lda =  10  --->  Accuracy = 66.35%\n",
      "M_pca =  244 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  244 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  244 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  244 , M_lda =  14  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  244 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  244 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  244 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  244 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  244 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  244 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  244 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  244 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  244 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  244 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  244 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  244 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  244 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  244 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  244 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  244 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  244 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  244 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  244 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  244 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  244 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  244 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  244 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  245 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  245 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  245 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  245 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  245 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  245 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  245 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  245 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  245 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  245 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  245 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  245 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  245 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  245 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  245 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  245 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  245 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  245 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  245 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  245 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  245 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  245 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  245 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  245 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  245 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  245 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  245 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  245 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  245 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  245 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  245 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  245 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  245 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  245 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  245 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  245 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  245 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  245 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  245 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  245 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  245 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  245 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  245 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  246 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  246 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  246 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  246 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  246 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  246 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  246 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  246 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  246 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  246 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  246 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  246 , M_lda =  12  --->  Accuracy = 66.35%\n",
      "M_pca =  246 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  246 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  246 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  246 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  246 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  246 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  246 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  246 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  246 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  246 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  246 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  246 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  246 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  246 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  246 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  246 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  246 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  246 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  246 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  246 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  246 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  246 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  246 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  246 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  246 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  246 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  246 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  246 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  246 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  246 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  246 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  246 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  246 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  246 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  246 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  246 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  246 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  246 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  246 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  247 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  247 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  247 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  247 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  247 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  247 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  247 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  247 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  247 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  247 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  247 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  247 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  247 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  247 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  247 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  247 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  247 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  247 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  247 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  247 , M_lda =  20  --->  Accuracy = 78.85%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  247 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  247 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  247 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  247 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  247 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  247 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  247 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  247 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  247 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  247 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  247 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  247 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  247 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  247 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  247 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  247 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  247 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  247 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  247 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  247 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  247 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  247 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  247 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  247 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  247 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  247 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  247 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  247 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  247 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  247 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  247 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  248 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  248 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  248 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  248 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  248 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  248 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  248 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  248 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  248 , M_lda =  9  --->  Accuracy = 65.38%\n",
      "M_pca =  248 , M_lda =  10  --->  Accuracy = 66.35%\n",
      "M_pca =  248 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  248 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  248 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  248 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  248 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  248 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  248 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  248 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  248 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  248 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  248 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  248 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  248 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  248 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  248 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  248 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  248 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  248 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  248 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  248 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  248 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  248 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  248 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  248 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  248 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  248 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  248 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  248 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  248 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  248 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  248 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  248 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  248 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  248 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  249 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  249 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  249 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  249 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  249 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  249 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  249 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  249 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  249 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  249 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  249 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  249 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  249 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  249 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  249 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  249 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  249 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  249 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  249 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  249 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  249 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  249 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  249 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  249 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  249 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  249 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  249 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  249 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  249 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  249 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  249 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  249 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  249 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  249 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  249 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  249 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  250 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  250 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  250 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  250 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  250 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  250 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  250 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  250 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  250 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  250 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  250 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  250 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  250 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  250 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  250 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  250 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  250 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  250 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  250 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  250 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  250 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  250 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  250 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  250 , M_lda =  26  --->  Accuracy = 79.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  250 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  250 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  250 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  250 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  250 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  250 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  250 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  250 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  250 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  250 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  250 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  250 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  250 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  250 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  250 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  250 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  250 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  250 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  250 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  250 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  250 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  251 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  251 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  251 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  251 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  251 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  251 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  251 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  251 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  251 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  251 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  251 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  251 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  251 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  251 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  251 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  251 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  251 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  251 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  251 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  251 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  251 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  251 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  251 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  251 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  251 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  251 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  251 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  251 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  251 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  251 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  251 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  251 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  251 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  251 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  251 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  251 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  251 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  251 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  251 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  251 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  251 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  251 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  251 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  251 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  251 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  252 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  252 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  252 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  252 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  252 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  252 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  252 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  252 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  252 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  252 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  252 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  252 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  252 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  252 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  252 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  252 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  252 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  252 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  252 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  252 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  252 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  252 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  252 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  252 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  252 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  252 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  252 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  252 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  252 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  252 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  252 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  252 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  252 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  252 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  252 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  252 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  252 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  253 , M_lda =  1  --->  Accuracy = 12.50%\n",
      "M_pca =  253 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  253 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  253 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  253 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  253 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  253 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  253 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  253 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  253 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  253 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  253 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  253 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  253 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  253 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  253 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  253 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  253 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  253 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  253 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  253 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  253 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  253 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  253 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  253 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  253 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  253 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  253 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  253 , M_lda =  32  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  253 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  253 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  253 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  253 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  253 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  253 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  253 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  253 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  253 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  253 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  253 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  253 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  253 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  253 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  253 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  253 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  254 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  254 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  254 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  254 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  254 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  254 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  254 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  254 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  254 , M_lda =  9  --->  Accuracy = 65.38%\n",
      "M_pca =  254 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  254 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  254 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  254 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  254 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  254 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  254 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  254 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  254 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  254 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  254 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  254 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  254 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  254 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  254 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  254 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  254 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  254 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  254 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  254 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  254 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  254 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  254 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  254 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  254 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  254 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  254 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  254 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  254 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  254 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  254 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  254 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  254 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  254 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  254 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  255 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  255 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  255 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  255 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  255 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  255 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  255 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  255 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  255 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  255 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  255 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  255 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  255 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  255 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  255 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  255 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  255 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  255 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  255 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  255 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  255 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  255 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  255 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  255 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  255 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  255 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  255 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  255 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  255 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  255 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  255 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  255 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  255 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  255 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  255 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  255 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  255 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  255 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  255 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  255 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  255 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  255 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  256 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  256 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  256 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  256 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  256 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  256 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  256 , M_lda =  7  --->  Accuracy = 56.73%\n",
      "M_pca =  256 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  256 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  256 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  256 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  256 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  256 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  256 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  256 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  256 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  256 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  256 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  256 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  256 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  256 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  256 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  256 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  256 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  256 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  256 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  256 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  256 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  256 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  256 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  256 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  256 , M_lda =  38  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  256 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  256 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  256 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  256 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  256 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  256 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  256 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  256 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  256 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  256 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  256 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  256 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  256 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  257 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  257 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  257 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  257 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  257 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  257 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  257 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  257 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  257 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  257 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  257 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  257 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  257 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  257 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  257 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  257 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  257 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  257 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  257 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  257 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  257 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  257 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  257 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  257 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  257 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  257 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  257 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  257 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  257 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  257 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  257 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  257 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  257 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  257 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  257 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  257 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  257 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  257 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  257 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  257 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  257 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  257 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  257 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  258 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  258 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  258 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  258 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  258 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  258 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  258 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  258 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  258 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  258 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  258 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  258 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  258 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  258 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  258 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  258 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  258 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  258 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  258 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  258 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  258 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  258 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  258 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  258 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  258 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  258 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  258 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  258 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  258 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  258 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  258 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  258 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  258 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  258 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  258 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  258 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  258 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  258 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  258 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  258 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  258 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  258 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  259 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  259 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  259 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  259 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  259 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  259 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  259 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  259 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  259 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  259 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  259 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  259 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  259 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  259 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  259 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  259 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  259 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  259 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  259 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  259 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  259 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  259 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  259 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  259 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  259 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  259 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  259 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  259 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  259 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  259 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  259 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  259 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  259 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  259 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  44  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  259 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  259 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  259 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  259 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  259 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  260 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  260 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  260 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  260 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  260 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  260 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  260 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  260 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  260 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  260 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  260 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  260 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  260 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  260 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  260 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  260 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  260 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  260 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  260 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  260 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  260 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  260 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  260 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  260 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  25  --->  Accuracy = 75.96%\n",
      "M_pca =  260 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  260 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  260 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  260 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  260 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  260 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  260 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  260 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  260 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  260 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  260 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  260 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  260 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  260 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  260 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  260 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  260 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  260 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  260 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  260 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  260 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  261 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  261 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  261 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  261 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  261 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  261 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  261 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  261 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  261 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  261 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  261 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  261 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  261 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  261 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  261 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  261 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  261 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  261 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  261 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  261 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  261 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  261 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  261 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  261 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  261 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  261 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  261 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  261 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  261 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  261 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  261 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  261 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  262 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  262 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  262 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  262 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  262 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  262 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  262 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  262 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  262 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  262 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  262 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  262 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  262 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  262 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  262 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  262 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  262 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  262 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  262 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  262 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  262 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  262 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  262 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  262 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  262 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  262 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  262 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  262 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  262 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  262 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  262 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  262 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  262 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  262 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  262 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  262 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  262 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  262 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  262 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  262 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  262 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  262 , M_lda =  50  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  262 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  263 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  263 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  263 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  263 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  263 , M_lda =  5  --->  Accuracy = 35.58%\n",
      "M_pca =  263 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  263 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  263 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  263 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  263 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  263 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  263 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  263 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  263 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  263 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  263 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  263 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  263 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  263 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  263 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  263 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  263 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  263 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  263 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  263 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  263 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  263 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  263 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  263 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  263 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  263 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  263 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  263 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  263 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  263 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  263 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  263 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  263 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  263 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  263 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  263 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  263 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  263 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  263 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  264 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  264 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  264 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  264 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  264 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  264 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  264 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  264 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  264 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  264 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  264 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  264 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  264 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  264 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  264 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  264 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  264 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  264 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  264 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  264 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  264 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  264 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  264 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  264 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  264 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  264 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  264 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  264 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  264 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  264 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  264 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  264 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  264 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  264 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  264 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  264 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  264 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  264 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  264 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  264 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  264 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  264 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  264 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  264 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  264 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  265 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  265 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  265 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  265 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  265 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  265 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  265 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  265 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  265 , M_lda =  9  --->  Accuracy = 61.54%\n",
      "M_pca =  265 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  265 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  265 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  265 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  265 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  265 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  265 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  265 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  265 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  265 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  265 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  265 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  265 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  265 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  265 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  265 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  265 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  265 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  265 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  265 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  265 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  265 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  265 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  265 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  265 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  266 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  266 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  266 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  266 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  266 , M_lda =  5  --->  Accuracy = 45.19%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  266 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  266 , M_lda =  7  --->  Accuracy = 51.92%\n",
      "M_pca =  266 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  266 , M_lda =  9  --->  Accuracy = 60.58%\n",
      "M_pca =  266 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  266 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  266 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  266 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  266 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  266 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  266 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  266 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  266 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  266 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  266 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  266 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  266 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  266 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  266 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  266 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  266 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  266 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  266 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  266 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  266 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  266 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  266 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  266 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  266 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  266 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  266 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  266 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  266 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  266 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  266 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  266 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  266 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  266 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  266 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  266 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  266 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  266 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  266 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  266 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  266 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  266 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  267 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  267 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  267 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  267 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  267 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  267 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  267 , M_lda =  7  --->  Accuracy = 55.77%\n",
      "M_pca =  267 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  267 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  267 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  267 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  267 , M_lda =  12  --->  Accuracy = 66.35%\n",
      "M_pca =  267 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  267 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  267 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  267 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  267 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  267 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  267 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  267 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  267 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  267 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  267 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  267 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  267 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  267 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  267 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  267 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  267 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  267 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  267 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  267 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  267 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  267 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  267 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  267 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  267 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  267 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  267 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  267 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  267 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  267 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  267 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  267 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  267 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  268 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  268 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  268 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  268 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  268 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  268 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  268 , M_lda =  7  --->  Accuracy = 50.96%\n",
      "M_pca =  268 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  268 , M_lda =  9  --->  Accuracy = 65.38%\n",
      "M_pca =  268 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  268 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  268 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  268 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  268 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  268 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  268 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  268 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  268 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  268 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  268 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  268 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  268 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  268 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  268 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  268 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  268 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  268 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  268 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  268 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  268 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  268 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  268 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  268 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  268 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  268 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  268 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  268 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  268 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  268 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  268 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  268 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  268 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  268 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  268 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  268 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  268 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  268 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  268 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  268 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  268 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  268 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  269 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  269 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  269 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  269 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  269 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  269 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  269 , M_lda =  7  --->  Accuracy = 48.08%\n",
      "M_pca =  269 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  269 , M_lda =  9  --->  Accuracy = 60.58%\n",
      "M_pca =  269 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  269 , M_lda =  11  --->  Accuracy = 67.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  269 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  269 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  269 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  269 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  269 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  269 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  269 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  269 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  269 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  269 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  269 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  269 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  269 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  269 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  269 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  269 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  269 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  269 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  269 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  269 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  269 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  269 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  269 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  269 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  269 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  269 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  269 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  269 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  269 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  269 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  269 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  269 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  269 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  270 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  270 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  270 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  270 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  270 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  270 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  270 , M_lda =  7  --->  Accuracy = 51.92%\n",
      "M_pca =  270 , M_lda =  8  --->  Accuracy = 52.88%\n",
      "M_pca =  270 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  270 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  270 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  270 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  270 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  270 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  270 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  270 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  270 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  270 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  270 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  270 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  270 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  270 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  270 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  270 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  270 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  270 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  270 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  270 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  270 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  270 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  270 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  270 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  270 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  270 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  270 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  270 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  270 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  270 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  270 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  270 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  270 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  271 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  271 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  271 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  271 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  271 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  271 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  271 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  271 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  271 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  271 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  271 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  271 , M_lda =  12  --->  Accuracy = 64.42%\n",
      "M_pca =  271 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  271 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  271 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  271 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  271 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  271 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  271 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  271 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  271 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  271 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  271 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  271 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  271 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  271 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  271 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  271 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  271 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  271 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  271 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  271 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  271 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  271 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  271 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  271 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  271 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  271 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  271 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  271 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  271 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  271 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  271 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  272 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  272 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  272 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  272 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  272 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  272 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  272 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  272 , M_lda =  8  --->  Accuracy = 57.69%\n",
      "M_pca =  272 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  272 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  272 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  272 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  272 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  272 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  272 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  272 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  272 , M_lda =  17  --->  Accuracy = 75.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  272 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  272 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  272 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  272 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  272 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  272 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  272 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  272 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  272 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  272 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  272 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  272 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  272 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  272 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  272 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  272 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  272 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  272 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  272 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  272 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  272 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  272 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  272 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  272 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  272 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  272 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  272 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  272 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  273 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  273 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  273 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  273 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  273 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  273 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  273 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  273 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  273 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  273 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  273 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  273 , M_lda =  12  --->  Accuracy = 65.38%\n",
      "M_pca =  273 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  273 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  273 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  273 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  273 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  273 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  273 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  273 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  273 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  273 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  273 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  273 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  273 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  273 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  273 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  273 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  273 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  273 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  273 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  273 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  273 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  273 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  273 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  273 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  273 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  273 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  273 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  273 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  273 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  273 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  273 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  273 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  273 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  274 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  274 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  274 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  274 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  274 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  274 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  274 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  274 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  274 , M_lda =  10  --->  Accuracy = 66.35%\n",
      "M_pca =  274 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  274 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  274 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  274 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  274 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  274 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  274 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  274 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  274 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  274 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  274 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  274 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  274 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  274 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  274 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  274 , M_lda =  26  --->  Accuracy = 76.92%\n",
      "M_pca =  274 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  274 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  274 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  274 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  274 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  274 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  274 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  274 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  274 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  274 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  274 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  274 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  274 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  274 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  274 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  274 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  274 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  274 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  274 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  274 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  275 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  275 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  275 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  275 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  275 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  275 , M_lda =  6  --->  Accuracy = 46.15%\n",
      "M_pca =  275 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  275 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  275 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  275 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  275 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  275 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  275 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  275 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  275 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  275 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  275 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  275 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  275 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  275 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  275 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  275 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  275 , M_lda =  23  --->  Accuracy = 79.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  275 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  275 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  275 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  275 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  275 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  275 , M_lda =  29  --->  Accuracy = 78.85%\n",
      "M_pca =  275 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  275 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  275 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  33  --->  Accuracy = 78.85%\n",
      "M_pca =  275 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  275 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  275 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  275 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  275 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  275 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  275 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  275 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  275 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  275 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  275 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  276 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  276 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  276 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  276 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  276 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  276 , M_lda =  7  --->  Accuracy = 56.73%\n",
      "M_pca =  276 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  276 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  276 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  276 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  276 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  276 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  276 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  276 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  276 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  276 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  276 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  276 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  276 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  276 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  276 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  276 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  276 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  276 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  276 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  276 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  276 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  276 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  276 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  276 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  276 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  276 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  276 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  276 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  276 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  276 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  277 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  277 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  277 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  277 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  277 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  277 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  277 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  277 , M_lda =  8  --->  Accuracy = 57.69%\n",
      "M_pca =  277 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  277 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  277 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  277 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  277 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  277 , M_lda =  14  --->  Accuracy = 72.12%\n",
      "M_pca =  277 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  277 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  277 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  277 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  277 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  277 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  277 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  277 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  277 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  277 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  277 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  277 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  277 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  277 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  277 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  277 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  277 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  277 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  277 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  277 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  277 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  277 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  277 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  277 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  277 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  277 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  277 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  277 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  277 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  277 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  277 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  278 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  278 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  278 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  278 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  278 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  278 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  278 , M_lda =  7  --->  Accuracy = 55.77%\n",
      "M_pca =  278 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  278 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  278 , M_lda =  10  --->  Accuracy = 66.35%\n",
      "M_pca =  278 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  278 , M_lda =  12  --->  Accuracy = 64.42%\n",
      "M_pca =  278 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  278 , M_lda =  14  --->  Accuracy = 67.31%\n",
      "M_pca =  278 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  278 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  278 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  278 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  278 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  278 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  278 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  278 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  278 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  278 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  278 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  278 , M_lda =  29  --->  Accuracy = 80.77%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  278 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  278 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  278 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  278 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  278 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  278 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  278 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  278 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  278 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  278 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  278 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  278 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  278 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  278 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  278 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  278 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  278 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  278 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  279 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  279 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  279 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  279 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  279 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  279 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  279 , M_lda =  8  --->  Accuracy = 55.77%\n",
      "M_pca =  279 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  279 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  279 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  279 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  279 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  279 , M_lda =  14  --->  Accuracy = 72.12%\n",
      "M_pca =  279 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  279 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  279 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  279 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  279 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  279 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  279 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  279 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  279 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  279 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  279 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  279 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  279 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  279 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  279 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  279 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  279 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  279 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  279 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  279 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  280 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  280 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  280 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  280 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  280 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  280 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  280 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  280 , M_lda =  8  --->  Accuracy = 52.88%\n",
      "M_pca =  280 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  280 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  280 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  280 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  280 , M_lda =  13  --->  Accuracy = 68.27%\n",
      "M_pca =  280 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  280 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  280 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  280 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  280 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  280 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  280 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  280 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  280 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  280 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  280 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  280 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  26  --->  Accuracy = 76.92%\n",
      "M_pca =  280 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  280 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  280 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  280 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  280 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  280 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  280 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  280 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  280 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  280 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  280 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  280 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  280 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  280 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  281 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  281 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  281 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  281 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  281 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  281 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  281 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  281 , M_lda =  8  --->  Accuracy = 50.00%\n",
      "M_pca =  281 , M_lda =  9  --->  Accuracy = 55.77%\n",
      "M_pca =  281 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  281 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  281 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  281 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  281 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  281 , M_lda =  15  --->  Accuracy = 70.19%\n",
      "M_pca =  281 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  281 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  281 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  281 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  281 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  281 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  281 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  281 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  281 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  281 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  281 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  281 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  281 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  281 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  281 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  35  --->  Accuracy = 82.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  281 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  281 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  281 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  281 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  281 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  281 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  281 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  281 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  281 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  282 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  282 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  282 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  282 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  282 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  282 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  282 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  282 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  282 , M_lda =  9  --->  Accuracy = 61.54%\n",
      "M_pca =  282 , M_lda =  10  --->  Accuracy = 66.35%\n",
      "M_pca =  282 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  282 , M_lda =  12  --->  Accuracy = 66.35%\n",
      "M_pca =  282 , M_lda =  13  --->  Accuracy = 66.35%\n",
      "M_pca =  282 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  282 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  282 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  282 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  282 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  282 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  282 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  282 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  282 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  282 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  282 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  282 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  282 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  282 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  282 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  282 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  282 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  282 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  282 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  282 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  282 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  282 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  282 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  283 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  283 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  283 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  283 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  283 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  283 , M_lda =  6  --->  Accuracy = 42.31%\n",
      "M_pca =  283 , M_lda =  7  --->  Accuracy = 55.77%\n",
      "M_pca =  283 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  283 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  283 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  283 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  283 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  283 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  283 , M_lda =  14  --->  Accuracy = 72.12%\n",
      "M_pca =  283 , M_lda =  15  --->  Accuracy = 70.19%\n",
      "M_pca =  283 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  283 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  283 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  283 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  283 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  283 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  283 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  283 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  283 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  283 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  283 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  283 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  283 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  283 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  283 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  283 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  283 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  283 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  283 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  283 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  283 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  283 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  284 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  284 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  284 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  284 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  284 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  284 , M_lda =  6  --->  Accuracy = 43.27%\n",
      "M_pca =  284 , M_lda =  7  --->  Accuracy = 55.77%\n",
      "M_pca =  284 , M_lda =  8  --->  Accuracy = 51.92%\n",
      "M_pca =  284 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  284 , M_lda =  10  --->  Accuracy = 58.65%\n",
      "M_pca =  284 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  284 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  284 , M_lda =  13  --->  Accuracy = 66.35%\n",
      "M_pca =  284 , M_lda =  14  --->  Accuracy = 70.19%\n",
      "M_pca =  284 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  284 , M_lda =  16  --->  Accuracy = 70.19%\n",
      "M_pca =  284 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  284 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  284 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  284 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  284 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  284 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  284 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  284 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  284 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  284 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  284 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  284 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  284 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  284 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  284 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  284 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  284 , M_lda =  41  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  284 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  284 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  284 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  284 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  284 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  284 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  284 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  284 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  284 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  284 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  285 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  285 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  285 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  285 , M_lda =  4  --->  Accuracy = 25.00%\n",
      "M_pca =  285 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  285 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  285 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  285 , M_lda =  8  --->  Accuracy = 51.92%\n",
      "M_pca =  285 , M_lda =  9  --->  Accuracy = 53.85%\n",
      "M_pca =  285 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  285 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  285 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  285 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  285 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  285 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  285 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  285 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  285 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  285 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  285 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  285 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  285 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  285 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  285 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  285 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  285 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  285 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  285 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  285 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  285 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  285 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  285 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  285 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  285 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  285 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  285 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  286 , M_lda =  1  --->  Accuracy = 13.46%\n",
      "M_pca =  286 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  286 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  286 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  286 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  286 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  286 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  286 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  286 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  286 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  286 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  286 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  286 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  286 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  286 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  286 , M_lda =  16  --->  Accuracy = 69.23%\n",
      "M_pca =  286 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  286 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  286 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  286 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  286 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  286 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  286 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  286 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  286 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  286 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  286 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  286 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  286 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  286 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  286 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  286 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  286 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  286 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  286 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  286 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  286 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  286 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  286 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  286 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  286 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  47  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  286 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  286 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  286 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  287 , M_lda =  1  --->  Accuracy = 1.92%\n",
      "M_pca =  287 , M_lda =  2  --->  Accuracy = 8.65%\n",
      "M_pca =  287 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  287 , M_lda =  4  --->  Accuracy = 30.77%\n",
      "M_pca =  287 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  287 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  287 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  287 , M_lda =  8  --->  Accuracy = 57.69%\n",
      "M_pca =  287 , M_lda =  9  --->  Accuracy = 60.58%\n",
      "M_pca =  287 , M_lda =  10  --->  Accuracy = 59.62%\n",
      "M_pca =  287 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  287 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  287 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  287 , M_lda =  14  --->  Accuracy = 67.31%\n",
      "M_pca =  287 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  287 , M_lda =  16  --->  Accuracy = 70.19%\n",
      "M_pca =  287 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  287 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  287 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  287 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  287 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  287 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  287 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  287 , M_lda =  24  --->  Accuracy = 75.96%\n",
      "M_pca =  287 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  287 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  287 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  287 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  287 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  287 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  287 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  287 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  287 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  287 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  287 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  287 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  287 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  287 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  287 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  287 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  47  --->  Accuracy = 85.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  287 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  287 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  287 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  287 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  288 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  288 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  288 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  288 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  288 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  288 , M_lda =  6  --->  Accuracy = 46.15%\n",
      "M_pca =  288 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  288 , M_lda =  8  --->  Accuracy = 55.77%\n",
      "M_pca =  288 , M_lda =  9  --->  Accuracy = 53.85%\n",
      "M_pca =  288 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  288 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  288 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  288 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  288 , M_lda =  14  --->  Accuracy = 69.23%\n",
      "M_pca =  288 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  288 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  288 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  288 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  288 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  288 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  288 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  288 , M_lda =  22  --->  Accuracy = 75.00%\n",
      "M_pca =  288 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  288 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  288 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  288 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  288 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  288 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  288 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  288 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  288 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  288 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  289 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  289 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  289 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  289 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  289 , M_lda =  6  --->  Accuracy = 41.35%\n",
      "M_pca =  289 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  289 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  289 , M_lda =  9  --->  Accuracy = 58.65%\n",
      "M_pca =  289 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  289 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  289 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  289 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  289 , M_lda =  14  --->  Accuracy = 69.23%\n",
      "M_pca =  289 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  289 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  289 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  289 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  289 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  289 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  289 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  289 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  289 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  289 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  289 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  27  --->  Accuracy = 76.92%\n",
      "M_pca =  289 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  29  --->  Accuracy = 76.92%\n",
      "M_pca =  289 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  289 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  289 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  289 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  289 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  289 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  289 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  289 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  289 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  289 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  289 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  289 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  289 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  289 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  290 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  290 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  290 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  290 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  290 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  290 , M_lda =  6  --->  Accuracy = 46.15%\n",
      "M_pca =  290 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  290 , M_lda =  8  --->  Accuracy = 50.96%\n",
      "M_pca =  290 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  290 , M_lda =  10  --->  Accuracy = 62.50%\n",
      "M_pca =  290 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  290 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  290 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  290 , M_lda =  14  --->  Accuracy = 64.42%\n",
      "M_pca =  290 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  290 , M_lda =  16  --->  Accuracy = 69.23%\n",
      "M_pca =  290 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  290 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  290 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  290 , M_lda =  20  --->  Accuracy = 70.19%\n",
      "M_pca =  290 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  290 , M_lda =  22  --->  Accuracy = 74.04%\n",
      "M_pca =  290 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  290 , M_lda =  24  --->  Accuracy = 76.92%\n",
      "M_pca =  290 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  290 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  290 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  290 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  290 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  290 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  290 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  290 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  290 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  290 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  290 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  290 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  290 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  290 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  290 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  290 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  290 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  290 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  290 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  290 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  290 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  291 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  291 , M_lda =  2  --->  Accuracy = 13.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  291 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  291 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  291 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  291 , M_lda =  6  --->  Accuracy = 37.50%\n",
      "M_pca =  291 , M_lda =  7  --->  Accuracy = 55.77%\n",
      "M_pca =  291 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  291 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  291 , M_lda =  10  --->  Accuracy = 58.65%\n",
      "M_pca =  291 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  291 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  291 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  291 , M_lda =  14  --->  Accuracy = 70.19%\n",
      "M_pca =  291 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  291 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  291 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  291 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  291 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  291 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  291 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  291 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  291 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  291 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  291 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  291 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  291 , M_lda =  27  --->  Accuracy = 76.92%\n",
      "M_pca =  291 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  291 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  291 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  291 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  291 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  291 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  291 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  291 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  291 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  291 , M_lda =  37  --->  Accuracy = 76.92%\n",
      "M_pca =  291 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  291 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  291 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  291 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  291 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  291 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  291 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  291 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  291 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  291 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  291 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  291 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  291 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  291 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  292 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  292 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  292 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  292 , M_lda =  4  --->  Accuracy = 30.77%\n",
      "M_pca =  292 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  292 , M_lda =  6  --->  Accuracy = 41.35%\n",
      "M_pca =  292 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  292 , M_lda =  8  --->  Accuracy = 53.85%\n",
      "M_pca =  292 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  292 , M_lda =  10  --->  Accuracy = 56.73%\n",
      "M_pca =  292 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  292 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  292 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  292 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  292 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  292 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  292 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  292 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  292 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  292 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  292 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  292 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  292 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  292 , M_lda =  24  --->  Accuracy = 76.92%\n",
      "M_pca =  292 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  292 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  292 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  292 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  292 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  36  --->  Accuracy = 78.85%\n",
      "M_pca =  292 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  292 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  292 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  292 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  292 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  292 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  293 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  293 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  293 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  293 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  293 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  293 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  293 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  293 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  293 , M_lda =  9  --->  Accuracy = 52.88%\n",
      "M_pca =  293 , M_lda =  10  --->  Accuracy = 56.73%\n",
      "M_pca =  293 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  293 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  293 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  293 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  293 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  293 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  293 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  293 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  293 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  293 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  293 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  293 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  293 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  293 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  293 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  293 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  293 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  293 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  293 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  293 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  35  --->  Accuracy = 77.88%\n",
      "M_pca =  293 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  293 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  293 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  293 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  293 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  293 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  293 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  293 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  293 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  293 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  293 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  293 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  293 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  293 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  294 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  294 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  294 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  294 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  294 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  294 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  294 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  294 , M_lda =  8  --->  Accuracy = 53.85%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  294 , M_lda =  9  --->  Accuracy = 55.77%\n",
      "M_pca =  294 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  294 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  294 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  294 , M_lda =  13  --->  Accuracy = 68.27%\n",
      "M_pca =  294 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  294 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  294 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  294 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  294 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  294 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  294 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  294 , M_lda =  21  --->  Accuracy = 75.00%\n",
      "M_pca =  294 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  294 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  294 , M_lda =  24  --->  Accuracy = 75.96%\n",
      "M_pca =  294 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  294 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  294 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  294 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  294 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  294 , M_lda =  30  --->  Accuracy = 75.96%\n",
      "M_pca =  294 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  294 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  294 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  294 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  294 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  294 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  294 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  294 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  294 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  294 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  294 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  294 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  294 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  294 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  294 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  295 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  295 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  295 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  295 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  295 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  295 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  295 , M_lda =  7  --->  Accuracy = 47.12%\n",
      "M_pca =  295 , M_lda =  8  --->  Accuracy = 51.92%\n",
      "M_pca =  295 , M_lda =  9  --->  Accuracy = 54.81%\n",
      "M_pca =  295 , M_lda =  10  --->  Accuracy = 58.65%\n",
      "M_pca =  295 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  295 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  295 , M_lda =  13  --->  Accuracy = 68.27%\n",
      "M_pca =  295 , M_lda =  14  --->  Accuracy = 65.38%\n",
      "M_pca =  295 , M_lda =  15  --->  Accuracy = 69.23%\n",
      "M_pca =  295 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  295 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  295 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  295 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  295 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  295 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  295 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  295 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  295 , M_lda =  24  --->  Accuracy = 74.04%\n",
      "M_pca =  295 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  295 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  295 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  295 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  295 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  295 , M_lda =  31  --->  Accuracy = 77.88%\n",
      "M_pca =  295 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  295 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  295 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  295 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  295 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  295 , M_lda =  42  --->  Accuracy = 76.92%\n",
      "M_pca =  295 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  295 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  295 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  295 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  295 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  295 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  295 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  296 , M_lda =  1  --->  Accuracy = 1.92%\n",
      "M_pca =  296 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  296 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  296 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  296 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  296 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  296 , M_lda =  7  --->  Accuracy = 45.19%\n",
      "M_pca =  296 , M_lda =  8  --->  Accuracy = 43.27%\n",
      "M_pca =  296 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  296 , M_lda =  10  --->  Accuracy = 64.42%\n",
      "M_pca =  296 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  296 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  296 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  296 , M_lda =  14  --->  Accuracy = 69.23%\n",
      "M_pca =  296 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  296 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  296 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  296 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  296 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  296 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  296 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  296 , M_lda =  22  --->  Accuracy = 74.04%\n",
      "M_pca =  296 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  296 , M_lda =  24  --->  Accuracy = 76.92%\n",
      "M_pca =  296 , M_lda =  25  --->  Accuracy = 76.92%\n",
      "M_pca =  296 , M_lda =  26  --->  Accuracy = 76.92%\n",
      "M_pca =  296 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  296 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  296 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  296 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  296 , M_lda =  31  --->  Accuracy = 78.85%\n",
      "M_pca =  296 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  296 , M_lda =  33  --->  Accuracy = 78.85%\n",
      "M_pca =  296 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  296 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  296 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  296 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  296 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  296 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  296 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  296 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  296 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  296 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  296 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  296 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  296 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  296 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  296 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  296 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  296 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  296 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  297 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  297 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  297 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  297 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  297 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  297 , M_lda =  6  --->  Accuracy = 36.54%\n",
      "M_pca =  297 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  297 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  297 , M_lda =  9  --->  Accuracy = 61.54%\n",
      "M_pca =  297 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  297 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  297 , M_lda =  12  --->  Accuracy = 63.46%\n",
      "M_pca =  297 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  297 , M_lda =  14  --->  Accuracy = 68.27%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  297 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  297 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  297 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  297 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  297 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  297 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  297 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  297 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  297 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  297 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  297 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  297 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  297 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  297 , M_lda =  31  --->  Accuracy = 76.92%\n",
      "M_pca =  297 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  297 , M_lda =  33  --->  Accuracy = 77.88%\n",
      "M_pca =  297 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  297 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  297 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  297 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  297 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  297 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  297 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  297 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  297 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  297 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  297 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  297 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  297 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  297 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  297 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  298 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  298 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  298 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  298 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  298 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  298 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  298 , M_lda =  7  --->  Accuracy = 45.19%\n",
      "M_pca =  298 , M_lda =  8  --->  Accuracy = 52.88%\n",
      "M_pca =  298 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  298 , M_lda =  10  --->  Accuracy = 56.73%\n",
      "M_pca =  298 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  298 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  298 , M_lda =  13  --->  Accuracy = 64.42%\n",
      "M_pca =  298 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  298 , M_lda =  15  --->  Accuracy = 67.31%\n",
      "M_pca =  298 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  298 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  298 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  298 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  298 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  298 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  298 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  298 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  298 , M_lda =  24  --->  Accuracy = 75.96%\n",
      "M_pca =  298 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  26  --->  Accuracy = 74.04%\n",
      "M_pca =  298 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  298 , M_lda =  29  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  298 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  38  --->  Accuracy = 76.92%\n",
      "M_pca =  298 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  41  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  298 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  298 , M_lda =  44  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  298 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  298 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  298 , M_lda =  48  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  298 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  298 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  299 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  299 , M_lda =  2  --->  Accuracy = 10.58%\n",
      "M_pca =  299 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  299 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  299 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  299 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  299 , M_lda =  7  --->  Accuracy = 46.15%\n",
      "M_pca =  299 , M_lda =  8  --->  Accuracy = 50.00%\n",
      "M_pca =  299 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  299 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  299 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  299 , M_lda =  12  --->  Accuracy = 63.46%\n",
      "M_pca =  299 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  299 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  299 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  299 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  299 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  299 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  299 , M_lda =  19  --->  Accuracy = 69.23%\n",
      "M_pca =  299 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  299 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  299 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  299 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  299 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  299 , M_lda =  25  --->  Accuracy = 75.00%\n",
      "M_pca =  299 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  27  --->  Accuracy = 75.96%\n",
      "M_pca =  299 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  299 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  299 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  299 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  299 , M_lda =  37  --->  Accuracy = 76.92%\n",
      "M_pca =  299 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  299 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  299 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  299 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  299 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  299 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  299 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  299 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  299 , M_lda =  47  --->  Accuracy = 82.69%\n",
      "M_pca =  299 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  299 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  299 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  299 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  300 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  300 , M_lda =  2  --->  Accuracy = 8.65%\n",
      "M_pca =  300 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  300 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  300 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  300 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  300 , M_lda =  7  --->  Accuracy = 44.23%\n",
      "M_pca =  300 , M_lda =  8  --->  Accuracy = 55.77%\n",
      "M_pca =  300 , M_lda =  9  --->  Accuracy = 54.81%\n",
      "M_pca =  300 , M_lda =  10  --->  Accuracy = 53.85%\n",
      "M_pca =  300 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  300 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  300 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  300 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  300 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  300 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  300 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  300 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  300 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  300 , M_lda =  20  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  300 , M_lda =  21  --->  Accuracy = 75.00%\n",
      "M_pca =  300 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  300 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  300 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  300 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  26  --->  Accuracy = 75.00%\n",
      "M_pca =  300 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  300 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  300 , M_lda =  29  --->  Accuracy = 76.92%\n",
      "M_pca =  300 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  300 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  33  --->  Accuracy = 76.92%\n",
      "M_pca =  300 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  300 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  300 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  300 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  300 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  300 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  300 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  300 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  300 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  300 , M_lda =  44  --->  Accuracy = 78.85%\n",
      "M_pca =  300 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  47  --->  Accuracy = 82.69%\n",
      "M_pca =  300 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  300 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  300 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  300 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  301 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  301 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  301 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  301 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  301 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  301 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  301 , M_lda =  8  --->  Accuracy = 52.88%\n",
      "M_pca =  301 , M_lda =  9  --->  Accuracy = 58.65%\n",
      "M_pca =  301 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  301 , M_lda =  11  --->  Accuracy = 61.54%\n",
      "M_pca =  301 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  301 , M_lda =  13  --->  Accuracy = 66.35%\n",
      "M_pca =  301 , M_lda =  14  --->  Accuracy = 69.23%\n",
      "M_pca =  301 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  301 , M_lda =  16  --->  Accuracy = 69.23%\n",
      "M_pca =  301 , M_lda =  17  --->  Accuracy = 69.23%\n",
      "M_pca =  301 , M_lda =  18  --->  Accuracy = 68.27%\n",
      "M_pca =  301 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  301 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  301 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  23  --->  Accuracy = 73.08%\n",
      "M_pca =  301 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  25  --->  Accuracy = 74.04%\n",
      "M_pca =  301 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  301 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  31  --->  Accuracy = 78.85%\n",
      "M_pca =  301 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  301 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  301 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  301 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  301 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  301 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  301 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  301 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  302 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  302 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  302 , M_lda =  3  --->  Accuracy = 19.23%\n",
      "M_pca =  302 , M_lda =  4  --->  Accuracy = 30.77%\n",
      "M_pca =  302 , M_lda =  5  --->  Accuracy = 36.54%\n",
      "M_pca =  302 , M_lda =  6  --->  Accuracy = 40.38%\n",
      "M_pca =  302 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  302 , M_lda =  8  --->  Accuracy = 53.85%\n",
      "M_pca =  302 , M_lda =  9  --->  Accuracy = 58.65%\n",
      "M_pca =  302 , M_lda =  10  --->  Accuracy = 56.73%\n",
      "M_pca =  302 , M_lda =  11  --->  Accuracy = 62.50%\n",
      "M_pca =  302 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  302 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  302 , M_lda =  14  --->  Accuracy = 63.46%\n",
      "M_pca =  302 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  302 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  302 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  302 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  302 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  302 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  302 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  302 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  302 , M_lda =  23  --->  Accuracy = 73.08%\n",
      "M_pca =  302 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  302 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  302 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  302 , M_lda =  29  --->  Accuracy = 78.85%\n",
      "M_pca =  302 , M_lda =  30  --->  Accuracy = 77.88%\n",
      "M_pca =  302 , M_lda =  31  --->  Accuracy = 77.88%\n",
      "M_pca =  302 , M_lda =  32  --->  Accuracy = 76.92%\n",
      "M_pca =  302 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  302 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  302 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  302 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  302 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  302 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  302 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  302 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  302 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  302 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  302 , M_lda =  47  --->  Accuracy = 82.69%\n",
      "M_pca =  302 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  302 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  302 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  303 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  303 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  303 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  303 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  303 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  303 , M_lda =  6  --->  Accuracy = 45.19%\n",
      "M_pca =  303 , M_lda =  7  --->  Accuracy = 44.23%\n",
      "M_pca =  303 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  303 , M_lda =  9  --->  Accuracy = 58.65%\n",
      "M_pca =  303 , M_lda =  10  --->  Accuracy = 55.77%\n",
      "M_pca =  303 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  303 , M_lda =  12  --->  Accuracy = 63.46%\n",
      "M_pca =  303 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  303 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  303 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  303 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  303 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  303 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  303 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  303 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  303 , M_lda =  21  --->  Accuracy = 72.12%\n",
      "M_pca =  303 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  303 , M_lda =  23  --->  Accuracy = 74.04%\n",
      "M_pca =  303 , M_lda =  24  --->  Accuracy = 75.00%\n",
      "M_pca =  303 , M_lda =  25  --->  Accuracy = 75.96%\n",
      "M_pca =  303 , M_lda =  26  --->  Accuracy = 75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  303 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  303 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  303 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  303 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  303 , M_lda =  31  --->  Accuracy = 75.00%\n",
      "M_pca =  303 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  303 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  303 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  303 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  303 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  303 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  303 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  303 , M_lda =  39  --->  Accuracy = 75.96%\n",
      "M_pca =  303 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  303 , M_lda =  41  --->  Accuracy = 77.88%\n",
      "M_pca =  303 , M_lda =  42  --->  Accuracy = 79.81%\n",
      "M_pca =  303 , M_lda =  43  --->  Accuracy = 79.81%\n",
      "M_pca =  303 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  303 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  303 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  303 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  303 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  303 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  303 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  303 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  304 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  304 , M_lda =  3  --->  Accuracy = 17.31%\n",
      "M_pca =  304 , M_lda =  4  --->  Accuracy = 27.88%\n",
      "M_pca =  304 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  304 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  304 , M_lda =  7  --->  Accuracy = 44.23%\n",
      "M_pca =  304 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  304 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  304 , M_lda =  10  --->  Accuracy = 60.58%\n",
      "M_pca =  304 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  304 , M_lda =  12  --->  Accuracy = 56.73%\n",
      "M_pca =  304 , M_lda =  13  --->  Accuracy = 66.35%\n",
      "M_pca =  304 , M_lda =  14  --->  Accuracy = 63.46%\n",
      "M_pca =  304 , M_lda =  15  --->  Accuracy = 65.38%\n",
      "M_pca =  304 , M_lda =  16  --->  Accuracy = 70.19%\n",
      "M_pca =  304 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  304 , M_lda =  18  --->  Accuracy = 69.23%\n",
      "M_pca =  304 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  304 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  304 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  304 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  304 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  304 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  304 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  304 , M_lda =  26  --->  Accuracy = 73.08%\n",
      "M_pca =  304 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  304 , M_lda =  28  --->  Accuracy = 76.92%\n",
      "M_pca =  304 , M_lda =  29  --->  Accuracy = 76.92%\n",
      "M_pca =  304 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  304 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  304 , M_lda =  32  --->  Accuracy = 77.88%\n",
      "M_pca =  304 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  34  --->  Accuracy = 78.85%\n",
      "M_pca =  304 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  304 , M_lda =  37  --->  Accuracy = 76.92%\n",
      "M_pca =  304 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  304 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  304 , M_lda =  41  --->  Accuracy = 77.88%\n",
      "M_pca =  304 , M_lda =  42  --->  Accuracy = 78.85%\n",
      "M_pca =  304 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  304 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  304 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  304 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  304 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  304 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  304 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  305 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  305 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  305 , M_lda =  3  --->  Accuracy = 19.23%\n",
      "M_pca =  305 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  305 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  305 , M_lda =  6  --->  Accuracy = 40.38%\n",
      "M_pca =  305 , M_lda =  7  --->  Accuracy = 48.08%\n",
      "M_pca =  305 , M_lda =  8  --->  Accuracy = 49.04%\n",
      "M_pca =  305 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  305 , M_lda =  10  --->  Accuracy = 54.81%\n",
      "M_pca =  305 , M_lda =  11  --->  Accuracy = 60.58%\n",
      "M_pca =  305 , M_lda =  12  --->  Accuracy = 65.38%\n",
      "M_pca =  305 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  305 , M_lda =  14  --->  Accuracy = 67.31%\n",
      "M_pca =  305 , M_lda =  15  --->  Accuracy = 69.23%\n",
      "M_pca =  305 , M_lda =  16  --->  Accuracy = 66.35%\n",
      "M_pca =  305 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  305 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  305 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  305 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  305 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  305 , M_lda =  22  --->  Accuracy = 75.00%\n",
      "M_pca =  305 , M_lda =  23  --->  Accuracy = 74.04%\n",
      "M_pca =  305 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  305 , M_lda =  25  --->  Accuracy = 76.92%\n",
      "M_pca =  305 , M_lda =  26  --->  Accuracy = 74.04%\n",
      "M_pca =  305 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  305 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  305 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  305 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  31  --->  Accuracy = 75.96%\n",
      "M_pca =  305 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  305 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  305 , M_lda =  35  --->  Accuracy = 77.88%\n",
      "M_pca =  305 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  305 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  305 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  305 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  305 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  42  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  305 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  305 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  305 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  305 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  305 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  305 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  305 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  306 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  306 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  306 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  306 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  306 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  306 , M_lda =  6  --->  Accuracy = 42.31%\n",
      "M_pca =  306 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  306 , M_lda =  8  --->  Accuracy = 49.04%\n",
      "M_pca =  306 , M_lda =  9  --->  Accuracy = 54.81%\n",
      "M_pca =  306 , M_lda =  10  --->  Accuracy = 59.62%\n",
      "M_pca =  306 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  306 , M_lda =  12  --->  Accuracy = 64.42%\n",
      "M_pca =  306 , M_lda =  13  --->  Accuracy = 66.35%\n",
      "M_pca =  306 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  306 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  306 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  306 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  306 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  306 , M_lda =  19  --->  Accuracy = 73.08%\n",
      "M_pca =  306 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  306 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  306 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  306 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  306 , M_lda =  24  --->  Accuracy = 75.00%\n",
      "M_pca =  306 , M_lda =  25  --->  Accuracy = 76.92%\n",
      "M_pca =  306 , M_lda =  26  --->  Accuracy = 75.96%\n",
      "M_pca =  306 , M_lda =  27  --->  Accuracy = 76.92%\n",
      "M_pca =  306 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  306 , M_lda =  29  --->  Accuracy = 78.85%\n",
      "M_pca =  306 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  31  --->  Accuracy = 77.88%\n",
      "M_pca =  306 , M_lda =  32  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  306 , M_lda =  33  --->  Accuracy = 78.85%\n",
      "M_pca =  306 , M_lda =  34  --->  Accuracy = 76.92%\n",
      "M_pca =  306 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  306 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  306 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  306 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  306 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  306 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  306 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  306 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  306 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  306 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  306 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  307 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  307 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  307 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  307 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  307 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  307 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  307 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  307 , M_lda =  8  --->  Accuracy = 50.00%\n",
      "M_pca =  307 , M_lda =  9  --->  Accuracy = 52.88%\n",
      "M_pca =  307 , M_lda =  10  --->  Accuracy = 59.62%\n",
      "M_pca =  307 , M_lda =  11  --->  Accuracy = 60.58%\n",
      "M_pca =  307 , M_lda =  12  --->  Accuracy = 64.42%\n",
      "M_pca =  307 , M_lda =  13  --->  Accuracy = 70.19%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d9eeb5e78e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mM_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mW_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msvd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             raise ValueError(\"Unrecognized svd_solver='{0}'\"\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit_truncated\u001b[0;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[1;32m    493\u001b[0m                                      \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterated_power\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                                      \u001b[0mflip_sign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                                      random_state=random_state)\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[0;32m--> 326\u001b[0;31m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'QR'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/linalg/decomp_lu.py\u001b[0m in \u001b[0;36mlu\u001b[0;34m(a, permute_l, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0moverwrite_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverwrite_a\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_datacopied\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mflu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_flinalg_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpermute_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         raise ValueError('illegal value in %d-th argument of '\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LDA-PCA using scikit learn PCA function\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# KNN Classifer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "_card = 52\n",
    "D, N = X_train.shape\n",
    "        \n",
    "    \n",
    "M_pca = 1\n",
    "M_lda = 1\n",
    "\n",
    "\n",
    "standard = False\n",
    "\n",
    "M__pca_ideal = None\n",
    "M__lda_ideal = None\n",
    "acc_max = 0\n",
    "\n",
    "while M_pca <= (N-_card):\n",
    "    M_lda = 1\n",
    "    while M_lda <= (_card-1):\n",
    "\n",
    "        pca = PCA(n_components=M_pca)\n",
    "        W_train = pca.fit_transform(X_train.T)\n",
    "\n",
    "        lda = LinearDiscriminantAnalysis(n_components=M_lda)\n",
    "        W_train_2 = lda.fit_transform(W_train, y_train.T.ravel())\n",
    "\n",
    "        nn = KNeighborsClassifier(n_neighbors=1)\n",
    "        nn.fit(W_train_2, y_train.T.ravel())\n",
    "\n",
    "        W_test = pca.transform(X_test.T)\n",
    "\n",
    "        W_test_2 = lda.transform(W_test)\n",
    "\n",
    "        acc = nn.score(W_test_2, y_test.T.ravel())\n",
    "\n",
    "        print('M_pca = ', M_pca, ', M_lda = ', M_lda,' --->  Accuracy = %.2f%%' % (acc * 100))\n",
    "        \n",
    "        if (acc > acc_max):\n",
    "            M__pca_ideal = M_pca\n",
    "            M__lda_ideal = M_lda\n",
    "            acc_max = acc\n",
    "\n",
    "        M_lda = M_lda + 1\n",
    "        \n",
    "    M_pca = M_pca + 1\n",
    "    \n",
    "print (\"Accuracy is maximum for M__pca = \", M__pca_ideal, \", M_lda = \", M__lda_ideal, \" with accuracy of %.2f%%\"% (acc_max * 100), \".\")\n",
    "\n",
    "#Accuracy is maximum for M__pca =  150 , M_lda =  47  with accuracy of 94.23% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M__pca_ideal =  150\n",
      "M__lda_ideal =  47\n",
      "(2576, 416)\n",
      "(1, 416)\n",
      "Accuracy of base estimator with no pre PCA = 90.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of base estimator with pre PCA applied = 91.35%\n",
      "(416, 415)\n",
      "Accuracy of ensemble estimator = 84.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation mean accuracy = 6.51%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=1704556621, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  1  = 0.00%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=908355190, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  2  = 2.88%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=1629840588, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  3  = 0.00%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=1337434398, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  4  = 0.00%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=449113202, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  5  = 1.92%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=195085914, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  6  = 0.96%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=444907332, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  7  = 0.96%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=1664333891, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  8  = 0.96%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=148405549, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  9  = 2.88%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=1448271824, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  10  = 2.88%\n",
      "Average accuracy of sub models = 1.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of voting = 91.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "### Draft cell, do not use\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "D, N = X_train.shape\n",
    "\n",
    "standard = False\n",
    "#M__pca_ideal = 147\n",
    "#M__lda_ideal = 46\n",
    "\n",
    "print ('M__pca_ideal = ', M__pca_ideal)\n",
    "print ('M__lda_ideal = ', M__lda_ideal)\n",
    "\n",
    "M_pca_bag = N-1\n",
    "\n",
    "M_pca = 150 #M__pca_ideal\n",
    "M_lda = 47 #M__lda_ideal\n",
    "\n",
    "n_est = 10\n",
    "\n",
    "estimators = [('pca', PCA(n_components=M_pca)), ('lda', LinearDiscriminantAnalysis(n_components=M_lda)), ('knn', KNeighborsClassifier(n_neighbors=1))]\n",
    "\n",
    "base_est = Pipeline (estimators)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "\n",
    "base_est.fit(X_train.T, y_train.T.ravel())\n",
    "\n",
    "acc = base_est.score(X_test.T, y_test.T.ravel())\n",
    "print ('Accuracy of base estimator with no pre PCA = %.2f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "pca = PCA(n_components=M_pca_bag)\n",
    "W_train = pca.fit_transform(X_train.T)\n",
    "W_test = pca.transform(X_test.T)\n",
    "\n",
    "base_est.fit(W_train, y_train.T.ravel())\n",
    "\n",
    "acc = base_est.score(W_test, y_test.T.ravel())\n",
    "print ('Accuracy of base estimator with pre PCA applied = %.2f%%' % (acc * 100))\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=base_est,\n",
    "                            max_samples=1.0,\n",
    "                            max_features=1.0,\n",
    "                            bootstrap=True,\n",
    "                            #bootstrap_features=True,\n",
    "                            n_estimators=n_est)\n",
    "\n",
    "print (W_train.shape)\n",
    "\n",
    "bagging = bagging.fit(W_train, y_train.T.ravel())\n",
    "\n",
    "acc = bagging.score(W_test, y_test.T.ravel())\n",
    "print ('Accuracy of ensemble estimator = %.2f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "results = model_selection.cross_val_score(bagging, W_train, y_train.T.ravel(), cv=kfold)\n",
    "print('Cross validation mean accuracy = %.2f%%' % (results.mean() * 100))\n",
    "\n",
    "sub_model_accuracies = []\n",
    "\n",
    "sub_estimators = []\n",
    "\n",
    "for i, estimator in enumerate(bagging.estimators_):\n",
    "    print(estimator)\n",
    "    sub_model_acc = estimator.score(W_test, y_test.T.ravel())\n",
    "    print ('Accuracy of sub model ', i+1, ' = %.2f%%' % (sub_model_acc * 100))\n",
    "    sub_model_accuracies.append(sub_model_acc)\n",
    "    name = 'est'+str(i+1)\n",
    "    sub_estimators.append((name, estimator))\n",
    "    \n",
    "ave_sub_model_acc = sum(sub_model_accuracies)/n_est\n",
    "\n",
    "print ('Average accuracy of sub models = %.2f%%' % (ave_sub_model_acc * 100))\n",
    "\n",
    "\n",
    "voting = VotingClassifier(estimators=sub_estimators, voting='soft')\n",
    "voting = voting.fit(W_train, y_train.T.ravel())\n",
    "acc = voting.score(W_test, y_test.T.ravel())\n",
    "print ('Accuracy of voting = %.2f%%' % (acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Bagging\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "def bagging(n_estimators, max_samples, verbose = False):\n",
    "\n",
    "    D, N = X_train.shape\n",
    "\n",
    "    standard = False\n",
    "    #M__pca_ideal = 147\n",
    "    #M__lda_ideal = 46\n",
    "\n",
    "    if verbose:\n",
    "        print ('M__pca_ideal = ', M__pca_ideal)\n",
    "        print ('M__lda_ideal = ', M__lda_ideal)\n",
    "\n",
    "    M_pca_bag = N-1\n",
    "\n",
    "    M_pca = 150 #M__pca_ideal\n",
    "    M_lda = 47 #M__lda_ideal\n",
    "\n",
    "    estimators = [('pca', PCA(n_components=M_pca)), ('lda', LinearDiscriminantAnalysis(n_components=M_lda)), ('knn', KNeighborsClassifier(n_neighbors=1))]\n",
    "\n",
    "    base_est = Pipeline (estimators)\n",
    "\n",
    "    base_est.fit(X_train.T, y_train.T.ravel())\n",
    "\n",
    "    acc = base_est.score(X_test.T, y_test.T.ravel())\n",
    "    if verbose:\n",
    "        print ('Accuracy of base estimator with no pre PCA = %.2f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "    pca = PCA(n_components=M_pca_bag)\n",
    "    W_train = pca.fit_transform(X_train.T)\n",
    "    W_test = pca.transform(X_test.T)\n",
    "\n",
    "    base_est.fit(W_train, y_train.T.ravel())\n",
    "\n",
    "    acc = base_est.score(W_test, y_test.T.ravel())\n",
    "    if verbose:\n",
    "        print ('Accuracy of base estimator with pre PCA applied = %.2f%%' % (acc * 100))\n",
    "\n",
    "    estimators = []\n",
    "    sub_model_accuracies = []\n",
    "\n",
    "    for i in range (n_estimators):\n",
    "\n",
    "        mask = np.random.choice(np.arange(N), int(max_samples * N), replace=False)\n",
    "\n",
    "        mask = np.array(mask).ravel()\n",
    "\n",
    "        W_bag = W_train[mask, :]\n",
    "        y_bag = y_train[:, mask]\n",
    "    \n",
    "        estimator = clone(base_est)\n",
    "\n",
    "        estimator.fit(W_bag, y_bag.T.ravel())\n",
    "    \n",
    "        name = 'est_'+str(i+1)\n",
    "        estimators.append((name, estimator))\n",
    "    \n",
    "        sub_model_acc = estimator.score(W_test, y_test.T.ravel())\n",
    "        sub_model_accuracies.append(sub_model_acc)\n",
    "        if verbose:\n",
    "            print ('Accuracy of sub model ', i+1, ' = %.2f%%' % (sub_model_acc * 100))\n",
    "    \n",
    "\n",
    "    ave_sub_model_acc = sum(sub_model_accuracies)/n_estimators\n",
    "    if verbose:\n",
    "        print ('Average accuracy of sub models = %.2f%%' % (ave_sub_model_acc * 100))\n",
    "    \n",
    "    y_hat = []\n",
    "\n",
    "    for w in W_test:\n",
    "        prediction_sum = 0\n",
    "        predictions = np.empty(n_estimators, dtype = np.int64)\n",
    "        for i, (name, estimator) in enumerate(estimators):\n",
    "            y = estimator.predict(w.reshape(1, -1))\n",
    "        \n",
    "            prediction_sum = prediction_sum + float(y[0])\n",
    "            predictions[i] = int(y[0])\n",
    "        prediction = round(prediction_sum/n_estimators)\n",
    "        \n",
    "        counts = np.bincount(predictions)\n",
    "        #y_hat.append(prediction)\n",
    "        y_hat.append(np.argmax(counts))\n",
    "    \n",
    "    acc = accuracy_score(y_test.T, y_hat)\n",
    "    if verbose:\n",
    "        print ('Accuracy of ensemble estimator = %.2f%%' % (acc * 100))\n",
    "    \n",
    "    return acc, ave_sub_model_acc\n",
    "    \n",
    "        \n",
    "\n",
    "n_estimators = 50\n",
    "max_samples = 0.8\n",
    "\n",
    "acc, ave_sub_model_acc = bagging(n_estimators, max_samples)\n",
    "\n",
    "n_estimators = 30\n",
    "max_samples = 0.025\n",
    "\n",
    "acc_varying_samples = []\n",
    "acc_varying_samples_ave = []\n",
    "num_samples = []\n",
    "\n",
    "while max_samples <= 1.0:\n",
    "    acc, ave_sub_model_acc = bagging(n_estimators, max_samples)\n",
    "    acc_varying_samples.append(acc*100)\n",
    "    acc_varying_samples_ave.append(ave_sub_model_acc*100)\n",
    "    num_samples.append(max_samples)\n",
    "    max_samples = max_samples + 0.025\n",
    "\n",
    "n_estimators = 1\n",
    "max_samples = 0.8\n",
    "\n",
    "acc_varying_num_est_bag = []\n",
    "num_estimators_list = []\n",
    "n_est_test_range = 60\n",
    "\n",
    "while n_estimators <= n_est_test_range:\n",
    "    acc, ave_sub_model_acc = bagging(n_estimators, max_samples)\n",
    "    acc_varying_num_est_bag.append(acc*100)\n",
    "    num_estimators_list.append(n_estimators)\n",
    "    n_estimators = n_estimators + 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10cd78a90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10caf0358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(8.0, 6.0))\n",
    "plt.plot(num_samples, acc_varying_samples, color='green', linestyle='dashed', label='Ensemble model accuracy')\n",
    "plt.plot(num_samples, acc_varying_samples_ave, color='blue', linestyle='dashed', label='Average error individual models')\n",
    "plt.title('Accuracy vs Ratio of Samples Drawn for n_estimators=30\\n')\n",
    "plt.xlabel('Ratio of samples drawn to N')\n",
    "plt.ylabel('Recogniton Accuracy / %')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of base estimator with no pre PCA = 89.42%\n",
      "Accuracy of base estimator with pre PCA applied = 91.35%\n",
      "Accuracy of sub model  1  = 82.69%\n",
      "Accuracy of sub model  2  = 75.96%\n",
      "Accuracy of sub model  3  = 79.81%\n",
      "Accuracy of sub model  4  = 78.85%\n",
      "Accuracy of sub model  5  = 83.65%\n",
      "Accuracy of sub model  6  = 83.65%\n",
      "Accuracy of sub model  7  = 84.62%\n",
      "Accuracy of sub model  8  = 79.81%\n",
      "Accuracy of sub model  9  = 82.69%\n",
      "Accuracy of sub model  10  = 79.81%\n",
      "Accuracy of sub model  11  = 76.92%\n",
      "Accuracy of sub model  12  = 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  13  = 83.65%\n",
      "Accuracy of sub model  14  = 82.69%\n",
      "Accuracy of sub model  15  = 81.73%\n",
      "Accuracy of sub model  16  = 83.65%\n",
      "Accuracy of sub model  17  = 82.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  18  = 81.73%\n",
      "Accuracy of sub model  19  = 80.77%\n",
      "Accuracy of sub model  20  = 80.77%\n",
      "Accuracy of sub model  21  = 80.77%\n",
      "Accuracy of sub model  22  = 79.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  23  = 82.69%\n",
      "Accuracy of sub model  24  = 87.50%\n",
      "Accuracy of sub model  25  = 87.50%\n",
      "Accuracy of sub model  26  = 80.77%\n",
      "Accuracy of sub model  27  = 82.69%\n",
      "Accuracy of sub model  28  = 83.65%\n",
      "Accuracy of sub model  29  = 83.65%\n",
      "Accuracy of sub model  30  = 81.73%\n",
      "Average accuracy of sub models = 81.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ensemble estimator = 92.31%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110b86dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2561db00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "D, N = X_train.shape\n",
    "\n",
    "\n",
    "n_estimators = 30\n",
    "max_samples = 0.8\n",
    "\n",
    "verbose = True\n",
    "\n",
    "standard = False\n",
    "#M__pca_ideal = 147\n",
    "#M__lda_ideal = 46\n",
    "\n",
    "M_pca_bag = N-1\n",
    "\n",
    "M_pca = 150 #M__pca_ideal\n",
    "M_lda = 47 #M__lda_ideal\n",
    "\n",
    "estimators = [('pca', PCA(n_components=M_pca)), ('lda', LinearDiscriminantAnalysis(n_components=M_lda)), ('knn', KNeighborsClassifier(n_neighbors=1))]\n",
    "\n",
    "base_est = Pipeline (estimators)\n",
    "\n",
    "base_est.fit(X_train.T, y_train.T.ravel())\n",
    "\n",
    "acc = base_est.score(X_test.T, y_test.T.ravel())\n",
    "if verbose:\n",
    "    print ('Accuracy of base estimator with no pre PCA = %.2f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "pca = PCA(n_components=M_pca_bag)\n",
    "W_train = pca.fit_transform(X_train.T)\n",
    "W_test = pca.transform(X_test.T)\n",
    "\n",
    "base_est.fit(W_train, y_train.T.ravel())\n",
    "\n",
    "acc = base_est.score(W_test, y_test.T.ravel())\n",
    "if verbose:\n",
    "    print ('Accuracy of base estimator with pre PCA applied = %.2f%%' % (acc * 100))\n",
    "\n",
    "estimators = []\n",
    "sub_model_accuracies = []\n",
    "\n",
    "for i in range (n_estimators):\n",
    "\n",
    "    mask = np.random.choice(np.arange(N), int(max_samples * N), replace=False)\n",
    "\n",
    "    mask = np.array(mask).ravel()\n",
    "\n",
    "    W_bag = W_train[mask, :]\n",
    "    y_bag = y_train[:, mask]\n",
    "    \n",
    "    estimator = clone(base_est)\n",
    "\n",
    "    estimator.fit(W_bag, y_bag.T.ravel())\n",
    "    \n",
    "    name = 'est_'+str(i+1)\n",
    "    estimators.append((name, estimator))\n",
    "    \n",
    "    sub_model_acc = estimator.score(W_test, y_test.T.ravel())\n",
    "    sub_model_accuracies.append(sub_model_acc)\n",
    "    if verbose:\n",
    "        print ('Accuracy of sub model ', i+1, ' = %.2f%%' % (sub_model_acc * 100))\n",
    "    \n",
    "\n",
    "ave_sub_model_acc = sum(sub_model_accuracies)/n_estimators\n",
    "if verbose:\n",
    "    print ('Average accuracy of sub models = %.2f%%' % (ave_sub_model_acc * 100))\n",
    "    \n",
    "y_hat = []\n",
    "\n",
    "for w in W_test:\n",
    "    prediction_sum = 0\n",
    "    predictions = np.empty(n_estimators, dtype = np.int64)\n",
    "    for i, (name, estimator) in enumerate(estimators):\n",
    "        y = estimator.predict(w.reshape(1, -1))\n",
    "        \n",
    "        prediction_sum = prediction_sum + float(y[0])\n",
    "        predictions[i] = int(y[0])\n",
    "    prediction = round(prediction_sum/n_estimators)\n",
    "        \n",
    "    counts = np.bincount(predictions)\n",
    "    #y_hat.append(prediction)\n",
    "    y_hat.append(np.argmax(counts))\n",
    "    \n",
    "acc = accuracy_score(y_test.T, y_hat)\n",
    "if verbose:\n",
    "    print ('Accuracy of ensemble estimator = %.2f%%' % (acc * 100))\n",
    "    \n",
    "    \n",
    "cfn_matrix = confusion_matrix(y_test.T, y_hat)\n",
    "\n",
    "class_names = np.arange(1,53)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plot_confusion_matrix(cm           = cfn_matrix, \n",
    "                      normalize    = False,\n",
    "                      target_names = class_names,\n",
    "                      title        = \"Confusion Matrix\")\n",
    "\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of base estimator with no pre PCA = 91.35%\n",
      "Accuracy of base estimator with pre PCA applied = 23.08%\n",
      "Accuracy of sub model  1  = 90.38%\n",
      "Accuracy of sub model  2  = 88.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  3  = 84.62%\n",
      "Accuracy of sub model  4  = 83.65%\n",
      "Accuracy of sub model  5  = 91.35%\n",
      "Accuracy of sub model  6  = 94.23%\n",
      "Accuracy of sub model  7  = 87.50%\n",
      "Accuracy of sub model  8  = 83.65%\n",
      "Accuracy of sub model  9  = 90.38%\n",
      "Accuracy of sub model  10  = 90.38%\n",
      "Accuracy of sub model  11  = 90.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  12  = 90.38%\n",
      "Accuracy of sub model  13  = 85.58%\n",
      "Accuracy of sub model  14  = 87.50%\n",
      "Accuracy of sub model  15  = 90.38%\n",
      "Accuracy of sub model  16  = 88.46%\n",
      "Accuracy of sub model  17  = 84.62%\n",
      "Accuracy of sub model  18  = 90.38%\n",
      "Accuracy of sub model  19  = 91.35%\n",
      "Accuracy of sub model  20  = 85.58%\n",
      "Accuracy of sub model  21  = 86.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  22  = 87.50%\n",
      "Accuracy of sub model  23  = 85.58%\n",
      "Accuracy of sub model  24  = 87.50%\n",
      "Accuracy of sub model  25  = 91.35%\n",
      "Accuracy of sub model  26  = 87.50%\n",
      "Accuracy of sub model  27  = 84.62%\n",
      "Accuracy of sub model  28  = 89.42%\n",
      "Accuracy of sub model  29  = 84.62%\n",
      "Accuracy of sub model  30  = 90.38%\n",
      "Average accuracy of sub models = 88.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of ensemble estimator = 92.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Random subspace\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "def random_subspace(n_estimators, M0, M1, verbose = False):\n",
    "\n",
    "    D, N = X_train.shape\n",
    "\n",
    "    standard = False\n",
    "    #M__pca_ideal = 147\n",
    "    #M__lda_ideal = 46\n",
    "\n",
    "    #if verbose:\n",
    "    #    print ('M__pca_ideal = ', M__pca_ideal)\n",
    "    #    print ('M__lda_ideal = ', M__lda_ideal)\n",
    "\n",
    "    M_pca_bag = N-1\n",
    "\n",
    "    M_pca = 150 #M__pca_ideal\n",
    "    M_lda = 47 #M__lda_ideal\n",
    "\n",
    "    \n",
    "    assert(M1 <= (N-1-M0))\n",
    "    assert(M0+M1 > M_pca)\n",
    "    assert(M_pca > M_lda)\n",
    "\n",
    "    estimators = [('lda', LinearDiscriminantAnalysis(n_components=M_lda)), ('knn', KNeighborsClassifier(n_neighbors=1))]\n",
    "\n",
    "    base_est = Pipeline (estimators)\n",
    "\n",
    "    base_est.fit(X_train.T, y_train.T.ravel())\n",
    "\n",
    "    acc = base_est.score(X_test.T, y_test.T.ravel())\n",
    "    if verbose:\n",
    "        print ('Accuracy of base estimator with no pre PCA = %.2f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "    pca = PCA(n_components=M_pca_bag)\n",
    "    W_train = pca.fit_transform(X_train.T)\n",
    "    W_test = pca.transform(X_test.T)\n",
    "\n",
    "    base_est.fit(W_train, y_train.T.ravel())\n",
    "\n",
    "    acc = base_est.score(W_test, y_test.T.ravel())\n",
    "    if verbose:\n",
    "        print ('Accuracy of base estimator with pre PCA applied = %.2f%%' % (acc * 100))\n",
    "\n",
    "    estimators = []\n",
    "    sub_model_accuracies = []\n",
    "    masks = []\n",
    "\n",
    "    for i in range (n_estimators):\n",
    "\n",
    "        mask0 = np.arange(M0)\n",
    "        mask1 = np.random.choice(np.arange(M0, (N-1)), M1, replace=False)\n",
    "\n",
    "        mask1 = np.array(mask1).ravel()\n",
    "    \n",
    "        mask = np.concatenate((mask0, mask1), axis = None)\n",
    "        masks.append(mask)\n",
    "\n",
    "        W_bag = W_train[:, mask]\n",
    "        y_bag = y_train\n",
    "    \n",
    "        estimator = clone(base_est)\n",
    "\n",
    "        estimator.fit(W_bag, y_bag.T.ravel())\n",
    "    \n",
    "        name = 'est_'+str(i+1)\n",
    "        estimators.append((name, estimator))\n",
    "    \n",
    "        sub_model_acc = estimator.score(W_test[:, mask], y_test.T.ravel())\n",
    "        sub_model_accuracies.append(sub_model_acc)\n",
    "        if verbose:\n",
    "            print ('Accuracy of sub model ', i+1, ' = %.2f%%' % (sub_model_acc * 100))\n",
    "    \n",
    "\n",
    "    ave_sub_model_acc = sum(sub_model_accuracies)/n_estimators\n",
    "    if verbose:\n",
    "        print ('Average accuracy of sub models = %.2f%%' % (ave_sub_model_acc * 100))\n",
    "    \n",
    "    y_hat = []\n",
    "\n",
    "    for w in W_test:\n",
    "        prediction_sum = 0\n",
    "        predictions = np.empty(n_estimators, dtype = np.int64)\n",
    "        for i, (name, estimator) in enumerate(estimators):\n",
    "            y = estimator.predict(w[masks[i]].reshape(1, -1))\n",
    "        \n",
    "            prediction_sum = prediction_sum + float(y[0])\n",
    "            predictions[i] = int(y[0])\n",
    "        prediction = round(prediction_sum/n_estimators)\n",
    "    \n",
    "        counts = np.bincount(predictions)\n",
    "        #y_hat.append(prediction)\n",
    "        y_hat.append(np.argmax(counts))\n",
    "        \n",
    "    acc = accuracy_score(y_test.T, y_hat)\n",
    "    if verbose:\n",
    "        print ('Accuracy of ensemble estimator = %.2f%%' % (acc * 100))\n",
    "        \n",
    "    return acc, ave_sub_model_acc\n",
    "    \n",
    "D, N = X_train.shape      \n",
    "        \n",
    "n_estimators = 30\n",
    "M0 = 120\n",
    "M1 = 60    \n",
    "\n",
    "acc, ave_sub_model_acc = random_subspace(n_estimators, M0, M1, verbose= True)\n",
    "\n",
    "n_estimators = 30\n",
    "M0 = 0\n",
    "M1 = 150-M0+1 \n",
    "\n",
    "acc_varying_subspace = []\n",
    "num_M0 = []\n",
    "num_M1 = []\n",
    "\n",
    "M0_ideal = None\n",
    "M1_ideal = None\n",
    "acc_max = 0\n",
    "\n",
    "while M0 <= N-1:\n",
    "    M1 = max((150-M0+1), 0)\n",
    "    num_M1_i = []\n",
    "    acc_varying_subspace_i = []\n",
    "    while M1 <= (N-1-M0):\n",
    "        acc, ave_sub_model_acc = random_subspace(n_estimators, M0, M1)\n",
    "        acc_varying_subspace_i.append((acc*100))\n",
    "        num_M1_i.append(M1)\n",
    "        \n",
    "        if (acc > acc_max):\n",
    "            M0_ideal = M0\n",
    "            M1_ideal = M1\n",
    "            acc_max = acc\n",
    "        \n",
    "        M1 = M1 + 20\n",
    "        \n",
    "    num_M1.append(num_M1_i)\n",
    "    acc_varying_subspace.append(acc_varying_subspace_i)\n",
    "    num_M0.append(M0)\n",
    "    M0 = M0 + 50\n",
    "\n",
    "print (\"Accuracy is maximum for M0 = \", M0_ideal, \", M1 = \", M1_ideal, \" with accuracy of %.2f%%\"% (acc_max * 100), \".\")\n",
    "    \n",
    "n_estimators = 1\n",
    "M0 = 120\n",
    "M1 = 60\n",
    "\n",
    "acc_varying_num_est_ran_subsp = []\n",
    "\n",
    "while n_estimators <= n_est_test_range:\n",
    "    acc, ave_sub_model_acc = random_subspace(n_estimators, M0, M1)\n",
    "    acc_varying_num_est_ran_subsp.append(acc*100)\n",
    "    n_estimators = n_estimators + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 50, 100]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a21740cc0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a217b4e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (num_M1[3])\n",
    "plt.figure(figsize=(8.0, 6.0))\n",
    "color_list = ['green', 'blue', 'red', 'purple', 'orange', 'magenta', 'cyan', 'black', 'indianred', 'lightseagreen', 'gold']\n",
    "for i in range(len(num_M0)):\n",
    "    plt.plot(num_M1[i], acc_varying_subspace[i], color=color_list[i], linestyle='dashed', label='$\\mathcal{M0}$ = '+str(num_M0[i]))\n",
    "\n",
    "plt.title('Recogniton Accuracy (%) vs No. of Random Dimensions ($\\mathcal{M1}$) for a range of fixed $\\mathcal{M0}$')\n",
    "plt.xlabel('No. of Randomly Selected Dimensions $\\mathcal{M1}$')\n",
    "plt.ylabel('Recogniton Accuracy / %')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1107df860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cbe75f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8.0, 6.0))\n",
    "plt.plot(num_estimators_list, acc_varying_num_est_bag, color='green', linestyle='dashed', label='Bagging')\n",
    "plt.plot(num_estimators_list, acc_varying_num_est_ran_subsp, color='blue', linestyle='dashed', label='Random Subspace')\n",
    "plt.title('Accuracy vs number of estimators\\n')\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.ylabel('Recogniton Accuracy / %')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of base estimator with no pre PCA = 91.35%\n",
      "Accuracy of base estimator with pre PCA applied = 23.08%\n",
      "Accuracy of sub model  1  = 90.38%\n",
      "Accuracy of sub model  2  = 86.54%\n",
      "Accuracy of sub model  3  = 83.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  4  = 89.42%\n",
      "Accuracy of sub model  5  = 89.42%\n",
      "Accuracy of sub model  6  = 87.50%\n",
      "Accuracy of sub model  7  = 88.46%\n",
      "Accuracy of sub model  8  = 91.35%\n",
      "Accuracy of sub model  9  = 88.46%\n",
      "Accuracy of sub model  10  = 91.35%\n",
      "Accuracy of sub model  11  = 85.58%\n",
      "Accuracy of sub model  12  = 89.42%\n",
      "Accuracy of sub model  13  = 84.62%\n",
      "Accuracy of sub model  14  = 85.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  15  = 89.42%\n",
      "Accuracy of sub model  16  = 88.46%\n",
      "Accuracy of sub model  17  = 93.27%\n",
      "Accuracy of sub model  18  = 89.42%\n",
      "Accuracy of sub model  19  = 86.54%\n",
      "Accuracy of sub model  20  = 91.35%\n",
      "Accuracy of sub model  21  = 86.54%\n",
      "Accuracy of sub model  22  = 88.46%\n",
      "Accuracy of sub model  23  = 88.46%\n",
      "Accuracy of sub model  24  = 85.58%\n",
      "Accuracy of sub model  25  = 89.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  26  = 90.38%\n",
      "Accuracy of sub model  27  = 89.42%\n",
      "Accuracy of sub model  28  = 87.50%\n",
      "Accuracy of sub model  29  = 92.31%\n",
      "Accuracy of sub model  30  = 87.50%\n",
      "Average accuracy of sub models = 88.53%\n",
      "Accuracy of ensemble estimator = 94.23%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2182a240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2182a208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "D, N = X_train.shape      \n",
    "        \n",
    "n_estimators = 50\n",
    "M0 = M0_ideal\n",
    "M1 = M1_ideal  \n",
    "\n",
    "verbose = False\n",
    "\n",
    "standard = False\n",
    "#M__pca_ideal = 147\n",
    "#M__lda_ideal = 46\n",
    "\n",
    "#if verbose:\n",
    "#    print ('M__pca_ideal = ', M__pca_ideal)\n",
    "#    print ('M__lda_ideal = ', M__lda_ideal)\n",
    "\n",
    "M_pca_bag = N-1\n",
    "\n",
    "M_pca = 150 #M__pca_ideal\n",
    "M_lda = 47 #M__lda_ideal\n",
    "\n",
    "    \n",
    "assert(M1 <= (N-1-M0))\n",
    "assert(M0+M1 > M_pca)\n",
    "assert(M_pca > M_lda)\n",
    "\n",
    "estimators = [('lda', LinearDiscriminantAnalysis(n_components=M_lda)), ('knn', KNeighborsClassifier(n_neighbors=1))]\n",
    "\n",
    "base_est = Pipeline (estimators)\n",
    "\n",
    "base_est.fit(X_train.T, y_train.T.ravel())\n",
    "\n",
    "acc = base_est.score(X_test.T, y_test.T.ravel())\n",
    "if verbose:\n",
    "    print ('Accuracy of base estimator with no pre PCA = %.2f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "pca = PCA(n_components=M_pca_bag)\n",
    "W_train = pca.fit_transform(X_train.T)\n",
    "W_test = pca.transform(X_test.T)\n",
    "\n",
    "base_est.fit(W_train, y_train.T.ravel())\n",
    "\n",
    "acc = base_est.score(W_test, y_test.T.ravel())\n",
    "if verbose:\n",
    "    print ('Accuracy of base estimator with pre PCA applied = %.2f%%' % (acc * 100))\n",
    "\n",
    "estimators = []\n",
    "sub_model_accuracies = []\n",
    "masks = []\n",
    "\n",
    "for i in range (n_estimators):\n",
    "\n",
    "    mask0 = np.arange(M0)\n",
    "    mask1 = np.random.choice(np.arange(M0, (N-1)), M1, replace=False)\n",
    "\n",
    "    mask1 = np.array(mask1).ravel()\n",
    "    \n",
    "    mask = np.concatenate((mask0, mask1), axis = None)\n",
    "    masks.append(mask)\n",
    "\n",
    "    W_bag = W_train[:, mask]\n",
    "    y_bag = y_train\n",
    "    \n",
    "    estimator = clone(base_est)\n",
    "\n",
    "    estimator.fit(W_bag, y_bag.T.ravel())\n",
    "    \n",
    "    name = 'est_'+str(i+1)\n",
    "    estimators.append((name, estimator))\n",
    "    \n",
    "    sub_model_acc = estimator.score(W_test[:, mask], y_test.T.ravel())\n",
    "    sub_model_accuracies.append(sub_model_acc)\n",
    "    if verbose:\n",
    "        print ('Accuracy of sub model ', i+1, ' = %.2f%%' % (sub_model_acc * 100))\n",
    "    \n",
    "\n",
    "ave_sub_model_acc = sum(sub_model_accuracies)/n_estimators\n",
    "if verbose:\n",
    "    print ('Average accuracy of sub models = %.2f%%' % (ave_sub_model_acc * 100))\n",
    "    \n",
    "y_hat = []\n",
    "\n",
    "for w in W_test:\n",
    "    prediction_sum = 0\n",
    "    predictions = np.empty(n_estimators, dtype = np.int64)\n",
    "    for i, (name, estimator) in enumerate(estimators):\n",
    "        y = estimator.predict(w[masks[i]].reshape(1, -1))\n",
    "        \n",
    "        prediction_sum = prediction_sum + float(y[0])\n",
    "        predictions[i] = int(y[0])\n",
    "    prediction = round(prediction_sum/n_estimators)\n",
    "    \n",
    "    counts = np.bincount(predictions)\n",
    "    #y_hat.append(prediction)\n",
    "    y_hat.append(np.argmax(counts))\n",
    "        \n",
    "acc = accuracy_score(y_test.T, y_hat)\n",
    "if verbose:\n",
    "    print ('Accuracy of ensemble estimator = %.2f%%' % (acc * 100))\n",
    "\n",
    "cfn_matrix = confusion_matrix(y_test.T, y_hat)\n",
    "\n",
    "class_names = np.arange(1,53)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plot_confusion_matrix(cm           = cfn_matrix, \n",
    "                      normalize    = False,\n",
    "                      target_names = class_names,\n",
    "                      title        = \"Confusion Matrix\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
