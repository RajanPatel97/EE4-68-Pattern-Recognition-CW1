{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# load `.mat` file\n",
    "data = scipy.io.loadmat('face.mat')\n",
    "\n",
    "# Images\n",
    "# N: number of images\n",
    "# D: number of pixels\n",
    "X = data['X']  # shape: [D x N]\n",
    "y = data['l']  # shape: [1 x N]\n",
    "\n",
    "assert(X.shape[1] == y.shape[1])\n",
    "# Number of images\n",
    "D, N = X.shape\n",
    "\n",
    "# Fix the random seed\n",
    "np.random.seed(13)\n",
    "\n",
    "# Cardinality of labels\n",
    "_card = len(set(y.ravel()))\n",
    "\n",
    "# Step splitting of dataset\n",
    "_step = int(N / _card)\n",
    "\n",
    "# Shape boundaries\n",
    "_bounds = np.arange(0, N+1, _step)\n",
    "\n",
    "# Shapes\n",
    "shapes = list(zip(_bounds[:-1], _bounds[1:]))\n",
    "\n",
    "# Training Mask\n",
    "_mask = []\n",
    "\n",
    "for _shape in shapes:\n",
    "    _idx = np.random.choice(\n",
    "        np.arange(*_shape), int(0.8 * _step), replace=False)\n",
    "    _mask.append(_idx)\n",
    "\n",
    "mask_train = np.array(_mask).ravel()\n",
    "\n",
    "mask_test = np.array(list(set(np.arange(0, N)) - set(mask_train)))\n",
    "\n",
    "# Partition dataset to train and test sets\n",
    "X_train, X_test = X[:, mask_train], X[:, mask_test]\n",
    "y_train, y_test = y[:, mask_train], y[:, mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA(object):\n",
    "    \"\"\"Principle Component Analysis.\"\"\"\n",
    "\n",
    "    def __init__(self, n_comps=5, standard=True):\n",
    "        \"\"\"Contructor.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_comps: int\n",
    "            Number of principle components\n",
    "        \"\"\"\n",
    "        self._fitted = False\n",
    "        self.n_comps = n_comps\n",
    "        self.standard = standard\n",
    "        self.mean = None\n",
    "        self.U = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit PCA according to `X.cov()`.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Features matrix\n",
    "        Returns\n",
    "        -------\n",
    "        array: numpy.ndarray\n",
    "            Transformed features matrix\n",
    "        \"\"\"\n",
    "        self.D, N = X.shape\n",
    "        self.mean = X.mean(axis=1).reshape(-1, 1)\n",
    "        # center data\n",
    "        A = X - self.mean\n",
    "        # covariance matrix\n",
    "        S = (1 / N) * np.dot(A.T, A)\n",
    "        \n",
    "        _l, _v = np.linalg.eig(S)\n",
    "\n",
    "        _indexes = np.argsort(_l)[::-1]\n",
    "\n",
    "        # Sorted eigenvalues and eigenvectors\n",
    "        l, v = _l[_indexes], _v[:, _indexes]\n",
    "\n",
    "        V = v[:, :self.n_comps]\n",
    "\n",
    "        _U = np.dot(A, V)\n",
    "\n",
    "        self.U = _U / np.apply_along_axis(np.linalg.norm, 0, _U)\n",
    "\n",
    "        W = np.dot(self.U.T, A)\n",
    "\n",
    "        if self.standard:\n",
    "            self.W_mean = np.mean(W, axis=1)\n",
    "            self.W_std = np.std(W, axis=1)\n",
    "\n",
    "        self._fitted = True\n",
    "\n",
    "        if self.standard:\n",
    "            return ((W.T - self.W_mean) / self.W_std).T\n",
    "        else:\n",
    "            return W\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform `X` by projecting it to PCA feature space.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy.ndarray\n",
    "            Features matrix\n",
    "        Returns\n",
    "        -------\n",
    "        array: numpy.ndarray\n",
    "            Transformed features matrix\n",
    "        \"\"\"\n",
    "\n",
    "        Phi = X - self.mean\n",
    "\n",
    "        W = np.dot(self.U.T, Phi)\n",
    "\n",
    "        if self.standard:\n",
    "            return ((W.T - self.W_mean) / self.W_std).T\n",
    "        else:\n",
    "            return W\n",
    "\n",
    "    def reconstruct(self, W):\n",
    "        \"\"\"Recontruct compressed data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        W: numpy.ndarray\n",
    "            Projection coefficients matrix\n",
    "        Returns\n",
    "        -------\n",
    "        X_hat: numpy.ndarray\n",
    "            Reconstructed features matrix\n",
    "        \"\"\"\n",
    "        A_hat = np.dot(self.U, W).reshape(-1, 1)\n",
    "        A_hat = A_hat + self.mean\n",
    "        return A_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  1 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  2  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  3  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  4  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  5  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  6  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  7  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  8  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  9  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  10  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  11  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  12  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  13  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  14  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  15  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  16  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  17  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  18  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  19  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  20  --->  Accuracy = 4.81%\n",
      "M_pca =  2 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  2 , M_lda =  2  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  3  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  4  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  5  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  6  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  7  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  8  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  9  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  10  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  11  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  12  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  13  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  14  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  15  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  16  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  17  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  18  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  19  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  20  --->  Accuracy = 9.62%\n",
      "M_pca =  3 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  3 , M_lda =  2  --->  Accuracy = 5.77%\n",
      "M_pca =  3 , M_lda =  3  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  4  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  5  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  6  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  7  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  8  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  9  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  10  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  11  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  12  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  13  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  14  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  15  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  16  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  17  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  18  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  19  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  20  --->  Accuracy = 17.31%\n",
      "M_pca =  4 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  4 , M_lda =  2  --->  Accuracy = 6.73%\n",
      "M_pca =  4 , M_lda =  3  --->  Accuracy = 18.27%\n",
      "M_pca =  4 , M_lda =  4  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  5  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  6  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  7  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  8  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  9  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  10  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  11  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  12  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  13  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  14  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  15  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  16  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  17  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  18  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  19  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  20  --->  Accuracy = 25.00%\n",
      "M_pca =  5 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  5 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  5 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  5 , M_lda =  4  --->  Accuracy = 25.96%\n",
      "M_pca =  5 , M_lda =  5  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  7  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  8  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  9  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  10  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  11  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  12  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  13  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  14  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  15  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  16  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  17  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  18  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  19  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  20  --->  Accuracy = 35.58%\n",
      "M_pca =  6 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  6 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  6 , M_lda =  3  --->  Accuracy = 20.19%\n",
      "M_pca =  6 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  6 , M_lda =  5  --->  Accuracy = 35.58%\n",
      "M_pca =  6 , M_lda =  6  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  7  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  8  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  9  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  10  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  11  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  12  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  13  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  14  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  15  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  16  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  17  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  18  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  19  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  20  --->  Accuracy = 41.35%\n",
      "M_pca =  7 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  7 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  7 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  7 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  7 , M_lda =  5  --->  Accuracy = 32.69%\n",
      "M_pca =  7 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  7 , M_lda =  7  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  8  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  9  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  10  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  11  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  12  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  13  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  14  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  15  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  16  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  17  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  18  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  19  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  20  --->  Accuracy = 37.50%\n",
      "M_pca =  8 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  8 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  8 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  8 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  8 , M_lda =  5  --->  Accuracy = 33.65%\n",
      "M_pca =  8 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  8 , M_lda =  7  --->  Accuracy = 44.23%\n",
      "M_pca =  8 , M_lda =  8  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  9  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  10  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  11  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  12  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  13  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  14  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  15  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  16  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  17  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  18  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  19  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  20  --->  Accuracy = 42.31%\n",
      "M_pca =  9 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  9 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  9 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  9 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  9 , M_lda =  5  --->  Accuracy = 36.54%\n",
      "M_pca =  9 , M_lda =  6  --->  Accuracy = 38.46%\n",
      "M_pca =  9 , M_lda =  7  --->  Accuracy = 40.38%\n",
      "M_pca =  9 , M_lda =  8  --->  Accuracy = 44.23%\n",
      "M_pca =  9 , M_lda =  9  --->  Accuracy = 48.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  9 , M_lda =  10  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  11  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  12  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  13  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  14  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  15  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  16  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  17  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  18  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  19  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  20  --->  Accuracy = 48.08%\n",
      "M_pca =  10 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  10 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  10 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  10 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  10 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  10 , M_lda =  6  --->  Accuracy = 42.31%\n",
      "M_pca =  10 , M_lda =  7  --->  Accuracy = 45.19%\n",
      "M_pca =  10 , M_lda =  8  --->  Accuracy = 50.96%\n",
      "M_pca =  10 , M_lda =  9  --->  Accuracy = 49.04%\n",
      "M_pca =  10 , M_lda =  10  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  11  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  12  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  13  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  14  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  15  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  16  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  17  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  18  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  19  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  20  --->  Accuracy = 52.88%\n",
      "M_pca =  11 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  11 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  11 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  11 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  11 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  11 , M_lda =  6  --->  Accuracy = 39.42%\n",
      "M_pca =  11 , M_lda =  7  --->  Accuracy = 47.12%\n",
      "M_pca =  11 , M_lda =  8  --->  Accuracy = 51.92%\n",
      "M_pca =  11 , M_lda =  9  --->  Accuracy = 50.96%\n",
      "M_pca =  11 , M_lda =  10  --->  Accuracy = 51.92%\n",
      "M_pca =  11 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  12  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  13  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  14  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  15  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  16  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  17  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  18  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  19  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  20  --->  Accuracy = 56.73%\n",
      "M_pca =  12 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  12 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  12 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  12 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  12 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  12 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  12 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  12 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  12 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  12 , M_lda =  10  --->  Accuracy = 60.58%\n",
      "M_pca =  12 , M_lda =  11  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  12  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  13  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  14  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  15  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  16  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  17  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  18  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  19  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  20  --->  Accuracy = 59.62%\n",
      "M_pca =  13 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  13 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  13 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  13 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  13 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  13 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  13 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  13 , M_lda =  8  --->  Accuracy = 53.85%\n",
      "M_pca =  13 , M_lda =  9  --->  Accuracy = 53.85%\n",
      "M_pca =  13 , M_lda =  10  --->  Accuracy = 55.77%\n",
      "M_pca =  13 , M_lda =  11  --->  Accuracy = 57.69%\n",
      "M_pca =  13 , M_lda =  12  --->  Accuracy = 58.65%\n",
      "M_pca =  13 , M_lda =  13  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  14  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  15  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  16  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  17  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  18  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  19  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  20  --->  Accuracy = 63.46%\n",
      "M_pca =  14 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  14 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  14 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  14 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  14 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  14 , M_lda =  6  --->  Accuracy = 43.27%\n",
      "M_pca =  14 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  14 , M_lda =  8  --->  Accuracy = 48.08%\n",
      "M_pca =  14 , M_lda =  9  --->  Accuracy = 51.92%\n",
      "M_pca =  14 , M_lda =  10  --->  Accuracy = 57.69%\n",
      "M_pca =  14 , M_lda =  11  --->  Accuracy = 61.54%\n",
      "M_pca =  14 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  14 , M_lda =  13  --->  Accuracy = 63.46%\n",
      "M_pca =  14 , M_lda =  14  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  15  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  16  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  17  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  18  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  19  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  20  --->  Accuracy = 64.42%\n",
      "M_pca =  15 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  15 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  15 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  15 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  15 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  15 , M_lda =  6  --->  Accuracy = 46.15%\n",
      "M_pca =  15 , M_lda =  7  --->  Accuracy = 50.96%\n",
      "M_pca =  15 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  15 , M_lda =  9  --->  Accuracy = 53.85%\n",
      "M_pca =  15 , M_lda =  10  --->  Accuracy = 55.77%\n",
      "M_pca =  15 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  15 , M_lda =  12  --->  Accuracy = 60.58%\n",
      "M_pca =  15 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  15 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  15 , M_lda =  15  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  16  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  17  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  18  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  19  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  20  --->  Accuracy = 65.38%\n",
      "M_pca =  16 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  16 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  16 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  16 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  16 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  16 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  16 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  16 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  16 , M_lda =  9  --->  Accuracy = 54.81%\n",
      "M_pca =  16 , M_lda =  10  --->  Accuracy = 54.81%\n",
      "M_pca =  16 , M_lda =  11  --->  Accuracy = 58.65%\n",
      "M_pca =  16 , M_lda =  12  --->  Accuracy = 57.69%\n",
      "M_pca =  16 , M_lda =  13  --->  Accuracy = 61.54%\n",
      "M_pca =  16 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  15  --->  Accuracy = 67.31%\n",
      "M_pca =  16 , M_lda =  16  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  17  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  18  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  19  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  20  --->  Accuracy = 66.35%\n",
      "M_pca =  17 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  17 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  17 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  17 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  17 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  17 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  17 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  17 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  17 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  17 , M_lda =  10  --->  Accuracy = 56.73%\n",
      "M_pca =  17 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  17 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  17 , M_lda =  13  --->  Accuracy = 62.50%\n",
      "M_pca =  17 , M_lda =  14  --->  Accuracy = 65.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  17 , M_lda =  15  --->  Accuracy = 66.35%\n",
      "M_pca =  17 , M_lda =  16  --->  Accuracy = 65.38%\n",
      "M_pca =  17 , M_lda =  17  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  18  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  19  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  20  --->  Accuracy = 67.31%\n",
      "M_pca =  18 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  18 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  18 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  18 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  18 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  18 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  18 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  18 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  18 , M_lda =  9  --->  Accuracy = 55.77%\n",
      "M_pca =  18 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  11  --->  Accuracy = 57.69%\n",
      "M_pca =  18 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  13  --->  Accuracy = 63.46%\n",
      "M_pca =  18 , M_lda =  14  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  15  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  16  --->  Accuracy = 67.31%\n",
      "M_pca =  18 , M_lda =  17  --->  Accuracy = 68.27%\n",
      "M_pca =  18 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  20  --->  Accuracy = 70.19%\n",
      "M_pca =  19 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  19 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  19 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  19 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  19 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  19 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  19 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  19 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  19 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  19 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  19 , M_lda =  11  --->  Accuracy = 61.54%\n",
      "M_pca =  19 , M_lda =  12  --->  Accuracy = 63.46%\n",
      "M_pca =  19 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  19 , M_lda =  14  --->  Accuracy = 65.38%\n",
      "M_pca =  19 , M_lda =  15  --->  Accuracy = 64.42%\n",
      "M_pca =  19 , M_lda =  16  --->  Accuracy = 66.35%\n",
      "M_pca =  19 , M_lda =  17  --->  Accuracy = 68.27%\n",
      "M_pca =  19 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  19 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  20 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  20 , M_lda =  2  --->  Accuracy = 10.58%\n",
      "M_pca =  20 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  20 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  20 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  20 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  20 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  20 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  20 , M_lda =  9  --->  Accuracy = 60.58%\n",
      "M_pca =  20 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  20 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  20 , M_lda =  12  --->  Accuracy = 66.35%\n",
      "M_pca =  20 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  20 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  20 , M_lda =  15  --->  Accuracy = 69.23%\n",
      "M_pca =  20 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  20 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  21 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  21 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  21 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  21 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  21 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  21 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  21 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  21 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  21 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  21 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  21 , M_lda =  14  --->  Accuracy = 64.42%\n",
      "M_pca =  21 , M_lda =  15  --->  Accuracy = 65.38%\n",
      "M_pca =  21 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  21 , M_lda =  17  --->  Accuracy = 68.27%\n",
      "M_pca =  21 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  21 , M_lda =  19  --->  Accuracy = 69.23%\n",
      "M_pca =  21 , M_lda =  20  --->  Accuracy = 70.19%\n",
      "M_pca =  22 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  22 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  22 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  22 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  22 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  22 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  22 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  22 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  22 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  22 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  22 , M_lda =  12  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  22 , M_lda =  15  --->  Accuracy = 67.31%\n",
      "M_pca =  22 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  22 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  22 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  22 , M_lda =  19  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  20  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  23 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  23 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  23 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  23 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  23 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  23 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  23 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  23 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  23 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  23 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  14  --->  Accuracy = 70.19%\n",
      "M_pca =  23 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  23 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  24 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  24 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  24 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  24 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  24 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  24 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  24 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  24 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  14  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  24 , M_lda =  16  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  17  --->  Accuracy = 69.23%\n",
      "M_pca =  24 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  20  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  25 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  25 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  25 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  25 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  25 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  25 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  25 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  25 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  25 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  25 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  25 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  25 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  18  --->  Accuracy = 69.23%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  25 , M_lda =  19  --->  Accuracy = 69.23%\n",
      "M_pca =  25 , M_lda =  20  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  26 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  26 , M_lda =  3  --->  Accuracy = 20.19%\n",
      "M_pca =  26 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  26 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  26 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  26 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  26 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  26 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  26 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  26 , M_lda =  13  --->  Accuracy = 68.27%\n",
      "M_pca =  26 , M_lda =  14  --->  Accuracy = 67.31%\n",
      "M_pca =  26 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  26 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  26 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  27 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  27 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  27 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  27 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  27 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  27 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  27 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  27 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  27 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  27 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  27 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  27 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  27 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  27 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  27 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  27 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  27 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  27 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  28 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  28 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  28 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  28 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  28 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  28 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  28 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  28 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  28 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  28 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  28 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  28 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  28 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  28 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  28 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  28 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  29 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  29 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  29 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  29 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  29 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  29 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  29 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  29 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  29 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  29 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  29 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  29 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  29 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  29 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  29 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  29 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  29 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  30 , M_lda =  1  --->  Accuracy = 13.46%\n",
      "M_pca =  30 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  30 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  30 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  30 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  30 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  30 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  30 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  30 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  30 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  30 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  30 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  30 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  30 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  30 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  30 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  30 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  30 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  30 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  31 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  31 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  31 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  31 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  31 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  31 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  31 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  31 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  31 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  31 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  31 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  32 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  32 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  32 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  32 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  32 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  32 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  32 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  32 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  32 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  32 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  32 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  32 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  32 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  32 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  32 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  32 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  33 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  33 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  33 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  33 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  33 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  33 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  33 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  33 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  33 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  33 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  33 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  1  --->  Accuracy = 6.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  34 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  34 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  34 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  34 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  34 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  34 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  34 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  34 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  34 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  34 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  34 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  34 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  34 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  35 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  35 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  35 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  35 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  35 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  35 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  35 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  35 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  35 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  35 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  35 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  35 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  36 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  36 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  36 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  36 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  36 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  36 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  36 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  36 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  36 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  36 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  36 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  36 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  36 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  36 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  37 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  37 , M_lda =  3  --->  Accuracy = 20.19%\n",
      "M_pca =  37 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  37 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  37 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  37 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  37 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  37 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  37 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  38 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  38 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  38 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  38 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  38 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  38 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  38 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  38 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  38 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  38 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  38 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  39 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  39 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  39 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  39 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  39 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  39 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  39 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  39 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  39 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  39 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  39 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  39 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  39 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  39 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  40 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  40 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  40 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  40 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  40 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  40 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  40 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  40 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  40 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  40 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  41 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  41 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  41 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  41 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  41 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  41 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  41 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  41 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  41 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  41 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  42 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  42 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  42 , M_lda =  4  --->  Accuracy = 42.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  42 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  42 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  42 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  42 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  42 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  43 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  43 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  43 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  43 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  43 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  43 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  43 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  43 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  43 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  43 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  43 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  43 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  44 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  44 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  44 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  44 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  44 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  44 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  44 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  44 , M_lda =  8  --->  Accuracy = 75.96%\n",
      "M_pca =  44 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  44 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  44 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  44 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  44 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  44 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  45 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  45 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  45 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  45 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  45 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  45 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  45 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  45 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  45 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  45 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  45 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  45 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  45 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  45 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  46 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  46 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  46 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  46 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  46 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  46 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  46 , M_lda =  8  --->  Accuracy = 75.00%\n",
      "M_pca =  46 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  46 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  46 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  46 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  46 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  47 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  47 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  47 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  47 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  47 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  47 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  47 , M_lda =  8  --->  Accuracy = 75.96%\n",
      "M_pca =  47 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  47 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  47 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  47 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  47 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  48 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  48 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  48 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  48 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  48 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  48 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  48 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  48 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  48 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  48 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  48 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  48 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  49 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  49 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  49 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  49 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  49 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  49 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  49 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  49 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  49 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  49 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  49 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  49 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  49 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  50 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  50 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  50 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  50 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  50 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  50 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  50 , M_lda =  8  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  50 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  50 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  50 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  50 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  50 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  50 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "Accuracy is maximum for M__pca =  47 , M_lda =  15  with accuracy of 84.62% .\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# KNN Classifer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "_card = 52\n",
    "D, N = X_train.shape\n",
    "\n",
    "M_pca = 1\n",
    "M_lda = 1\n",
    "\n",
    "acc_array = np.empty((50, 20))\n",
    "M_pca_array = np.arange(1, 50+1)\n",
    "M_lda_array = np.arange(1, 20+1)\n",
    "\n",
    "\n",
    "standard = False\n",
    "\n",
    "M__pca_ideal = None\n",
    "M__lda_ideal = None\n",
    "acc_max = 0\n",
    "\n",
    "while M_pca <= 50: #(N-_card):\n",
    "    M_lda = 1\n",
    "    while M_lda <= 20: #(_card-1):\n",
    "\n",
    "        pca = PCA(n_comps=M_pca, standard=standard)\n",
    "        W_train = pca.fit(X_train)\n",
    "        \n",
    "\n",
    "        lda = LinearDiscriminantAnalysis(n_components=M_lda)\n",
    "        W_train_2 = lda.fit_transform(W_train.T, y_train.T.ravel())\n",
    "\n",
    "        nn = KNeighborsClassifier(n_neighbors=1)\n",
    "        nn.fit(W_train_2, y_train.T.ravel())\n",
    "\n",
    "        W_test = pca.transform(X_test)\n",
    "\n",
    "        W_test_2 = lda.transform(W_test.T)\n",
    "\n",
    "        acc = nn.score(W_test_2, y_test.T.ravel())\n",
    "        \n",
    "        acc_array[M_pca-1, M_lda-1] = acc\n",
    "\n",
    "        print('M_pca = ', M_pca, ', M_lda = ', M_lda,' --->  Accuracy = %.2f%%' % (acc * 100))\n",
    "        \n",
    "        if (acc > acc_max):\n",
    "            M__pca_ideal = M_pca\n",
    "            M__lda_ideal = M_lda\n",
    "            acc_max = acc\n",
    "\n",
    "        M_lda = M_lda + 1\n",
    "        \n",
    "    M_pca = M_pca + 1\n",
    "    \n",
    "print (\"Accuracy is maximum for M__pca = \", M__pca_ideal, \", M_lda = \", M__lda_ideal, \" with accuracy of %.2f%%\"% (acc_max * 100), \".\")\n",
    "\n",
    "#Ideal: M_pca =  147 , M_lda =  46  --->  Accuracy = 94.23%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 20)\n",
      "(50, 20)\n",
      "(50, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1f099eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "x = np.linspace(1, 20, 20)\n",
    "y = np.linspace(1, 50, 50)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "print(acc_array.shape)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, acc_array, rstride=1, cstride=1,\n",
    "                cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Accuracy varying M_pca & M_lda');\n",
    "ax.set_xlabel('M_lda')\n",
    "ax.set_ylabel('M_pca')\n",
    "ax.set_zlabel('accuracy');\n",
    "\n",
    "ax.view_init(30, 230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm,\n",
    "                          target_names,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=90, size=6)\n",
    "        plt.yticks(tick_marks, target_names, size=6)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     size=3,\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     size=3,\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  147 , M_lda =  46  --->  Accuracy = 94.23%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a21c49be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a21c49898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "_card = 52\n",
    "D, N = X_train.shape\n",
    "\n",
    "M_pca = 147\n",
    "M_lda = 46\n",
    "\n",
    "standard = False\n",
    "\n",
    "pca = PCA(n_comps=M_pca, standard=standard)\n",
    "W_train = pca.fit(X_train)\n",
    "        \n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=M_lda)\n",
    "W_train_2 = lda.fit_transform(W_train.T, y_train.T.ravel())\n",
    "\n",
    "nn = KNeighborsClassifier(n_neighbors=1)\n",
    "nn.fit(W_train_2, y_train.T.ravel())\n",
    "\n",
    "W_test = pca.transform(X_test)\n",
    "\n",
    "W_test_2 = lda.transform(W_test.T)\n",
    "\n",
    "acc = nn.score(W_test_2, y_test.T.ravel())\n",
    "\n",
    "print('M_pca = ', M_pca, ', M_lda = ', M_lda,' --->  Accuracy = %.2f%%' % (acc * 100))\n",
    "\n",
    "y_hat = nn.predict(W_test_2)\n",
    "\n",
    "cfn_matrix = confusion_matrix(y_test.T, y_hat)\n",
    "\n",
    "class_names = np.arange(1,53)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plot_confusion_matrix(cm           = cfn_matrix, \n",
    "                      normalize    = False,\n",
    "                      target_names = class_names,\n",
    "                      title        = \"Confusion Matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a234b0c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prettify plots\n",
    "plt.rcParams['figure.figsize'] = [12.0, 9.0]\n",
    "sns.set_palette(sns.color_palette(\"muted\"))\n",
    "_palette = sns.color_palette(\"muted\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "\n",
    "done = {'success': False, 'failure': False}\n",
    "first_failure = True\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "\n",
    "for y, t, w, x in zip(y_hat, y_test.T.ravel(), W_test.T, X_test.T):\n",
    "    if y == t and done['success'] is False:\n",
    "        x_hat = pca.reconstruct(w)\n",
    "        axes[0].imshow(x.reshape(46,56).T,\n",
    "                       cmap=plt.get_cmap('gray'))\n",
    "        axes[0].set_title(\n",
    "            'Successful NN Classification\\nPredicted Class: %d, True Class: %d' % (y, t), color=_palette[1])\n",
    "        done['success'] = True\n",
    "    elif y != t and done['failure'] is False and first_failure is True:\n",
    "        first_failure = False\n",
    "    elif y != t and done['failure'] is False and first_failure is False:\n",
    "        x_hat = pca.reconstruct(w)\n",
    "        axes[1].imshow(x.reshape(46,56).T,\n",
    "                       cmap=plt.get_cmap('gray'))\n",
    "        axes[1].set_title(\n",
    "            'Failed NN Classification\\nPredicted Class: %d, True Class: %d' % (y, t), color=_palette[2])\n",
    "        done['failure'] = True\n",
    "    #elif done['failure'] is True and done['success'] is True:\n",
    "     #break\n",
    "\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  1 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  2  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  3  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  4  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  5  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  6  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  7  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  8  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  9  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  10  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  11  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  12  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  13  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  14  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  15  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  16  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  17  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  18  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  19  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  20  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  21  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  22  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  23  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  24  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  25  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  26  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  27  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  28  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  29  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  30  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  31  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  32  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  33  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  34  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  35  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  36  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  37  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  38  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  39  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  40  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  41  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  42  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  43  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  44  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  45  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  46  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  47  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  48  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  49  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  50  --->  Accuracy = 4.81%\n",
      "M_pca =  1 , M_lda =  51  --->  Accuracy = 4.81%\n",
      "M_pca =  2 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  2 , M_lda =  2  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  3  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  4  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  5  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  6  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  7  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  8  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  9  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  10  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  11  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  12  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  13  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  14  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  15  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  16  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  17  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  18  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  19  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  20  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  21  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  22  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  23  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  24  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  25  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  26  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  27  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  28  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  29  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  30  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  31  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  32  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  33  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  34  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  35  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  36  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  37  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  38  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  39  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  40  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  41  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  42  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  43  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  44  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  45  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  46  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  47  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  48  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  49  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  50  --->  Accuracy = 9.62%\n",
      "M_pca =  2 , M_lda =  51  --->  Accuracy = 9.62%\n",
      "M_pca =  3 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  3 , M_lda =  2  --->  Accuracy = 5.77%\n",
      "M_pca =  3 , M_lda =  3  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  4  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  5  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  6  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  7  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  8  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  9  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  10  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  11  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  12  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  13  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  14  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  15  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  16  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  17  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  18  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  19  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  20  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  21  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  22  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  23  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  24  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  25  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  26  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  27  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  28  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  29  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  30  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  31  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  32  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  33  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  34  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  35  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  36  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  37  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  38  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  39  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  40  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  41  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  42  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  43  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  44  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  45  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  46  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  47  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  48  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  49  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  50  --->  Accuracy = 17.31%\n",
      "M_pca =  3 , M_lda =  51  --->  Accuracy = 17.31%\n",
      "M_pca =  4 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  4 , M_lda =  2  --->  Accuracy = 6.73%\n",
      "M_pca =  4 , M_lda =  3  --->  Accuracy = 18.27%\n",
      "M_pca =  4 , M_lda =  4  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  5  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  6  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  7  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  8  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  9  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  10  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  11  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  12  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  13  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  14  --->  Accuracy = 25.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  4 , M_lda =  15  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  16  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  17  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  18  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  19  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  20  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  21  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  22  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  23  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  24  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  25  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  26  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  27  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  28  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  29  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  30  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  31  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  32  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  33  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  34  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  35  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  36  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  37  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  38  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  39  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  40  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  41  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  42  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  43  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  44  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  45  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  46  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  47  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  48  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  49  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  50  --->  Accuracy = 25.00%\n",
      "M_pca =  4 , M_lda =  51  --->  Accuracy = 25.00%\n",
      "M_pca =  5 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  5 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  5 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  5 , M_lda =  4  --->  Accuracy = 25.96%\n",
      "M_pca =  5 , M_lda =  5  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  7  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  8  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  9  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  10  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  11  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  12  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  13  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  14  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  15  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  16  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  17  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  18  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  19  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  20  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  21  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  22  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  23  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  24  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  25  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  26  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  27  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  28  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  29  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  30  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  31  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  32  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  33  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  34  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  35  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  36  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  37  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  38  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  39  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  40  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  41  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  42  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  43  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  44  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  45  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  46  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  47  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  48  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  49  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  50  --->  Accuracy = 35.58%\n",
      "M_pca =  5 , M_lda =  51  --->  Accuracy = 35.58%\n",
      "M_pca =  6 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  6 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  6 , M_lda =  3  --->  Accuracy = 20.19%\n",
      "M_pca =  6 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  6 , M_lda =  5  --->  Accuracy = 35.58%\n",
      "M_pca =  6 , M_lda =  6  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  7  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  8  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  9  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  10  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  11  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  12  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  13  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  14  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  15  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  16  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  17  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  18  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  19  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  20  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  21  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  22  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  23  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  24  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  25  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  26  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  27  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  28  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  29  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  30  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  31  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  32  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  33  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  34  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  35  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  36  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  37  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  38  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  39  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  40  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  41  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  42  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  43  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  44  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  45  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  46  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  47  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  48  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  49  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  50  --->  Accuracy = 41.35%\n",
      "M_pca =  6 , M_lda =  51  --->  Accuracy = 41.35%\n",
      "M_pca =  7 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  7 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  7 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  7 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  7 , M_lda =  5  --->  Accuracy = 32.69%\n",
      "M_pca =  7 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  7 , M_lda =  7  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  8  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  9  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  10  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  11  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  12  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  13  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  14  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  15  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  16  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  17  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  18  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  19  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  20  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  21  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  22  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  23  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  24  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  25  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  26  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  27  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  28  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  29  --->  Accuracy = 37.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  7 , M_lda =  30  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  31  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  32  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  33  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  34  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  35  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  36  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  37  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  38  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  39  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  40  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  41  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  42  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  43  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  44  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  45  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  46  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  47  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  48  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  49  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  50  --->  Accuracy = 37.50%\n",
      "M_pca =  7 , M_lda =  51  --->  Accuracy = 37.50%\n",
      "M_pca =  8 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  8 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  8 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  8 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  8 , M_lda =  5  --->  Accuracy = 33.65%\n",
      "M_pca =  8 , M_lda =  6  --->  Accuracy = 35.58%\n",
      "M_pca =  8 , M_lda =  7  --->  Accuracy = 44.23%\n",
      "M_pca =  8 , M_lda =  8  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  9  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  10  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  11  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  12  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  13  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  14  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  15  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  16  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  17  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  18  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  19  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  20  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  21  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  22  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  23  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  24  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  25  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  26  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  27  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  28  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  29  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  30  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  31  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  32  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  33  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  34  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  35  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  36  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  37  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  38  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  39  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  40  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  41  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  42  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  43  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  44  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  45  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  46  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  47  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  48  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  49  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  50  --->  Accuracy = 42.31%\n",
      "M_pca =  8 , M_lda =  51  --->  Accuracy = 42.31%\n",
      "M_pca =  9 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  9 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  9 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  9 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  9 , M_lda =  5  --->  Accuracy = 36.54%\n",
      "M_pca =  9 , M_lda =  6  --->  Accuracy = 38.46%\n",
      "M_pca =  9 , M_lda =  7  --->  Accuracy = 40.38%\n",
      "M_pca =  9 , M_lda =  8  --->  Accuracy = 44.23%\n",
      "M_pca =  9 , M_lda =  9  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  10  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  11  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  12  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  13  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  14  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  15  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  16  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  17  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  18  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  19  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  20  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  21  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  22  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  23  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  24  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  25  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  26  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  27  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  28  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  29  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  30  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  31  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  32  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  33  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  34  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  35  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  36  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  37  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  38  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  39  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  40  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  41  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  42  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  43  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  44  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  45  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  46  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  47  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  48  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  49  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  50  --->  Accuracy = 48.08%\n",
      "M_pca =  9 , M_lda =  51  --->  Accuracy = 48.08%\n",
      "M_pca =  10 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  10 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  10 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  10 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  10 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  10 , M_lda =  6  --->  Accuracy = 42.31%\n",
      "M_pca =  10 , M_lda =  7  --->  Accuracy = 45.19%\n",
      "M_pca =  10 , M_lda =  8  --->  Accuracy = 50.96%\n",
      "M_pca =  10 , M_lda =  9  --->  Accuracy = 49.04%\n",
      "M_pca =  10 , M_lda =  10  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  11  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  12  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  13  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  14  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  15  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  16  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  17  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  18  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  19  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  20  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  21  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  22  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  23  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  24  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  25  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  26  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  27  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  28  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  29  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  30  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  31  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  32  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  33  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  34  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  35  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  36  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  37  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  38  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  39  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  40  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  41  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  42  --->  Accuracy = 52.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  10 , M_lda =  43  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  44  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  45  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  46  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  47  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  48  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  49  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  50  --->  Accuracy = 52.88%\n",
      "M_pca =  10 , M_lda =  51  --->  Accuracy = 52.88%\n",
      "M_pca =  11 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  11 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  11 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  11 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  11 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  11 , M_lda =  6  --->  Accuracy = 39.42%\n",
      "M_pca =  11 , M_lda =  7  --->  Accuracy = 47.12%\n",
      "M_pca =  11 , M_lda =  8  --->  Accuracy = 51.92%\n",
      "M_pca =  11 , M_lda =  9  --->  Accuracy = 50.96%\n",
      "M_pca =  11 , M_lda =  10  --->  Accuracy = 51.92%\n",
      "M_pca =  11 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  12  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  13  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  14  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  15  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  16  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  17  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  18  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  19  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  20  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  21  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  22  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  23  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  24  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  25  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  26  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  27  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  28  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  29  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  30  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  31  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  32  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  33  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  34  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  35  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  36  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  37  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  38  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  39  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  40  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  41  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  42  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  43  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  44  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  45  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  46  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  47  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  48  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  49  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  50  --->  Accuracy = 56.73%\n",
      "M_pca =  11 , M_lda =  51  --->  Accuracy = 56.73%\n",
      "M_pca =  12 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  12 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  12 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  12 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  12 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  12 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  12 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  12 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  12 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  12 , M_lda =  10  --->  Accuracy = 60.58%\n",
      "M_pca =  12 , M_lda =  11  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  12  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  13  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  14  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  15  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  16  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  17  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  18  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  19  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  20  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  21  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  22  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  23  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  24  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  25  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  26  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  27  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  28  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  29  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  30  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  31  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  32  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  33  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  34  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  35  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  36  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  37  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  38  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  39  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  40  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  41  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  42  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  43  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  44  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  45  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  46  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  47  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  48  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  49  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  50  --->  Accuracy = 59.62%\n",
      "M_pca =  12 , M_lda =  51  --->  Accuracy = 59.62%\n",
      "M_pca =  13 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  13 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  13 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  13 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  13 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  13 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  13 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  13 , M_lda =  8  --->  Accuracy = 53.85%\n",
      "M_pca =  13 , M_lda =  9  --->  Accuracy = 53.85%\n",
      "M_pca =  13 , M_lda =  10  --->  Accuracy = 55.77%\n",
      "M_pca =  13 , M_lda =  11  --->  Accuracy = 57.69%\n",
      "M_pca =  13 , M_lda =  12  --->  Accuracy = 58.65%\n",
      "M_pca =  13 , M_lda =  13  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  14  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  15  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  16  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  17  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  18  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  19  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  20  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  21  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  22  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  23  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  24  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  25  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  26  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  27  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  28  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  29  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  30  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  31  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  32  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  33  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  34  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  35  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  36  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  37  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  38  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  39  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  40  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  41  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  42  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  43  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  44  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  45  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  46  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  47  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  48  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  49  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  50  --->  Accuracy = 63.46%\n",
      "M_pca =  13 , M_lda =  51  --->  Accuracy = 63.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  14 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  14 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  14 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  14 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  14 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  14 , M_lda =  6  --->  Accuracy = 43.27%\n",
      "M_pca =  14 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  14 , M_lda =  8  --->  Accuracy = 48.08%\n",
      "M_pca =  14 , M_lda =  9  --->  Accuracy = 51.92%\n",
      "M_pca =  14 , M_lda =  10  --->  Accuracy = 57.69%\n",
      "M_pca =  14 , M_lda =  11  --->  Accuracy = 61.54%\n",
      "M_pca =  14 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  14 , M_lda =  13  --->  Accuracy = 63.46%\n",
      "M_pca =  14 , M_lda =  14  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  15  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  16  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  17  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  18  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  19  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  20  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  21  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  22  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  23  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  24  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  25  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  26  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  27  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  28  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  29  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  30  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  31  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  32  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  33  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  34  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  35  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  36  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  37  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  38  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  39  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  40  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  41  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  42  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  43  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  44  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  45  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  46  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  47  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  48  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  49  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  50  --->  Accuracy = 64.42%\n",
      "M_pca =  14 , M_lda =  51  --->  Accuracy = 64.42%\n",
      "M_pca =  15 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  15 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  15 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  15 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  15 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  15 , M_lda =  6  --->  Accuracy = 46.15%\n",
      "M_pca =  15 , M_lda =  7  --->  Accuracy = 50.96%\n",
      "M_pca =  15 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  15 , M_lda =  9  --->  Accuracy = 52.88%\n",
      "M_pca =  15 , M_lda =  10  --->  Accuracy = 55.77%\n",
      "M_pca =  15 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  15 , M_lda =  12  --->  Accuracy = 60.58%\n",
      "M_pca =  15 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  15 , M_lda =  14  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  15  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  16  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  17  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  18  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  19  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  20  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  21  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  22  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  23  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  24  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  25  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  26  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  27  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  28  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  29  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  30  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  31  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  32  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  33  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  34  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  35  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  36  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  37  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  38  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  39  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  40  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  41  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  42  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  43  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  44  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  45  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  46  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  47  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  48  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  49  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  50  --->  Accuracy = 65.38%\n",
      "M_pca =  15 , M_lda =  51  --->  Accuracy = 65.38%\n",
      "M_pca =  16 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  16 , M_lda =  2  --->  Accuracy = 10.58%\n",
      "M_pca =  16 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  16 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  16 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  16 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  16 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  16 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  16 , M_lda =  9  --->  Accuracy = 54.81%\n",
      "M_pca =  16 , M_lda =  10  --->  Accuracy = 53.85%\n",
      "M_pca =  16 , M_lda =  11  --->  Accuracy = 58.65%\n",
      "M_pca =  16 , M_lda =  12  --->  Accuracy = 57.69%\n",
      "M_pca =  16 , M_lda =  13  --->  Accuracy = 61.54%\n",
      "M_pca =  16 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  15  --->  Accuracy = 67.31%\n",
      "M_pca =  16 , M_lda =  16  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  17  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  18  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  19  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  20  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  21  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  22  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  23  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  24  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  25  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  26  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  27  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  28  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  29  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  30  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  31  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  32  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  33  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  34  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  35  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  36  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  37  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  38  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  39  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  40  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  41  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  42  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  43  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  44  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  45  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  46  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  47  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  48  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  49  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  50  --->  Accuracy = 66.35%\n",
      "M_pca =  16 , M_lda =  51  --->  Accuracy = 66.35%\n",
      "M_pca =  17 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  17 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  17 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  17 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  17 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  17 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  17 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  17 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  17 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  17 , M_lda =  10  --->  Accuracy = 55.77%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  17 , M_lda =  11  --->  Accuracy = 56.73%\n",
      "M_pca =  17 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  17 , M_lda =  13  --->  Accuracy = 62.50%\n",
      "M_pca =  17 , M_lda =  14  --->  Accuracy = 65.38%\n",
      "M_pca =  17 , M_lda =  15  --->  Accuracy = 66.35%\n",
      "M_pca =  17 , M_lda =  16  --->  Accuracy = 65.38%\n",
      "M_pca =  17 , M_lda =  17  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  18  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  19  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  20  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  21  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  22  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  23  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  24  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  25  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  26  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  27  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  28  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  29  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  30  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  31  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  32  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  33  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  34  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  35  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  36  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  37  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  38  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  39  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  40  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  41  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  42  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  43  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  44  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  45  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  46  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  47  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  48  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  49  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  50  --->  Accuracy = 67.31%\n",
      "M_pca =  17 , M_lda =  51  --->  Accuracy = 67.31%\n",
      "M_pca =  18 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  18 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  18 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  18 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  18 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  18 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  18 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  18 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  18 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  18 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  11  --->  Accuracy = 57.69%\n",
      "M_pca =  18 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  13  --->  Accuracy = 64.42%\n",
      "M_pca =  18 , M_lda =  14  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  15  --->  Accuracy = 61.54%\n",
      "M_pca =  18 , M_lda =  16  --->  Accuracy = 67.31%\n",
      "M_pca =  18 , M_lda =  17  --->  Accuracy = 67.31%\n",
      "M_pca =  18 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  20  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  21  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  22  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  23  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  24  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  25  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  26  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  27  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  28  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  29  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  30  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  31  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  32  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  33  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  34  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  35  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  36  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  37  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  38  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  39  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  40  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  41  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  42  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  43  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  44  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  45  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  46  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  47  --->  Accuracy = 71.15%\n",
      "M_pca =  18 , M_lda =  48  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  49  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  50  --->  Accuracy = 70.19%\n",
      "M_pca =  18 , M_lda =  51  --->  Accuracy = 70.19%\n",
      "M_pca =  19 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  19 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  19 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  19 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  19 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  19 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  19 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  19 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  19 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  19 , M_lda =  10  --->  Accuracy = 62.50%\n",
      "M_pca =  19 , M_lda =  11  --->  Accuracy = 61.54%\n",
      "M_pca =  19 , M_lda =  12  --->  Accuracy = 64.42%\n",
      "M_pca =  19 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  19 , M_lda =  14  --->  Accuracy = 65.38%\n",
      "M_pca =  19 , M_lda =  15  --->  Accuracy = 65.38%\n",
      "M_pca =  19 , M_lda =  16  --->  Accuracy = 66.35%\n",
      "M_pca =  19 , M_lda =  17  --->  Accuracy = 68.27%\n",
      "M_pca =  19 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  19 , M_lda =  19  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  21  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  23  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  24  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  25  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  26  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  27  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  28  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  29  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  30  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  31  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  32  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  33  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  34  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  36  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  37  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  38  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  39  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  40  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  41  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  43  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  44  --->  Accuracy = 71.15%\n",
      "M_pca =  19 , M_lda =  45  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  46  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  47  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  48  --->  Accuracy = 73.08%\n",
      "M_pca =  19 , M_lda =  49  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  50  --->  Accuracy = 72.12%\n",
      "M_pca =  19 , M_lda =  51  --->  Accuracy = 72.12%\n",
      "M_pca =  20 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  20 , M_lda =  2  --->  Accuracy = 10.58%\n",
      "M_pca =  20 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  20 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  20 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  20 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  20 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  20 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  20 , M_lda =  9  --->  Accuracy = 60.58%\n",
      "M_pca =  20 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  20 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  20 , M_lda =  12  --->  Accuracy = 66.35%\n",
      "M_pca =  20 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  20 , M_lda =  15  --->  Accuracy = 69.23%\n",
      "M_pca =  20 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  20 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  20 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  20 , M_lda =  20  --->  Accuracy = 75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  20 , M_lda =  21  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  22  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  24  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  25  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  26  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  27  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  28  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  29  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  30  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  31  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  32  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  33  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  34  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  35  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  36  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  37  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  38  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  39  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  40  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  41  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  43  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  44  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  45  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  46  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  47  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  48  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  49  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  50  --->  Accuracy = 75.00%\n",
      "M_pca =  20 , M_lda =  51  --->  Accuracy = 75.00%\n",
      "M_pca =  21 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  21 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  21 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  21 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  21 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  21 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  21 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  21 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  21 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  21 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  21 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  21 , M_lda =  14  --->  Accuracy = 64.42%\n",
      "M_pca =  21 , M_lda =  15  --->  Accuracy = 66.35%\n",
      "M_pca =  21 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  21 , M_lda =  17  --->  Accuracy = 68.27%\n",
      "M_pca =  21 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  21 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  21 , M_lda =  20  --->  Accuracy = 71.15%\n",
      "M_pca =  21 , M_lda =  21  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  23  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  24  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  25  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  26  --->  Accuracy = 73.08%\n",
      "M_pca =  21 , M_lda =  27  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  28  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  29  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  30  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  31  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  32  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  33  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  36  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  37  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  38  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  39  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  40  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  41  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  43  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  44  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  45  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  46  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  47  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  48  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  49  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  50  --->  Accuracy = 72.12%\n",
      "M_pca =  21 , M_lda =  51  --->  Accuracy = 72.12%\n",
      "M_pca =  22 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  22 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  22 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  22 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  22 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  22 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  22 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  22 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  22 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  22 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  22 , M_lda =  12  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  22 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  22 , M_lda =  15  --->  Accuracy = 67.31%\n",
      "M_pca =  22 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  22 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  22 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  22 , M_lda =  19  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  20  --->  Accuracy = 71.15%\n",
      "M_pca =  22 , M_lda =  21  --->  Accuracy = 72.12%\n",
      "M_pca =  22 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  23  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  24  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  25  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  26  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  27  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  28  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  29  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  31  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  33  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  34  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  35  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  36  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  37  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  38  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  39  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  40  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  41  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  42  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  43  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  44  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  45  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  46  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  47  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  48  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  49  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  50  --->  Accuracy = 73.08%\n",
      "M_pca =  22 , M_lda =  51  --->  Accuracy = 73.08%\n",
      "M_pca =  23 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  23 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  23 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  23 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  23 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  23 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  23 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  23 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  23 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  23 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  23 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  14  --->  Accuracy = 70.19%\n",
      "M_pca =  23 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  23 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  23 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  23 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  23 , M_lda =  23  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  24  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  25  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  26  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  27  --->  Accuracy = 73.08%\n",
      "M_pca =  23 , M_lda =  28  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  29  --->  Accuracy = 74.04%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  23 , M_lda =  30  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  32  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  33  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  34  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  35  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  36  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  37  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  38  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  39  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  40  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  41  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  42  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  43  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  44  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  45  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  46  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  47  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  49  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  50  --->  Accuracy = 74.04%\n",
      "M_pca =  23 , M_lda =  51  --->  Accuracy = 74.04%\n",
      "M_pca =  24 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  24 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  24 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  24 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  24 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  24 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  24 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  24 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  24 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  14  --->  Accuracy = 72.12%\n",
      "M_pca =  24 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  24 , M_lda =  16  --->  Accuracy = 69.23%\n",
      "M_pca =  24 , M_lda =  17  --->  Accuracy = 69.23%\n",
      "M_pca =  24 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  24 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  20  --->  Accuracy = 69.23%\n",
      "M_pca =  24 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  22  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  23  --->  Accuracy = 71.15%\n",
      "M_pca =  24 , M_lda =  24  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  25  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  26  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  27  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  28  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  29  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  31  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  33  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  34  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  35  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  36  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  37  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  38  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  39  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  40  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  41  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  42  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  43  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  44  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  45  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  46  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  47  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  48  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  49  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  50  --->  Accuracy = 73.08%\n",
      "M_pca =  24 , M_lda =  51  --->  Accuracy = 73.08%\n",
      "M_pca =  25 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  25 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  25 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  25 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  25 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  25 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  25 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  25 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  25 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  25 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  25 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  25 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  25 , M_lda =  18  --->  Accuracy = 69.23%\n",
      "M_pca =  25 , M_lda =  19  --->  Accuracy = 69.23%\n",
      "M_pca =  25 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  25 , M_lda =  21  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  22  --->  Accuracy = 69.23%\n",
      "M_pca =  25 , M_lda =  23  --->  Accuracy = 68.27%\n",
      "M_pca =  25 , M_lda =  24  --->  Accuracy = 66.35%\n",
      "M_pca =  25 , M_lda =  25  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  26  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  27  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  28  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  29  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  30  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  31  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  32  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  33  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  34  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  35  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  36  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  37  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  38  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  39  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  40  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  41  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  42  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  43  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  44  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  45  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  46  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  47  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  48  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  49  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  50  --->  Accuracy = 71.15%\n",
      "M_pca =  25 , M_lda =  51  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  26 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  26 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  26 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  26 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  26 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  26 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  26 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  26 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  26 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  26 , M_lda =  13  --->  Accuracy = 68.27%\n",
      "M_pca =  26 , M_lda =  14  --->  Accuracy = 67.31%\n",
      "M_pca =  26 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  26 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  26 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  26 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  26 , M_lda =  22  --->  Accuracy = 71.15%\n",
      "M_pca =  26 , M_lda =  23  --->  Accuracy = 69.23%\n",
      "M_pca =  26 , M_lda =  24  --->  Accuracy = 69.23%\n",
      "M_pca =  26 , M_lda =  25  --->  Accuracy = 69.23%\n",
      "M_pca =  26 , M_lda =  26  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  27  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  28  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  29  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  30  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  31  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  32  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  33  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  36  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  37  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  38  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  39  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  40  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  41  --->  Accuracy = 72.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  26 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  43  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  44  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  45  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  46  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  47  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  48  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  49  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  50  --->  Accuracy = 72.12%\n",
      "M_pca =  26 , M_lda =  51  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  27 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  27 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  27 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  27 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  27 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  27 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  27 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  27 , M_lda =  9  --->  Accuracy = 65.38%\n",
      "M_pca =  27 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  27 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  27 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  27 , M_lda =  14  --->  Accuracy = 70.19%\n",
      "M_pca =  27 , M_lda =  15  --->  Accuracy = 70.19%\n",
      "M_pca =  27 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  27 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  27 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  27 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  27 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  27 , M_lda =  21  --->  Accuracy = 75.00%\n",
      "M_pca =  27 , M_lda =  22  --->  Accuracy = 74.04%\n",
      "M_pca =  27 , M_lda =  23  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  24  --->  Accuracy = 70.19%\n",
      "M_pca =  27 , M_lda =  25  --->  Accuracy = 70.19%\n",
      "M_pca =  27 , M_lda =  26  --->  Accuracy = 71.15%\n",
      "M_pca =  27 , M_lda =  27  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  28  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  29  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  30  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  31  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  32  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  33  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  36  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  37  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  38  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  39  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  40  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  41  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  43  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  44  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  45  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  46  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  47  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  48  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  49  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  50  --->  Accuracy = 72.12%\n",
      "M_pca =  27 , M_lda =  51  --->  Accuracy = 72.12%\n",
      "M_pca =  28 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  28 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  28 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  28 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  28 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  28 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  28 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  28 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  28 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  28 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  28 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  28 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  28 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  28 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  19  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  28 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  28 , M_lda =  23  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  24  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  25  --->  Accuracy = 67.31%\n",
      "M_pca =  28 , M_lda =  26  --->  Accuracy = 67.31%\n",
      "M_pca =  28 , M_lda =  27  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  28  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  29  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  30  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  31  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  32  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  33  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  34  --->  Accuracy = 68.27%\n",
      "M_pca =  28 , M_lda =  35  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  36  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  37  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  38  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  39  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  40  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  41  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  42  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  43  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  44  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  45  --->  Accuracy = 71.15%\n",
      "M_pca =  28 , M_lda =  46  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  47  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  48  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  49  --->  Accuracy = 69.23%\n",
      "M_pca =  28 , M_lda =  50  --->  Accuracy = 70.19%\n",
      "M_pca =  28 , M_lda =  51  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  29 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  29 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  29 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  29 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  29 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  29 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  29 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  29 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  29 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  29 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  29 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  29 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  29 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  29 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  29 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  22  --->  Accuracy = 71.15%\n",
      "M_pca =  29 , M_lda =  23  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  24  --->  Accuracy = 73.08%\n",
      "M_pca =  29 , M_lda =  25  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  26  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  27  --->  Accuracy = 68.27%\n",
      "M_pca =  29 , M_lda =  28  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  29  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  30  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  31  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  32  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  33  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  34  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  35  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  36  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  37  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  38  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  39  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  40  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  41  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  42  --->  Accuracy = 68.27%\n",
      "M_pca =  29 , M_lda =  43  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  44  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  45  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  46  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  47  --->  Accuracy = 69.23%\n",
      "M_pca =  29 , M_lda =  48  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  49  --->  Accuracy = 68.27%\n",
      "M_pca =  29 , M_lda =  50  --->  Accuracy = 70.19%\n",
      "M_pca =  29 , M_lda =  51  --->  Accuracy = 70.19%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  30 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  30 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  30 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  30 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  30 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  30 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  30 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  30 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  30 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  30 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  30 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  30 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  30 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  30 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  30 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  30 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  30 , M_lda =  18  --->  Accuracy = 70.19%\n",
      "M_pca =  30 , M_lda =  19  --->  Accuracy = 70.19%\n",
      "M_pca =  30 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  30 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  30 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  30 , M_lda =  23  --->  Accuracy = 72.12%\n",
      "M_pca =  30 , M_lda =  24  --->  Accuracy = 71.15%\n",
      "M_pca =  30 , M_lda =  25  --->  Accuracy = 72.12%\n",
      "M_pca =  30 , M_lda =  26  --->  Accuracy = 69.23%\n",
      "M_pca =  30 , M_lda =  27  --->  Accuracy = 70.19%\n",
      "M_pca =  30 , M_lda =  28  --->  Accuracy = 66.35%\n",
      "M_pca =  30 , M_lda =  29  --->  Accuracy = 66.35%\n",
      "M_pca =  30 , M_lda =  30  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  31  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  32  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  33  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  34  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  35  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  36  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  37  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  38  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  39  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  40  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  41  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  42  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  43  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  44  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  45  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  46  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  47  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  48  --->  Accuracy = 69.23%\n",
      "M_pca =  30 , M_lda =  49  --->  Accuracy = 67.31%\n",
      "M_pca =  30 , M_lda =  50  --->  Accuracy = 68.27%\n",
      "M_pca =  30 , M_lda =  51  --->  Accuracy = 67.31%\n",
      "M_pca =  31 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  31 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  31 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  31 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  31 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  31 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  31 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  31 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  31 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  31 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  31 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  31 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  31 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  31 , M_lda =  23  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  24  --->  Accuracy = 75.00%\n",
      "M_pca =  31 , M_lda =  25  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  26  --->  Accuracy = 74.04%\n",
      "M_pca =  31 , M_lda =  27  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  28  --->  Accuracy = 71.15%\n",
      "M_pca =  31 , M_lda =  29  --->  Accuracy = 71.15%\n",
      "M_pca =  31 , M_lda =  30  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  31  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  32  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  33  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  34  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  35  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  36  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  37  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  38  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  39  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  40  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  41  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  42  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  43  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  44  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  45  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  46  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  47  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  48  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  49  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  50  --->  Accuracy = 70.19%\n",
      "M_pca =  31 , M_lda =  51  --->  Accuracy = 70.19%\n",
      "M_pca =  32 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  32 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  32 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  32 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  32 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  32 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  32 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  32 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  32 , M_lda =  9  --->  Accuracy = 65.38%\n",
      "M_pca =  32 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  32 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  32 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  32 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  32 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  32 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  32 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  32 , M_lda =  24  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  25  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  26  --->  Accuracy = 75.00%\n",
      "M_pca =  32 , M_lda =  27  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  28  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  29  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  31  --->  Accuracy = 71.15%\n",
      "M_pca =  32 , M_lda =  32  --->  Accuracy = 70.19%\n",
      "M_pca =  32 , M_lda =  33  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  34  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  35  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  36  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  37  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  38  --->  Accuracy = 75.00%\n",
      "M_pca =  32 , M_lda =  39  --->  Accuracy = 71.15%\n",
      "M_pca =  32 , M_lda =  40  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  41  --->  Accuracy = 73.08%\n",
      "M_pca =  32 , M_lda =  42  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  43  --->  Accuracy = 69.23%\n",
      "M_pca =  32 , M_lda =  44  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  45  --->  Accuracy = 71.15%\n",
      "M_pca =  32 , M_lda =  46  --->  Accuracy = 69.23%\n",
      "M_pca =  32 , M_lda =  47  --->  Accuracy = 70.19%\n",
      "M_pca =  32 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  49  --->  Accuracy = 70.19%\n",
      "M_pca =  32 , M_lda =  50  --->  Accuracy = 74.04%\n",
      "M_pca =  32 , M_lda =  51  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  33 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  33 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  33 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  33 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  33 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  33 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  33 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  33 , M_lda =  9  --->  Accuracy = 66.35%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  33 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  33 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  33 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  21  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  33 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  24  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  25  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  26  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  27  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  28  --->  Accuracy = 75.00%\n",
      "M_pca =  33 , M_lda =  29  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  30  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  31  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  32  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  33  --->  Accuracy = 75.96%\n",
      "M_pca =  33 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  35  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  36  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  37  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  38  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  39  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  40  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  41  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  42  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  43  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  44  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  45  --->  Accuracy = 70.19%\n",
      "M_pca =  33 , M_lda =  46  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  47  --->  Accuracy = 72.12%\n",
      "M_pca =  33 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  33 , M_lda =  49  --->  Accuracy = 73.08%\n",
      "M_pca =  33 , M_lda =  50  --->  Accuracy = 71.15%\n",
      "M_pca =  33 , M_lda =  51  --->  Accuracy = 71.15%\n",
      "M_pca =  34 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  34 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  34 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  34 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  34 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  34 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  34 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  34 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  34 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  34 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  34 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  34 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  34 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  34 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  22  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  34 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  34 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  34 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  34 , M_lda =  27  --->  Accuracy = 76.92%\n",
      "M_pca =  34 , M_lda =  28  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  29  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  30  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  32  --->  Accuracy = 72.12%\n",
      "M_pca =  34 , M_lda =  33  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  34  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  35  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  36  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  37  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  38  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  39  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  40  --->  Accuracy = 75.96%\n",
      "M_pca =  34 , M_lda =  41  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  43  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  44  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  45  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  46  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  47  --->  Accuracy = 74.04%\n",
      "M_pca =  34 , M_lda =  48  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  49  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  50  --->  Accuracy = 75.00%\n",
      "M_pca =  34 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  35 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  35 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  35 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  35 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  35 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  35 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  35 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  35 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  35 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  35 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  35 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  35 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  35 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  35 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  35 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  35 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  35 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  35 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  35 , M_lda =  27  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  28  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  29  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  30  --->  Accuracy = 74.04%\n",
      "M_pca =  35 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  35 , M_lda =  32  --->  Accuracy = 74.04%\n",
      "M_pca =  35 , M_lda =  33  --->  Accuracy = 73.08%\n",
      "M_pca =  35 , M_lda =  34  --->  Accuracy = 75.00%\n",
      "M_pca =  35 , M_lda =  35  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  36  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  37  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  38  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  39  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  40  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  41  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  42  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  44  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  35 , M_lda =  46  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  48  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  49  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  50  --->  Accuracy = 75.96%\n",
      "M_pca =  35 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  36 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  36 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  36 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  36 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  36 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  36 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  36 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  36 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  36 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  36 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  36 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  36 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  36 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  36 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  19  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  36 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  36 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  36 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  36 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  36 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  36 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  36 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  36 , M_lda =  28  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  29  --->  Accuracy = 75.00%\n",
      "M_pca =  36 , M_lda =  30  --->  Accuracy = 74.04%\n",
      "M_pca =  36 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  36 , M_lda =  32  --->  Accuracy = 75.00%\n",
      "M_pca =  36 , M_lda =  33  --->  Accuracy = 74.04%\n",
      "M_pca =  36 , M_lda =  34  --->  Accuracy = 74.04%\n",
      "M_pca =  36 , M_lda =  35  --->  Accuracy = 75.00%\n",
      "M_pca =  36 , M_lda =  36  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  37  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  38  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  39  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  40  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  41  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  42  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  36 , M_lda =  44  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  46  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  47  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  48  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  36 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  37 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  37 , M_lda =  3  --->  Accuracy = 19.23%\n",
      "M_pca =  37 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  37 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  37 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  37 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  37 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  37 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  37 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  24  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  37 , M_lda =  26  --->  Accuracy = 76.92%\n",
      "M_pca =  37 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  37 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  37 , M_lda =  29  --->  Accuracy = 78.85%\n",
      "M_pca =  37 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  31  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  33  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  34  --->  Accuracy = 73.08%\n",
      "M_pca =  37 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  37 , M_lda =  36  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  37  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  38  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  37 , M_lda =  40  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  41  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  43  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  44  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  45  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  46  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  48  --->  Accuracy = 75.96%\n",
      "M_pca =  37 , M_lda =  49  --->  Accuracy = 75.00%\n",
      "M_pca =  37 , M_lda =  50  --->  Accuracy = 74.04%\n",
      "M_pca =  37 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  38 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  38 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  38 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  38 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  38 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  38 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  38 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  38 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  38 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  38 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  38 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  38 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  38 , M_lda =  29  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  38 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  38 , M_lda =  33  --->  Accuracy = 71.15%\n",
      "M_pca =  38 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  38 , M_lda =  35  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  36  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  37  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  38  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  39  --->  Accuracy = 76.92%\n",
      "M_pca =  38 , M_lda =  40  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  41  --->  Accuracy = 74.04%\n",
      "M_pca =  38 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  44  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  45  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  46  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  38 , M_lda =  48  --->  Accuracy = 73.08%\n",
      "M_pca =  38 , M_lda =  49  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  50  --->  Accuracy = 75.00%\n",
      "M_pca =  38 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  39 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  39 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  39 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  39 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  39 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  39 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  39 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  39 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  39 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  39 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  39 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  39 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  39 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  39 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  39 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  39 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  39 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  39 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  39 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  39 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  39 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  39 , M_lda =  29  --->  Accuracy = 75.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  39 , M_lda =  30  --->  Accuracy = 73.08%\n",
      "M_pca =  39 , M_lda =  31  --->  Accuracy = 73.08%\n",
      "M_pca =  39 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  39 , M_lda =  33  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  34  --->  Accuracy = 72.12%\n",
      "M_pca =  39 , M_lda =  35  --->  Accuracy = 72.12%\n",
      "M_pca =  39 , M_lda =  36  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  37  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  38  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  39  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  40  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  41  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  43  --->  Accuracy = 75.00%\n",
      "M_pca =  39 , M_lda =  44  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  45  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  46  --->  Accuracy = 74.04%\n",
      "M_pca =  39 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  48  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  49  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  50  --->  Accuracy = 75.96%\n",
      "M_pca =  39 , M_lda =  51  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  40 , M_lda =  2  --->  Accuracy = 9.62%\n",
      "M_pca =  40 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  40 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  40 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  40 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  40 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  40 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  40 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  40 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  40 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  40 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  24  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  25  --->  Accuracy = 76.92%\n",
      "M_pca =  40 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  40 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  40 , M_lda =  28  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  29  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  30  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  31  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  32  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  33  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  34  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  35  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  36  --->  Accuracy = 75.96%\n",
      "M_pca =  40 , M_lda =  37  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  38  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  39  --->  Accuracy = 75.00%\n",
      "M_pca =  40 , M_lda =  40  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  41  --->  Accuracy = 72.12%\n",
      "M_pca =  40 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  40 , M_lda =  43  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  44  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  45  --->  Accuracy = 72.12%\n",
      "M_pca =  40 , M_lda =  46  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  47  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  40 , M_lda =  49  --->  Accuracy = 72.12%\n",
      "M_pca =  40 , M_lda =  50  --->  Accuracy = 73.08%\n",
      "M_pca =  40 , M_lda =  51  --->  Accuracy = 72.12%\n",
      "M_pca =  41 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  41 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  41 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  41 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  41 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  41 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  41 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  41 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  41 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  41 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  41 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  41 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  41 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  41 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  41 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  41 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  41 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  41 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  41 , M_lda =  33  --->  Accuracy = 76.92%\n",
      "M_pca =  41 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  35  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  36  --->  Accuracy = 75.96%\n",
      "M_pca =  41 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  41 , M_lda =  39  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  40  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  41  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  43  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  44  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  45  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  46  --->  Accuracy = 75.00%\n",
      "M_pca =  41 , M_lda =  47  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  49  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  50  --->  Accuracy = 74.04%\n",
      "M_pca =  41 , M_lda =  51  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  42 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  42 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  42 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  42 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  42 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  42 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  42 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  42 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  42 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  42 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  42 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  42 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  42 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  42 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  33  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  36  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  42 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  39  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  40  --->  Accuracy = 73.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  42 , M_lda =  41  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  42 , M_lda =  43  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  44  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  42 , M_lda =  46  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  47  --->  Accuracy = 75.00%\n",
      "M_pca =  42 , M_lda =  48  --->  Accuracy = 75.96%\n",
      "M_pca =  42 , M_lda =  49  --->  Accuracy = 74.04%\n",
      "M_pca =  42 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  42 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  43 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  43 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  43 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  43 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  43 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  43 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  43 , M_lda =  8  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  43 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  43 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  43 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  43 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  43 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  43 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  43 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  43 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  43 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  32  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  33  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  34  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  43 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  43 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  43 , M_lda =  39  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  40  --->  Accuracy = 74.04%\n",
      "M_pca =  43 , M_lda =  41  --->  Accuracy = 74.04%\n",
      "M_pca =  43 , M_lda =  42  --->  Accuracy = 72.12%\n",
      "M_pca =  43 , M_lda =  43  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  44  --->  Accuracy = 75.00%\n",
      "M_pca =  43 , M_lda =  45  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  46  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  48  --->  Accuracy = 76.92%\n",
      "M_pca =  43 , M_lda =  49  --->  Accuracy = 74.04%\n",
      "M_pca =  43 , M_lda =  50  --->  Accuracy = 75.96%\n",
      "M_pca =  43 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  44 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  44 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  44 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  44 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  44 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  44 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  44 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  44 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  44 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  44 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  44 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  44 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  44 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  44 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  44 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  44 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  44 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  44 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  44 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  44 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  44 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  35  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  44 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  44 , M_lda =  40  --->  Accuracy = 76.92%\n",
      "M_pca =  44 , M_lda =  41  --->  Accuracy = 76.92%\n",
      "M_pca =  44 , M_lda =  42  --->  Accuracy = 75.00%\n",
      "M_pca =  44 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  44 , M_lda =  44  --->  Accuracy = 75.00%\n",
      "M_pca =  44 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  44 , M_lda =  46  --->  Accuracy = 73.08%\n",
      "M_pca =  44 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  44 , M_lda =  48  --->  Accuracy = 74.04%\n",
      "M_pca =  44 , M_lda =  49  --->  Accuracy = 75.00%\n",
      "M_pca =  44 , M_lda =  50  --->  Accuracy = 75.00%\n",
      "M_pca =  44 , M_lda =  51  --->  Accuracy = 74.04%\n",
      "M_pca =  45 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  45 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  45 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  45 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  45 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  45 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  45 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  45 , M_lda =  8  --->  Accuracy = 75.96%\n",
      "M_pca =  45 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  45 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  45 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  45 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  45 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  45 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  45 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  45 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  25  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  45 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  45 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  45 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  33  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  45 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  45 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  38  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  40  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  41  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  42  --->  Accuracy = 77.88%\n",
      "M_pca =  45 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  44  --->  Accuracy = 75.00%\n",
      "M_pca =  45 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  47  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  45 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  45 , M_lda =  51  --->  Accuracy = 75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  46 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  46 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  46 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  46 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  46 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  46 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  46 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  46 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  46 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  46 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  46 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  46 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  46 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  46 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  46 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  46 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  46 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  46 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  46 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  46 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  31  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  46 , M_lda =  33  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  34  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  35  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  46 , M_lda =  39  --->  Accuracy = 75.96%\n",
      "M_pca =  46 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  46 , M_lda =  41  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  42  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  43  --->  Accuracy = 77.88%\n",
      "M_pca =  46 , M_lda =  44  --->  Accuracy = 75.00%\n",
      "M_pca =  46 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  46 , M_lda =  46  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  46 , M_lda =  48  --->  Accuracy = 75.96%\n",
      "M_pca =  46 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  46 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  47 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  47 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  47 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  47 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  47 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  47 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  47 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  47 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  47 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  47 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  47 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  47 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  47 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  47 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  47 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  47 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  32  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  47 , M_lda =  34  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  47 , M_lda =  36  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  38  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  47 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  41  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  42  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  47 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  45  --->  Accuracy = 75.96%\n",
      "M_pca =  47 , M_lda =  46  --->  Accuracy = 76.92%\n",
      "M_pca =  47 , M_lda =  47  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  49  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  47 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  48 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  48 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  48 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  48 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  48 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  48 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  48 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  48 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  48 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  48 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  48 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  48 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  48 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  48 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  48 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  48 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  48 , M_lda =  32  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  33  --->  Accuracy = 76.92%\n",
      "M_pca =  48 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  48 , M_lda =  35  --->  Accuracy = 76.92%\n",
      "M_pca =  48 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  48 , M_lda =  40  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  48 , M_lda =  42  --->  Accuracy = 78.85%\n",
      "M_pca =  48 , M_lda =  43  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  48 , M_lda =  46  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  47  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  49  --->  Accuracy = 77.88%\n",
      "M_pca =  48 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  48 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  49 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  49 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  49 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  49 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  49 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  49 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  49 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  49 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  49 , M_lda =  11  --->  Accuracy = 77.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  49 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  49 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  49 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  49 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  49 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  49 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  49 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  49 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  49 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  49 , M_lda =  31  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  36  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  49 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  40  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  49 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  49 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  49 , M_lda =  48  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  49 , M_lda =  50  --->  Accuracy = 75.96%\n",
      "M_pca =  49 , M_lda =  51  --->  Accuracy = 75.00%\n",
      "M_pca =  50 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  50 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  50 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  50 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  50 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  50 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  50 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  50 , M_lda =  8  --->  Accuracy = 78.85%\n",
      "M_pca =  50 , M_lda =  9  --->  Accuracy = 78.85%\n",
      "M_pca =  50 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  50 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  50 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  50 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  50 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  50 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  50 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  50 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  50 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  50 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  50 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  50 , M_lda =  33  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  40  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  50 , M_lda =  42  --->  Accuracy = 78.85%\n",
      "M_pca =  50 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  50 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  45  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  46  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  47  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  50 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  50 , M_lda =  50  --->  Accuracy = 75.00%\n",
      "M_pca =  50 , M_lda =  51  --->  Accuracy = 77.88%\n",
      "M_pca =  51 , M_lda =  1  --->  Accuracy = 12.50%\n",
      "M_pca =  51 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  51 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  51 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  51 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  51 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  51 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  51 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  51 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  51 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  51 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  51 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  51 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  51 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  51 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  51 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  51 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  51 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  51 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  51 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  51 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  51 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  51 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  51 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  51 , M_lda =  33  --->  Accuracy = 77.88%\n",
      "M_pca =  51 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  51 , M_lda =  35  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  36  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  51 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  51 , M_lda =  41  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  42  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  51 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  51 , M_lda =  47  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  51 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  51 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  52 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  52 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  52 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  52 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  52 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  52 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  52 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  52 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  52 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  52 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  52 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  52 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  52 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  22  --->  Accuracy = 81.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  52 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  52 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  52 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  52 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  52 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  52 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  34  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  52 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  52 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  52 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  52 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  52 , M_lda =  47  --->  Accuracy = 75.96%\n",
      "M_pca =  52 , M_lda =  48  --->  Accuracy = 76.92%\n",
      "M_pca =  52 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  52 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  52 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  53 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  53 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  53 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  53 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  53 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  53 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  53 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  53 , M_lda =  8  --->  Accuracy = 75.00%\n",
      "M_pca =  53 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  53 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  53 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  53 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  53 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  53 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  53 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  53 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  53 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  53 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  53 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  53 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  53 , M_lda =  43  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  53 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  53 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  53 , M_lda =  47  --->  Accuracy = 76.92%\n",
      "M_pca =  53 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  53 , M_lda =  49  --->  Accuracy = 76.92%\n",
      "M_pca =  53 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  53 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  54 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  54 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  54 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  54 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  54 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  54 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  54 , M_lda =  7  --->  Accuracy = 71.15%\n",
      "M_pca =  54 , M_lda =  8  --->  Accuracy = 77.88%\n",
      "M_pca =  54 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  54 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  54 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  54 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  54 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  54 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  54 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  54 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  54 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  54 , M_lda =  41  --->  Accuracy = 77.88%\n",
      "M_pca =  54 , M_lda =  42  --->  Accuracy = 79.81%\n",
      "M_pca =  54 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  54 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  54 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  54 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  54 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  55 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  55 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  55 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  55 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  55 , M_lda =  6  --->  Accuracy = 66.35%\n",
      "M_pca =  55 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  55 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  55 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  55 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  55 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  55 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  55 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  55 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  55 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  55 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  55 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  55 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  55 , M_lda =  32  --->  Accuracy = 81.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  55 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  34  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  55 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  55 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  55 , M_lda =  41  --->  Accuracy = 77.88%\n",
      "M_pca =  55 , M_lda =  42  --->  Accuracy = 76.92%\n",
      "M_pca =  55 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  55 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  55 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  55 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  55 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  55 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  55 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  56 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  56 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  56 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  56 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  56 , M_lda =  6  --->  Accuracy = 67.31%\n",
      "M_pca =  56 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  56 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  56 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  56 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  56 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  56 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  56 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  56 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  56 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  56 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  56 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  56 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  56 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  41  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  42  --->  Accuracy = 76.92%\n",
      "M_pca =  56 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  56 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  56 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  56 , M_lda =  50  --->  Accuracy = 78.85%\n",
      "M_pca =  56 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  57 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  57 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  57 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  57 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  57 , M_lda =  6  --->  Accuracy = 65.38%\n",
      "M_pca =  57 , M_lda =  7  --->  Accuracy = 71.15%\n",
      "M_pca =  57 , M_lda =  8  --->  Accuracy = 76.92%\n",
      "M_pca =  57 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  57 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  57 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  57 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  57 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  57 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  57 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  57 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  57 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  57 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  57 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  57 , M_lda =  26  --->  Accuracy = 76.92%\n",
      "M_pca =  57 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  57 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  57 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  57 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  57 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  40  --->  Accuracy = 76.92%\n",
      "M_pca =  57 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  57 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  57 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  57 , M_lda =  46  --->  Accuracy = 75.96%\n",
      "M_pca =  57 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  57 , M_lda =  48  --->  Accuracy = 79.81%\n",
      "M_pca =  57 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  57 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  57 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  58 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  58 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  58 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  58 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  58 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  58 , M_lda =  6  --->  Accuracy = 64.42%\n",
      "M_pca =  58 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  58 , M_lda =  8  --->  Accuracy = 75.00%\n",
      "M_pca =  58 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  58 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  58 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  58 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  58 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  58 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  58 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  58 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  58 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  58 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  58 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  58 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  58 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  58 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  58 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  58 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  58 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  58 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  41  --->  Accuracy = 77.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  58 , M_lda =  42  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  43  --->  Accuracy = 77.88%\n",
      "M_pca =  58 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  58 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  58 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  58 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  58 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  58 , M_lda =  50  --->  Accuracy = 78.85%\n",
      "M_pca =  58 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  59 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  59 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  59 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  59 , M_lda =  5  --->  Accuracy = 60.58%\n",
      "M_pca =  59 , M_lda =  6  --->  Accuracy = 64.42%\n",
      "M_pca =  59 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  59 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  59 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  59 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  59 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  59 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  59 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  59 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  59 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  59 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  59 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  59 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  59 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  59 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  59 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  59 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  59 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  59 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  41  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  42  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  43  --->  Accuracy = 75.96%\n",
      "M_pca =  59 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  59 , M_lda =  46  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  59 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  59 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  59 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  60 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  60 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  60 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  60 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  60 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  60 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  60 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  60 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  60 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  60 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  60 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  60 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  60 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  60 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  60 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  60 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  60 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  60 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  60 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  60 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  60 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  60 , M_lda =  40  --->  Accuracy = 77.88%\n",
      "M_pca =  60 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  60 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  60 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  60 , M_lda =  47  --->  Accuracy = 76.92%\n",
      "M_pca =  60 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  60 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  60 , M_lda =  50  --->  Accuracy = 78.85%\n",
      "M_pca =  60 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  61 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  61 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  61 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  61 , M_lda =  4  --->  Accuracy = 50.96%\n",
      "M_pca =  61 , M_lda =  5  --->  Accuracy = 66.35%\n",
      "M_pca =  61 , M_lda =  6  --->  Accuracy = 67.31%\n",
      "M_pca =  61 , M_lda =  7  --->  Accuracy = 72.12%\n",
      "M_pca =  61 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  61 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  61 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  61 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  61 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  61 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  61 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  61 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  61 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  61 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  61 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  61 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  61 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  61 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  61 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  61 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  61 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  61 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  44  --->  Accuracy = 78.85%\n",
      "M_pca =  61 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  61 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  47  --->  Accuracy = 77.88%\n",
      "M_pca =  61 , M_lda =  48  --->  Accuracy = 79.81%\n",
      "M_pca =  61 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  61 , M_lda =  51  --->  Accuracy = 77.88%\n",
      "M_pca =  62 , M_lda =  1  --->  Accuracy = 4.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  62 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  62 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  62 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  62 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  62 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  62 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  62 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  62 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  62 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  62 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  62 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  62 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  62 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  62 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  62 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  62 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  62 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  62 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  62 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  62 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  62 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  62 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  62 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  62 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  62 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  62 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  63 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  63 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  63 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  63 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  63 , M_lda =  5  --->  Accuracy = 61.54%\n",
      "M_pca =  63 , M_lda =  6  --->  Accuracy = 66.35%\n",
      "M_pca =  63 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  63 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  63 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  63 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  63 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  63 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  63 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  63 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  63 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  63 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  63 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  63 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  63 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  63 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  63 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  63 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  63 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  63 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  63 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  63 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  63 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  63 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  64 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  64 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  64 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  64 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  64 , M_lda =  5  --->  Accuracy = 60.58%\n",
      "M_pca =  64 , M_lda =  6  --->  Accuracy = 70.19%\n",
      "M_pca =  64 , M_lda =  7  --->  Accuracy = 71.15%\n",
      "M_pca =  64 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  64 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  64 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  64 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  64 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  64 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  64 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  64 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  64 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  64 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  64 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  64 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  64 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  64 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  64 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  64 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  64 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  64 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  64 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  64 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  64 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  64 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  65 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  65 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  65 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  65 , M_lda =  5  --->  Accuracy = 58.65%\n",
      "M_pca =  65 , M_lda =  6  --->  Accuracy = 67.31%\n",
      "M_pca =  65 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  65 , M_lda =  8  --->  Accuracy = 75.00%\n",
      "M_pca =  65 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  65 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  65 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  12  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  65 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  65 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  65 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  65 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  65 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  65 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  65 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  65 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  65 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  65 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  65 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  65 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  65 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  47  --->  Accuracy = 82.69%\n",
      "M_pca =  65 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  65 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  65 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  66 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  66 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  66 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  66 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  66 , M_lda =  5  --->  Accuracy = 59.62%\n",
      "M_pca =  66 , M_lda =  6  --->  Accuracy = 66.35%\n",
      "M_pca =  66 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  66 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  66 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  66 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  66 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  66 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  66 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  66 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  66 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  66 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  66 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  66 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  66 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  66 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  66 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  66 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  66 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  66 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  66 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  66 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  66 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  66 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  66 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  66 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  66 , M_lda =  51  --->  Accuracy = 77.88%\n",
      "M_pca =  67 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  67 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  67 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  67 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  67 , M_lda =  5  --->  Accuracy = 59.62%\n",
      "M_pca =  67 , M_lda =  6  --->  Accuracy = 66.35%\n",
      "M_pca =  67 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  67 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  67 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  67 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  67 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  67 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  67 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  67 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  67 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  67 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  67 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  67 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  67 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  67 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  67 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  67 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  67 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  67 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  67 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  67 , M_lda =  43  --->  Accuracy = 79.81%\n",
      "M_pca =  67 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  67 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  67 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  67 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  67 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  67 , M_lda =  51  --->  Accuracy = 77.88%\n",
      "M_pca =  68 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  68 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  68 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  68 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  68 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  68 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  68 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  68 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  68 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  68 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  68 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  68 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  68 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  68 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  68 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  68 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  68 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  23  --->  Accuracy = 79.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  68 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  68 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  68 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  68 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  68 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  68 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  68 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  68 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  68 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  68 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  68 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  68 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  68 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  68 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  68 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  68 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  68 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  68 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  68 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  68 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  68 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  68 , M_lda =  51  --->  Accuracy = 76.92%\n",
      "M_pca =  69 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  69 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  69 , M_lda =  3  --->  Accuracy = 46.15%\n",
      "M_pca =  69 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  69 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  69 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  69 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  69 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  69 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  69 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  69 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  69 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  69 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  69 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  69 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  69 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  69 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  69 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  69 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  69 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  69 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  69 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  69 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  69 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  69 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  69 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  69 , M_lda =  48  --->  Accuracy = 79.81%\n",
      "M_pca =  69 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  69 , M_lda =  50  --->  Accuracy = 78.85%\n",
      "M_pca =  69 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  70 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  70 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  70 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  70 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  70 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  70 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  70 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  70 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  70 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  70 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  70 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  70 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  70 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  70 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  70 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  70 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  70 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  70 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  70 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  70 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  70 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  70 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  70 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  70 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  70 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  70 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  70 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  70 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  70 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  70 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  70 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  70 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  70 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  71 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  71 , M_lda =  3  --->  Accuracy = 39.42%\n",
      "M_pca =  71 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  71 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  71 , M_lda =  6  --->  Accuracy = 66.35%\n",
      "M_pca =  71 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  71 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  71 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  71 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  71 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  71 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  71 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  71 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  71 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  71 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  71 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  71 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  71 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  71 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  71 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  34  --->  Accuracy = 85.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  71 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  71 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  71 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  71 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  71 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  71 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  72 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  72 , M_lda =  3  --->  Accuracy = 39.42%\n",
      "M_pca =  72 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  72 , M_lda =  5  --->  Accuracy = 56.73%\n",
      "M_pca =  72 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  72 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  72 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  72 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  72 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  72 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  72 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  72 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  72 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  72 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  72 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  72 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  72 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  72 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  72 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  72 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  72 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  72 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  72 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  72 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  72 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  72 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  72 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  72 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  73 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  73 , M_lda =  3  --->  Accuracy = 41.35%\n",
      "M_pca =  73 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  73 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  73 , M_lda =  6  --->  Accuracy = 65.38%\n",
      "M_pca =  73 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  73 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  73 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  73 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  73 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  73 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  73 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  73 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  73 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  73 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  73 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  73 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  73 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  73 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  73 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  73 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  73 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  73 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  73 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  73 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  74 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  74 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  74 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  74 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  74 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  74 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  74 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  74 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  74 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  74 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  74 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  74 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  74 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  74 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  74 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  74 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  74 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  74 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  74 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  74 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  74 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  74 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  74 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  74 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  74 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  74 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  74 , M_lda =  43  --->  Accuracy = 82.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  74 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  74 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  74 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  74 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  74 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  74 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  74 , M_lda =  50  --->  Accuracy = 78.85%\n",
      "M_pca =  74 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  75 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  75 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  75 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  75 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  75 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  75 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  75 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  75 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  75 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  75 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  75 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  75 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  75 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  75 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  75 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  75 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  75 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  75 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  75 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  75 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  75 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  75 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  75 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  75 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  75 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  75 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  75 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  75 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  75 , M_lda =  32  --->  Accuracy = 77.88%\n",
      "M_pca =  75 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  75 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  75 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  75 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  75 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  75 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  75 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  75 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  75 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  75 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  75 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  75 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  75 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  75 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  75 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  75 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  76 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  76 , M_lda =  3  --->  Accuracy = 39.42%\n",
      "M_pca =  76 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  76 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  76 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  76 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  76 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  76 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  76 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  76 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  76 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  76 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  76 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  76 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  76 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  76 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  76 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  76 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  76 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  76 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  76 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  76 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  76 , M_lda =  49  --->  Accuracy = 77.88%\n",
      "M_pca =  76 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  76 , M_lda =  51  --->  Accuracy = 75.96%\n",
      "M_pca =  77 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  77 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  77 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  77 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  77 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  77 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  77 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  77 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  77 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  77 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  77 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  77 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  77 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  77 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  77 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  77 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  77 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  77 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  77 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  77 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  77 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  77 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  77 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  44  --->  Accuracy = 76.92%\n",
      "M_pca =  77 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  77 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  77 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  77 , M_lda =  48  --->  Accuracy = 76.92%\n",
      "M_pca =  77 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  77 , M_lda =  50  --->  Accuracy = 77.88%\n",
      "M_pca =  77 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  78 , M_lda =  1  --->  Accuracy = 6.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  78 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  78 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  78 , M_lda =  4  --->  Accuracy = 52.88%\n",
      "M_pca =  78 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  78 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  78 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  78 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  78 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  78 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  78 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  78 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  78 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  78 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  78 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  78 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  78 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  78 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  78 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  78 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  78 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  78 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  78 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  78 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  78 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  78 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  78 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  78 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  78 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  78 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  78 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  78 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  78 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  79 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  79 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  79 , M_lda =  3  --->  Accuracy = 38.46%\n",
      "M_pca =  79 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  79 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  79 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  79 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  79 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  79 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  79 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  79 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  79 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  79 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  79 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  79 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  79 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  79 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  79 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  79 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  79 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  79 , M_lda =  43  --->  Accuracy = 79.81%\n",
      "M_pca =  79 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  79 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  79 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  79 , M_lda =  48  --->  Accuracy = 77.88%\n",
      "M_pca =  79 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  79 , M_lda =  50  --->  Accuracy = 76.92%\n",
      "M_pca =  79 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  80 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  80 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  80 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  80 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  80 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  80 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  80 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  80 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  80 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  80 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  80 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  80 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  80 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  80 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  80 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  80 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  80 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  80 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  80 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  80 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  80 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  80 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  80 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  80 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  80 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  80 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  80 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  80 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  80 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  80 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  80 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  80 , M_lda =  44  --->  Accuracy = 78.85%\n",
      "M_pca =  80 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  80 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  80 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  80 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  80 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  80 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  80 , M_lda =  51  --->  Accuracy = 78.85%\n",
      "M_pca =  81 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  81 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  81 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  81 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  81 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  81 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  81 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  81 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  81 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  81 , M_lda =  10  --->  Accuracy = 75.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  81 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  81 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  81 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  81 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  81 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  81 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  81 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  81 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  81 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  81 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  81 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  81 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  81 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  81 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  81 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  81 , M_lda =  42  --->  Accuracy = 79.81%\n",
      "M_pca =  81 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  81 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  81 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  81 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  81 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  81 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  81 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  82 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  82 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  82 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  82 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  82 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  82 , M_lda =  6  --->  Accuracy = 64.42%\n",
      "M_pca =  82 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  82 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  82 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  82 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  82 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  82 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  82 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  82 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  82 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  82 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  82 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  82 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  82 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  82 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  82 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  82 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  82 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  82 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  82 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  82 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  45  --->  Accuracy = 78.85%\n",
      "M_pca =  82 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  82 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  82 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  82 , M_lda =  51  --->  Accuracy = 77.88%\n",
      "M_pca =  83 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  83 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  83 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  83 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  83 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  83 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  83 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  83 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  83 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  83 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  83 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  83 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  83 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  83 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  83 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  83 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  83 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  83 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  83 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  83 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  83 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  83 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  83 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  83 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  83 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  83 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  83 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  83 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  84 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  84 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  84 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  84 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  84 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  84 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  84 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  84 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  84 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  84 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  84 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  84 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  84 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  84 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  84 , M_lda =  19  --->  Accuracy = 84.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  84 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  84 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  84 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  84 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  84 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  84 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  84 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  84 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  84 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  84 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  84 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  84 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  84 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  84 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  84 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  84 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  84 , M_lda =  45  --->  Accuracy = 77.88%\n",
      "M_pca =  84 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  84 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  84 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  84 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  84 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  84 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  85 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  85 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  85 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  85 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  85 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  85 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  85 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  85 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  85 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  85 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  85 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  85 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  85 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  85 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  85 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  85 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  85 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  85 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  85 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  85 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  85 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  85 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  85 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  85 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  85 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  85 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  85 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  85 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  85 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  85 , M_lda =  43  --->  Accuracy = 79.81%\n",
      "M_pca =  85 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  85 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  85 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  85 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  85 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  86 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  86 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  86 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  86 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  86 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  86 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  86 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  86 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  86 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  86 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  86 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  86 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  86 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  86 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  86 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  86 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  86 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  86 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  86 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  86 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  86 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  86 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  86 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  86 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  86 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  86 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  86 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  86 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  86 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  86 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  86 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  86 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  86 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  86 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  86 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  86 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  86 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  86 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  86 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  86 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  86 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  86 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  87 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  87 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  87 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  87 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  87 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  87 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  87 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  87 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  87 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  87 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  87 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  87 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  87 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  87 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  87 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  87 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  87 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  87 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  87 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  87 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  87 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  87 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  28  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  87 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  87 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  87 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  87 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  87 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  87 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  87 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  87 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  87 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  87 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  87 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  87 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  87 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  87 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  87 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  87 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  88 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  88 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  88 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  88 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  88 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  88 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  88 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  88 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  88 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  88 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  88 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  88 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  88 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  88 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  88 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  88 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  88 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  88 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  88 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  88 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  88 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  88 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  88 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  88 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  88 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  88 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  88 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  88 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  88 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  88 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  88 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  88 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  88 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  89 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  89 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  89 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  89 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  89 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  89 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  89 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  89 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  89 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  89 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  89 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  89 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  89 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  89 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  89 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  89 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  89 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  89 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  89 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  89 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  89 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  89 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  89 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  89 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  89 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  89 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  89 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  89 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  89 , M_lda =  46  --->  Accuracy = 78.85%\n",
      "M_pca =  89 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  89 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  89 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  89 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  90 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  90 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  90 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  90 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  90 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  90 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  90 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  90 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  90 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  90 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  90 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  90 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  90 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  90 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  90 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  90 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  90 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  90 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  90 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  37  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  90 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  90 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  90 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  90 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  90 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  90 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  90 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  90 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  90 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  90 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  90 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  91 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  91 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  91 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  91 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  91 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  91 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  91 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  91 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  91 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  91 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  91 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  91 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  91 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  91 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  91 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  91 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  91 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  91 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  91 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  91 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  91 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  91 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  91 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  91 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  91 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  91 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  91 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  92 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  92 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  92 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  92 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  92 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  92 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  92 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  92 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  92 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  92 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  92 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  92 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  92 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  92 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  92 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  92 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  92 , M_lda =  18  --->  Accuracy = 89.42%\n",
      "M_pca =  92 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  92 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  92 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  92 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  92 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  92 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  92 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  92 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  92 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  92 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  92 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  92 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  92 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  92 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  92 , M_lda =  47  --->  Accuracy = 79.81%\n",
      "M_pca =  92 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  92 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  92 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  92 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  93 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  93 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  93 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  93 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  93 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  93 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  93 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  93 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  93 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  93 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  93 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  93 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  93 , M_lda =  13  --->  Accuracy = 87.50%\n",
      "M_pca =  93 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  93 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  93 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  93 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  93 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  93 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  93 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  93 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  93 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  93 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  93 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  93 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  93 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  93 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  93 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  93 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  93 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  93 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  93 , M_lda =  47  --->  Accuracy = 81.73%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  93 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  93 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  93 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  93 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  94 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  94 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  94 , M_lda =  3  --->  Accuracy = 41.35%\n",
      "M_pca =  94 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  94 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  94 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  94 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  94 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  94 , M_lda =  9  --->  Accuracy = 78.85%\n",
      "M_pca =  94 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  94 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  94 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  94 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  94 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  94 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  94 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  94 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  94 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  94 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  94 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  94 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  94 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  94 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  94 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  94 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  94 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  94 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  94 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  94 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  94 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  94 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  94 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  94 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  94 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  94 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  95 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  95 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  95 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  95 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  95 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  95 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  95 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  95 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  95 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  95 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  95 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  95 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  95 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  95 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  95 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  95 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  95 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  95 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  95 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  95 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  95 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  95 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  95 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  95 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  95 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  95 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  95 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  95 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  95 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  96 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  96 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  96 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  96 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  96 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  96 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  96 , M_lda =  8  --->  Accuracy = 75.00%\n",
      "M_pca =  96 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  96 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  96 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  96 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  96 , M_lda =  13  --->  Accuracy = 87.50%\n",
      "M_pca =  96 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  96 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  96 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  96 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  96 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  96 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  96 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  96 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  96 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  96 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  96 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  96 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  96 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  96 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  96 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  96 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  97 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  97 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  97 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  97 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  97 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  97 , M_lda =  6  --->  Accuracy = 65.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  97 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  97 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  97 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  97 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  97 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  97 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  97 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  97 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  97 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  97 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  97 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  97 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  97 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  97 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  97 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  97 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  97 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  97 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  97 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  97 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  97 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  97 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  97 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  97 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  97 , M_lda =  48  --->  Accuracy = 79.81%\n",
      "M_pca =  97 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  97 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  97 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  98 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  98 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  98 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  98 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  98 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  98 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  98 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  98 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  98 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  98 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  98 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  98 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  98 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  98 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  98 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  98 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  98 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  98 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  98 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  98 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  98 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  98 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  98 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  98 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  98 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  98 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  98 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  98 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  98 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  98 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  99 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  99 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  99 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  99 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  99 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  99 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  99 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  99 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  99 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  99 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  99 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  99 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  99 , M_lda =  13  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  99 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  99 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  99 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  99 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  99 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  99 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  99 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  99 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  99 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  99 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  99 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  99 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  99 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  99 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  99 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  99 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  99 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  99 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  99 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  99 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  99 , M_lda =  51  --->  Accuracy = 79.81%\n",
      "M_pca =  100 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  100 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  100 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  100 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  100 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  100 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  100 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  100 , M_lda =  8  --->  Accuracy = 74.04%\n",
      "M_pca =  100 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  100 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  100 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  100 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  100 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  14  --->  Accuracy = 88.46%\n",
      "M_pca =  100 , M_lda =  15  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  100 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  100 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  100 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  100 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  100 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  100 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  100 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  100 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  100 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  100 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  100 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  100 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  100 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  100 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  100 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  101 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  101 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  101 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  101 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  101 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  101 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  101 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  101 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  101 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  101 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  101 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  101 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  101 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  101 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  101 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  101 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  101 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  101 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  101 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  101 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  101 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  101 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  101 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  101 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  101 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  101 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  101 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  101 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  101 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  101 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  101 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  101 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  101 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  101 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  101 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  102 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  102 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  102 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  102 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  102 , M_lda =  6  --->  Accuracy = 63.46%\n",
      "M_pca =  102 , M_lda =  7  --->  Accuracy = 72.12%\n",
      "M_pca =  102 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  102 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  102 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  102 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  102 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  102 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  102 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  102 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  102 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  102 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  102 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  102 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  102 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  102 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  102 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  102 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  102 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  102 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  102 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  102 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  102 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  102 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  103 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  103 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  103 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  103 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  103 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  103 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  103 , M_lda =  7  --->  Accuracy = 71.15%\n",
      "M_pca =  103 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  103 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  103 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  103 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  103 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  103 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  22  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  103 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  103 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  103 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  103 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  103 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  103 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  103 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  103 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  103 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  103 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  103 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  103 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  103 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  104 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  104 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  104 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  104 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  104 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  104 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  104 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  104 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  104 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  104 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  104 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  12  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  104 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  104 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  104 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  104 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  104 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  104 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  104 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  104 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  104 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  104 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  104 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  104 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  105 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  105 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  105 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  105 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  105 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  105 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  105 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  105 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  105 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  105 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  105 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  13  --->  Accuracy = 87.50%\n",
      "M_pca =  105 , M_lda =  14  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  105 , M_lda =  19  --->  Accuracy = 89.42%\n",
      "M_pca =  105 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  105 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  105 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  24  --->  Accuracy = 90.38%\n",
      "M_pca =  105 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  105 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  105 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  105 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  105 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  105 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  105 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  105 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  105 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  105 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  105 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  105 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  105 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  105 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  105 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  106 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  106 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  106 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  106 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  106 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  106 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  106 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  106 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  106 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  106 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  106 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  106 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  106 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  106 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  16  --->  Accuracy = 91.35%\n",
      "M_pca =  106 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  106 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  106 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  106 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  22  --->  Accuracy = 90.38%\n",
      "M_pca =  106 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  24  --->  Accuracy = 90.38%\n",
      "M_pca =  106 , M_lda =  25  --->  Accuracy = 92.31%\n",
      "M_pca =  106 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  27  --->  Accuracy = 91.35%\n",
      "M_pca =  106 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  106 , M_lda =  29  --->  Accuracy = 90.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  106 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  106 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  106 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  106 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  106 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  106 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  106 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  106 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  106 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  106 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  106 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  106 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  106 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  106 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  106 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  107 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  107 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  107 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  107 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  107 , M_lda =  5  --->  Accuracy = 56.73%\n",
      "M_pca =  107 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  107 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  107 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  107 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  107 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  107 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  107 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  107 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  107 , M_lda =  15  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  107 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  107 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  107 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  107 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  107 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  26  --->  Accuracy = 91.35%\n",
      "M_pca =  107 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  107 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  107 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  107 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  107 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  107 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  107 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  107 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  107 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  107 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  107 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  107 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  107 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  107 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  107 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  107 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  107 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  107 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  108 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  108 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  108 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  108 , M_lda =  5  --->  Accuracy = 56.73%\n",
      "M_pca =  108 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  108 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  108 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  108 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  108 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  108 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  108 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  13  --->  Accuracy = 87.50%\n",
      "M_pca =  108 , M_lda =  14  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  15  --->  Accuracy = 91.35%\n",
      "M_pca =  108 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  108 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  108 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  108 , M_lda =  24  --->  Accuracy = 91.35%\n",
      "M_pca =  108 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  108 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  108 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  108 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  108 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  108 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  108 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  108 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  108 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  108 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  108 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  108 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  108 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  108 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  108 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  108 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  108 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  108 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  108 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  109 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  109 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  109 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  109 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  109 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  109 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  109 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  109 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  109 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  109 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  109 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  109 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  109 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  109 , M_lda =  14  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  15  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  16  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  109 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  19  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  109 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  109 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  109 , M_lda =  36  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  109 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  109 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  109 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  109 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  109 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  109 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  109 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  109 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  109 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  110 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  110 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  110 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  110 , M_lda =  5  --->  Accuracy = 55.77%\n",
      "M_pca =  110 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  110 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  110 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  110 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  110 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  110 , M_lda =  11  --->  Accuracy = 85.58%\n",
      "M_pca =  110 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  110 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  110 , M_lda =  14  --->  Accuracy = 89.42%\n",
      "M_pca =  110 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  110 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  110 , M_lda =  21  --->  Accuracy = 90.38%\n",
      "M_pca =  110 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  110 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  110 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  110 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  110 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  110 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  110 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  110 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  110 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  110 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  110 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  110 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  111 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  111 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  111 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  111 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  111 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  111 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  111 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  111 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  111 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  111 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  111 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  111 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  111 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  111 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  111 , M_lda =  15  --->  Accuracy = 90.38%\n",
      "M_pca =  111 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  111 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  111 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  111 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  111 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  111 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  111 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  111 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  111 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  111 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  111 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  111 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  111 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  111 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  111 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  111 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  111 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  112 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  112 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  112 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  112 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  112 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  112 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  112 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  112 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  112 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  112 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  112 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  112 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  112 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  14  --->  Accuracy = 90.38%\n",
      "M_pca =  112 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  17  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  112 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  112 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  112 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  112 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  112 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  43  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  112 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  112 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  112 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  112 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  112 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  113 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  113 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  113 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  113 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  113 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  113 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  113 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  113 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  113 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  113 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  113 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  113 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  113 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  17  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  113 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  113 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  113 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  113 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  113 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  113 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  114 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  114 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  114 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  114 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  114 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  114 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  114 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  114 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  114 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  114 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  114 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  114 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  114 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  14  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  15  --->  Accuracy = 90.38%\n",
      "M_pca =  114 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  114 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  114 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  114 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  114 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  114 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  114 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  114 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  114 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  114 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  114 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  114 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  114 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  114 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  114 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  114 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  114 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  114 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  115 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  115 , M_lda =  3  --->  Accuracy = 38.46%\n",
      "M_pca =  115 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  115 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  115 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  115 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  115 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  115 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  115 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  115 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  115 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  115 , M_lda =  13  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  115 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  115 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  115 , M_lda =  26  --->  Accuracy = 91.35%\n",
      "M_pca =  115 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  115 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  115 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  115 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  115 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  115 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  115 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  115 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  115 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  115 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  115 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  115 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  115 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  115 , M_lda =  50  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  115 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  116 , M_lda =  2  --->  Accuracy = 26.92%\n",
      "M_pca =  116 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  116 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  116 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  116 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  116 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  116 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  116 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  116 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  116 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  116 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  116 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  116 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  116 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  116 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  116 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  116 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  116 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  116 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  116 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  116 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  116 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  116 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  116 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  116 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  116 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  116 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  116 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  116 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  116 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  116 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  116 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  117 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  117 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  117 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  117 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  117 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  117 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  117 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  117 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  117 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  117 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  117 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  117 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  117 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  117 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  17  --->  Accuracy = 89.42%\n",
      "M_pca =  117 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  117 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  117 , M_lda =  27  --->  Accuracy = 91.35%\n",
      "M_pca =  117 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  117 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  117 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  117 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  117 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  117 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  117 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  117 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  117 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  117 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  117 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  118 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  118 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  118 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  118 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  118 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  118 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  118 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  118 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  118 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  118 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  118 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  118 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  16  --->  Accuracy = 89.42%\n",
      "M_pca =  118 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  118 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  118 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  118 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  118 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  118 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  118 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  118 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  118 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  118 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  118 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  118 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  118 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  118 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  119 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  119 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  119 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  119 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  119 , M_lda =  6  --->  Accuracy = 54.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  119 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  119 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  119 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  119 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  119 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  119 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  119 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  119 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  119 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  119 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  119 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  119 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  21  --->  Accuracy = 90.38%\n",
      "M_pca =  119 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  119 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  119 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  119 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  119 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  119 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  119 , M_lda =  32  --->  Accuracy = 92.31%\n",
      "M_pca =  119 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  119 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  119 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  119 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  119 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  119 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  119 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  119 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  119 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  120 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  120 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  120 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  120 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  120 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  120 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  120 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  120 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  120 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  120 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  120 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  120 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  120 , M_lda =  13  --->  Accuracy = 88.46%\n",
      "M_pca =  120 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  120 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  120 , M_lda =  20  --->  Accuracy = 90.38%\n",
      "M_pca =  120 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  26  --->  Accuracy = 91.35%\n",
      "M_pca =  120 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  120 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  120 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  120 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  120 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  120 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  120 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  120 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  120 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  120 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  120 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  120 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  120 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  120 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  1  --->  Accuracy = 13.46%\n",
      "M_pca =  121 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  121 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  121 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  121 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  121 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  121 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  121 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  121 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  121 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  121 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  121 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  121 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  121 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  121 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  121 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  121 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  121 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  121 , M_lda =  20  --->  Accuracy = 91.35%\n",
      "M_pca =  121 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  121 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  121 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  121 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  121 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  121 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  121 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  121 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  121 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  121 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  121 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  121 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  121 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  121 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  121 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  121 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  121 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  121 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  121 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  121 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  121 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  121 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  122 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  122 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  122 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  122 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  122 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  122 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  122 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  122 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  122 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  122 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  122 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  122 , M_lda =  13  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  122 , M_lda =  14  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  122 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  122 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  122 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  122 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  122 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  122 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  122 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  122 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  122 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  122 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  122 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  122 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  122 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  122 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  122 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  122 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  122 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  122 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  122 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  123 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  123 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  123 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  123 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  123 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  123 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  123 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  123 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  123 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  123 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  123 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  123 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  123 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  123 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  123 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  123 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  123 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  123 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  30  --->  Accuracy = 93.27%\n",
      "M_pca =  123 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  123 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  123 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  123 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  123 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  123 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  123 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  123 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  123 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  123 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  123 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  124 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  124 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  124 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  124 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  124 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  124 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  124 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  124 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  124 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  124 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  124 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  124 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  124 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  124 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  124 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  124 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  124 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  124 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  26  --->  Accuracy = 91.35%\n",
      "M_pca =  124 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  124 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  124 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  124 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  124 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  124 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  124 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  124 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  124 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  124 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  124 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  125 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  125 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  125 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  125 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  125 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  125 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  125 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  125 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  125 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  125 , M_lda =  10  --->  Accuracy = 81.73%\n",
      "M_pca =  125 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  125 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  125 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  125 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  125 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  20  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  125 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  125 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  125 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  125 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  125 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  125 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  125 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  125 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  125 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  125 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  125 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  125 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  126 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  126 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  126 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  126 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  126 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  126 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  126 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  126 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  126 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  126 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  126 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  126 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  126 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  126 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  126 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  126 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  126 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  126 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  32  --->  Accuracy = 92.31%\n",
      "M_pca =  126 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  126 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  126 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  126 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  126 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  126 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  126 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  126 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  126 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  126 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  127 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  127 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  127 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  127 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  127 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  127 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  127 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  127 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  127 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  127 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  127 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  127 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  127 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  127 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  127 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  127 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  127 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  127 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  127 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  127 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  127 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  127 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  127 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  127 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  127 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  127 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  127 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  127 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  127 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  127 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  127 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  127 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  127 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  128 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  128 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  128 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  128 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  128 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  128 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  128 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  128 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  128 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  128 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  128 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  128 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  128 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  128 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  128 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  128 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  22  --->  Accuracy = 90.38%\n",
      "M_pca =  128 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  27  --->  Accuracy = 90.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  128 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  128 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  128 , M_lda =  30  --->  Accuracy = 92.31%\n",
      "M_pca =  128 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  128 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  128 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  128 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  128 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  128 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  128 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  128 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  128 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  128 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  128 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  128 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  128 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  128 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  129 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  129 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  129 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  129 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  129 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  129 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  129 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  129 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  129 , M_lda =  10  --->  Accuracy = 82.69%\n",
      "M_pca =  129 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  129 , M_lda =  12  --->  Accuracy = 85.58%\n",
      "M_pca =  129 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  129 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  129 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  129 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  129 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  129 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  129 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  129 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  129 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  129 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  129 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  129 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  130 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  130 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  130 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  130 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  130 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  130 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  130 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  130 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  130 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  130 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  130 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  130 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  130 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  130 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  130 , M_lda =  16  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  130 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  130 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  130 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  130 , M_lda =  21  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  130 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  130 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  130 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  130 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  130 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  130 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  130 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  130 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  130 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  130 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  130 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  130 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  130 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  130 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  130 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  130 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  131 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  131 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  131 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  131 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  131 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  131 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  131 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  131 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  131 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  131 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  131 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  131 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  131 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  18  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  131 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  131 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  131 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  131 , M_lda =  27  --->  Accuracy = 90.38%\n",
      "M_pca =  131 , M_lda =  28  --->  Accuracy = 91.35%\n",
      "M_pca =  131 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  131 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  131 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  34  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  131 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  131 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  131 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  131 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  131 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  131 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  131 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  131 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  131 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  132 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  132 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  132 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  132 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  132 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  132 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  132 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  132 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  132 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  132 , M_lda =  11  --->  Accuracy = 84.62%\n",
      "M_pca =  132 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  132 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  132 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  132 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  132 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  132 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  132 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  132 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  132 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  132 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  132 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  132 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  132 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  132 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  132 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  132 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  132 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  133 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  133 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  133 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  133 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  133 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  133 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  133 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  133 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  133 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  133 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  133 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  133 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  133 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  133 , M_lda =  15  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  16  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  133 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  133 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  133 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  133 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  133 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  133 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  133 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  133 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  133 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  133 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  133 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  133 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  133 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  133 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  133 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  133 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  133 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  133 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  133 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  133 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  134 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  134 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  134 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  134 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  134 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  134 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  134 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  134 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  134 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  134 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  134 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  134 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  134 , M_lda =  14  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  134 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  134 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  134 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  134 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  134 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  134 , M_lda =  39  --->  Accuracy = 92.31%\n",
      "M_pca =  134 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  41  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  134 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  134 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  134 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  134 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  134 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  135 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  135 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  135 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  135 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  135 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  135 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  135 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  135 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  135 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  135 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  135 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  135 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  135 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  135 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  135 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  135 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  135 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  135 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  135 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  135 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  135 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  135 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  135 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  135 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  135 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  135 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  135 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  136 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  136 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  136 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  136 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  136 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  136 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  136 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  136 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  136 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  136 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  136 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  136 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  136 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  136 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  136 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  136 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  17  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  136 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  136 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  136 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  136 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  136 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  136 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  34  --->  Accuracy = 91.35%\n",
      "M_pca =  136 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  136 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  136 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  136 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  136 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  136 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  137 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  137 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  137 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  137 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  137 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  137 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  137 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  137 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  137 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  137 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  137 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  137 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  137 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  137 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  137 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  137 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  137 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  137 , M_lda =  19  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  137 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  137 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  137 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  137 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  137 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  137 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  137 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  137 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  137 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  137 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  37  --->  Accuracy = 91.35%\n",
      "M_pca =  137 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  137 , M_lda =  40  --->  Accuracy = 91.35%\n",
      "M_pca =  137 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  137 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  137 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  137 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  137 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  137 , M_lda =  48  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  137 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  137 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  137 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  138 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  138 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  138 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  138 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  138 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  138 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  138 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  138 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  138 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  138 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  138 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  138 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  138 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  138 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  138 , M_lda =  16  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  138 , M_lda =  18  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  138 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  138 , M_lda =  22  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  138 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  25  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  138 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  138 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  138 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  40  --->  Accuracy = 91.35%\n",
      "M_pca =  138 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  138 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  138 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  138 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  138 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  138 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  138 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  139 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  139 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  139 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  139 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  139 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  139 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  139 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  139 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  139 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  139 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  139 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  139 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  139 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  139 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  139 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  139 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  139 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  139 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  139 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  139 , M_lda =  21  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  139 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  139 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  139 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  139 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  139 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  139 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  35  --->  Accuracy = 92.31%\n",
      "M_pca =  139 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  37  --->  Accuracy = 91.35%\n",
      "M_pca =  139 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  139 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  139 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  139 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  139 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  139 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  139 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  140 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  140 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  140 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  140 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  140 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  140 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  140 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  140 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  140 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  140 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  140 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  140 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  140 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  140 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  140 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  140 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  140 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  140 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  140 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  140 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  140 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  140 , M_lda =  25  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  140 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  140 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  140 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  140 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  140 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  39  --->  Accuracy = 92.31%\n",
      "M_pca =  140 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  140 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  140 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  140 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  140 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  140 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  141 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  141 , M_lda =  3  --->  Accuracy = 25.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  141 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  141 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  141 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  141 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  141 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  141 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  141 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  141 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  141 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  141 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  141 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  141 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  141 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  141 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  141 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  141 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  141 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  141 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  141 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  141 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  141 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  141 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  141 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  35  --->  Accuracy = 93.27%\n",
      "M_pca =  141 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  141 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  141 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  141 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  141 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  141 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  141 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  141 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  141 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  141 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  142 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  142 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  142 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  142 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  142 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  142 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  142 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  142 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  142 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  142 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  142 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  142 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  142 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  142 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  142 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  142 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  142 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  142 , M_lda =  19  --->  Accuracy = 88.46%\n",
      "M_pca =  142 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  142 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  142 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  142 , M_lda =  23  --->  Accuracy = 90.38%\n",
      "M_pca =  142 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  142 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  142 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  142 , M_lda =  27  --->  Accuracy = 92.31%\n",
      "M_pca =  142 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  142 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  142 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  142 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  142 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  142 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  37  --->  Accuracy = 91.35%\n",
      "M_pca =  142 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  142 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  142 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  142 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  142 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  142 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  142 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  142 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  142 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  142 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  142 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  142 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  143 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  143 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  143 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  143 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  143 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  143 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  143 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  143 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  143 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  143 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  143 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  143 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  143 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  143 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  143 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  143 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  143 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  143 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  143 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  143 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  143 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  143 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  143 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  143 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  143 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  143 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  143 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  143 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  143 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  143 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  143 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  143 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  144 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  144 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  144 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  144 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  144 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  144 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  144 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  144 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  144 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  144 , M_lda =  10  --->  Accuracy = 75.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  144 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  144 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  144 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  144 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  15  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  18  --->  Accuracy = 88.46%\n",
      "M_pca =  144 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  144 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  144 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  144 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  144 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  144 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  144 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  144 , M_lda =  38  --->  Accuracy = 92.31%\n",
      "M_pca =  144 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  144 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  144 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  144 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  144 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  144 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  144 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  144 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  144 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  144 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  144 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  145 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  145 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  145 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  145 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  145 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  145 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  145 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  145 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  145 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  145 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  145 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  145 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  145 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  145 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  145 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  145 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  145 , M_lda =  18  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  145 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  145 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  145 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  145 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  145 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  145 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  27  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  145 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  145 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  145 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  145 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  145 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  145 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  145 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  146 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  146 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  146 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  146 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  146 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  146 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  146 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  146 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  146 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  146 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  146 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  146 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  146 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  146 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  146 , M_lda =  16  --->  Accuracy = 87.50%\n",
      "M_pca =  146 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  146 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  146 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  146 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  146 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  146 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  146 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  146 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  146 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  146 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  146 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  146 , M_lda =  28  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  146 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  146 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  34  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  146 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  146 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  146 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  146 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  146 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  146 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  146 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  146 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  146 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  146 , M_lda =  50  --->  Accuracy = 93.27%\n",
      "M_pca =  146 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  147 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  147 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  147 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  147 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  147 , M_lda =  6  --->  Accuracy = 64.42%\n",
      "M_pca =  147 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  147 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  147 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  147 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  147 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  147 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  147 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  147 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  147 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  147 , M_lda =  16  --->  Accuracy = 85.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  147 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  147 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  147 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  147 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  147 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  147 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  147 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  147 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  147 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  147 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  147 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  147 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  147 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  147 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  147 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  147 , M_lda =  48  --->  Accuracy = 93.27%\n",
      "M_pca =  147 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  147 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  148 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  148 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  148 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  148 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  148 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  148 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  148 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  148 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  148 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  148 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  148 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  148 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  148 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  148 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  148 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  148 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  148 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  148 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  148 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  148 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  148 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  148 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  148 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  148 , M_lda =  25  --->  Accuracy = 90.38%\n",
      "M_pca =  148 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  148 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  148 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  148 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  148 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  148 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  148 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  148 , M_lda =  33  --->  Accuracy = 91.35%\n",
      "M_pca =  148 , M_lda =  34  --->  Accuracy = 91.35%\n",
      "M_pca =  148 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  148 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  148 , M_lda =  37  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  148 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  148 , M_lda =  40  --->  Accuracy = 93.27%\n",
      "M_pca =  148 , M_lda =  41  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  148 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  148 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  46  --->  Accuracy = 93.27%\n",
      "M_pca =  148 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  48  --->  Accuracy = 92.31%\n",
      "M_pca =  148 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  148 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  148 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  149 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  149 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  149 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  149 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  149 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  149 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  149 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  149 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  149 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  149 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  149 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  149 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  149 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  149 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  149 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  149 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  149 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  149 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  149 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  149 , M_lda =  20  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  149 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  149 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  149 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  149 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  149 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  149 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  149 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  149 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  149 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  149 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  149 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  149 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  149 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  149 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  149 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  149 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  149 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  149 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  149 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  149 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  149 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  149 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  149 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  149 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  149 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  150 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  150 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  150 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  150 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  150 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  150 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  150 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  150 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  150 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  150 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  150 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  150 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  150 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  150 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  150 , M_lda =  15  --->  Accuracy = 86.54%\n",
      "M_pca =  150 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  150 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  150 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  150 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  150 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  150 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  150 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  150 , M_lda =  23  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  150 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  150 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  150 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  150 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  150 , M_lda =  31  --->  Accuracy = 92.31%\n",
      "M_pca =  150 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  150 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  150 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  150 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  150 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  150 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  150 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  150 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  150 , M_lda =  43  --->  Accuracy = 93.27%\n",
      "M_pca =  150 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  150 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  150 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  150 , M_lda =  47  --->  Accuracy = 94.23%\n",
      "M_pca =  150 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  150 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  150 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  150 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  151 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  151 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  151 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  151 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  151 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  151 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  151 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  151 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  151 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  151 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  151 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  151 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  151 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  151 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  151 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  151 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  151 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  151 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  151 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  151 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  151 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  151 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  151 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  151 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  151 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  151 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  151 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  151 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  151 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  151 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  151 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  151 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  35  --->  Accuracy = 92.31%\n",
      "M_pca =  151 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  151 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  151 , M_lda =  38  --->  Accuracy = 92.31%\n",
      "M_pca =  151 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  41  --->  Accuracy = 93.27%\n",
      "M_pca =  151 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  151 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  151 , M_lda =  44  --->  Accuracy = 93.27%\n",
      "M_pca =  151 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  151 , M_lda =  46  --->  Accuracy = 94.23%\n",
      "M_pca =  151 , M_lda =  47  --->  Accuracy = 93.27%\n",
      "M_pca =  151 , M_lda =  48  --->  Accuracy = 94.23%\n",
      "M_pca =  151 , M_lda =  49  --->  Accuracy = 93.27%\n",
      "M_pca =  151 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  151 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  152 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  152 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  152 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  152 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  152 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  152 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  152 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  152 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  152 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  152 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  152 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  152 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  152 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  152 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  152 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  152 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  152 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  152 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  152 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  152 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  152 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  152 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  152 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  152 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  152 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  31  --->  Accuracy = 92.31%\n",
      "M_pca =  152 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  152 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  152 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  38  --->  Accuracy = 92.31%\n",
      "M_pca =  152 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  152 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  152 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  152 , M_lda =  45  --->  Accuracy = 93.27%\n",
      "M_pca =  152 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  152 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  152 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  152 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  153 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  153 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  153 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  153 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  153 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  153 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  153 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  153 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  153 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  153 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  153 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  153 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  153 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  153 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  153 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  153 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  153 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  153 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  153 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  153 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  153 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  153 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  153 , M_lda =  28  --->  Accuracy = 91.35%\n",
      "M_pca =  153 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  153 , M_lda =  30  --->  Accuracy = 90.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  153 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  153 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  153 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  153 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  153 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  153 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  153 , M_lda =  37  --->  Accuracy = 92.31%\n",
      "M_pca =  153 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  153 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  153 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  153 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  153 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  153 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  153 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  153 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  153 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  153 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  153 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  153 , M_lda =  49  --->  Accuracy = 93.27%\n",
      "M_pca =  153 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  153 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  154 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  154 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  154 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  154 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  154 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  154 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  154 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  154 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  154 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  154 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  154 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  154 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  154 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  154 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  154 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  154 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  154 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  154 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  154 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  154 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  154 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  154 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  154 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  154 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  154 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  154 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  154 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  154 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  154 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  31  --->  Accuracy = 93.27%\n",
      "M_pca =  154 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  154 , M_lda =  33  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  34  --->  Accuracy = 92.31%\n",
      "M_pca =  154 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  154 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  154 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  154 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  154 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  154 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  48  --->  Accuracy = 94.23%\n",
      "M_pca =  154 , M_lda =  49  --->  Accuracy = 93.27%\n",
      "M_pca =  154 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  154 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  155 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  155 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  155 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  155 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  155 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  155 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  155 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  155 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  155 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  155 , M_lda =  11  --->  Accuracy = 82.69%\n",
      "M_pca =  155 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  155 , M_lda =  13  --->  Accuracy = 86.54%\n",
      "M_pca =  155 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  155 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  155 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  155 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  155 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  155 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  155 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  155 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  155 , M_lda =  27  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  28  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  155 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  155 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  155 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  155 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  155 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  155 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  155 , M_lda =  42  --->  Accuracy = 93.27%\n",
      "M_pca =  155 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  155 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  155 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  155 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  155 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  1  --->  Accuracy = 12.50%\n",
      "M_pca =  156 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  156 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  156 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  156 , M_lda =  5  --->  Accuracy = 57.69%\n",
      "M_pca =  156 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  156 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  156 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  156 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  156 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  156 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  156 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  156 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  156 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  156 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  156 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  156 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  156 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  156 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  156 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  156 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  156 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  156 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  156 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  156 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  156 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  156 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  156 , M_lda =  28  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  29  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  156 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  156 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  156 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  156 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  156 , M_lda =  37  --->  Accuracy = 91.35%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  156 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  156 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  156 , M_lda =  40  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  156 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  156 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  45  --->  Accuracy = 93.27%\n",
      "M_pca =  156 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  156 , M_lda =  48  --->  Accuracy = 92.31%\n",
      "M_pca =  156 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  156 , M_lda =  51  --->  Accuracy = 93.27%\n",
      "M_pca =  157 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  157 , M_lda =  2  --->  Accuracy = 24.04%\n",
      "M_pca =  157 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  157 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  157 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  157 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  157 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  157 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  157 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  157 , M_lda =  10  --->  Accuracy = 82.69%\n",
      "M_pca =  157 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  157 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  157 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  157 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  157 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  157 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  157 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  157 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  157 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  157 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  157 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  157 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  157 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  157 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  157 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  157 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  157 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  157 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  157 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  157 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  157 , M_lda =  33  --->  Accuracy = 92.31%\n",
      "M_pca =  157 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  157 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  157 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  157 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  157 , M_lda =  41  --->  Accuracy = 92.31%\n",
      "M_pca =  157 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  157 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  157 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  157 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  157 , M_lda =  46  --->  Accuracy = 94.23%\n",
      "M_pca =  157 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  157 , M_lda =  48  --->  Accuracy = 92.31%\n",
      "M_pca =  157 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  157 , M_lda =  50  --->  Accuracy = 93.27%\n",
      "M_pca =  157 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  158 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  158 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  158 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  158 , M_lda =  4  --->  Accuracy = 52.88%\n",
      "M_pca =  158 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  158 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  158 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  158 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  158 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  158 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  158 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  158 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  158 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  158 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  158 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  158 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  158 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  158 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  158 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  158 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  158 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  158 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  158 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  158 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  158 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  158 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  158 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  158 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  158 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  158 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  158 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  158 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  158 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  158 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  158 , M_lda =  38  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  158 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  158 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  158 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  158 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  158 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  158 , M_lda =  47  --->  Accuracy = 93.27%\n",
      "M_pca =  158 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  158 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  158 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  159 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  159 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  159 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  159 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  159 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  159 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  159 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  159 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  159 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  159 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  159 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  159 , M_lda =  12  --->  Accuracy = 83.65%\n",
      "M_pca =  159 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  159 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  159 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  159 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  159 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  159 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  159 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  159 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  159 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  159 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  159 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  159 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  159 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  159 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  159 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  159 , M_lda =  30  --->  Accuracy = 93.27%\n",
      "M_pca =  159 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  159 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  159 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  159 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  159 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  159 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  159 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  159 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  159 , M_lda =  43  --->  Accuracy = 89.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  159 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  159 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  159 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  159 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  159 , M_lda =  48  --->  Accuracy = 93.27%\n",
      "M_pca =  159 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  159 , M_lda =  50  --->  Accuracy = 93.27%\n",
      "M_pca =  159 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  160 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  160 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  160 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  160 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  160 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  160 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  160 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  160 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  160 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  160 , M_lda =  10  --->  Accuracy = 81.73%\n",
      "M_pca =  160 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  160 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  160 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  160 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  160 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  160 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  160 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  160 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  160 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  160 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  160 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  160 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  160 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  160 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  160 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  160 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  160 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  160 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  160 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  160 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  160 , M_lda =  31  --->  Accuracy = 92.31%\n",
      "M_pca =  160 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  160 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  160 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  160 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  160 , M_lda =  36  --->  Accuracy = 91.35%\n",
      "M_pca =  160 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  160 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  160 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  160 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  160 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  160 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  160 , M_lda =  43  --->  Accuracy = 93.27%\n",
      "M_pca =  160 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  160 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  160 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  160 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  160 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  160 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  160 , M_lda =  50  --->  Accuracy = 93.27%\n",
      "M_pca =  160 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  161 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  161 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  161 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  161 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  161 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  161 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  161 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  161 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  161 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  161 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  161 , M_lda =  11  --->  Accuracy = 85.58%\n",
      "M_pca =  161 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  161 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  161 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  161 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  161 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  161 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  161 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  161 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  161 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  161 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  161 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  161 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  161 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  28  --->  Accuracy = 93.27%\n",
      "M_pca =  161 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  161 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  161 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  161 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  161 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  161 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  161 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  161 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  161 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  161 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  161 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  161 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  161 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  161 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  161 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  161 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  161 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  161 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  161 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  161 , M_lda =  51  --->  Accuracy = 93.27%\n",
      "M_pca =  162 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  162 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  162 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  162 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  162 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  162 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  162 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  162 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  162 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  162 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  162 , M_lda =  11  --->  Accuracy = 83.65%\n",
      "M_pca =  162 , M_lda =  12  --->  Accuracy = 84.62%\n",
      "M_pca =  162 , M_lda =  13  --->  Accuracy = 89.42%\n",
      "M_pca =  162 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  162 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  162 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  162 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  162 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  162 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  162 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  162 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  162 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  162 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  162 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  162 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  162 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  162 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  162 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  162 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  162 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  162 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  162 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  162 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  162 , M_lda =  34  --->  Accuracy = 91.35%\n",
      "M_pca =  162 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  162 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  162 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  162 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  162 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  162 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  162 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  162 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  162 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  162 , M_lda =  44  --->  Accuracy = 94.23%\n",
      "M_pca =  162 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  162 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  162 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  162 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  162 , M_lda =  49  --->  Accuracy = 90.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  162 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  162 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  163 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  163 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  163 , M_lda =  3  --->  Accuracy = 38.46%\n",
      "M_pca =  163 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  163 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  163 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  163 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  163 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  163 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  163 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  163 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  163 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  163 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  163 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  163 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  163 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  163 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  163 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  163 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  163 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  163 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  163 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  29  --->  Accuracy = 91.35%\n",
      "M_pca =  163 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  163 , M_lda =  31  --->  Accuracy = 91.35%\n",
      "M_pca =  163 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  163 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  163 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  163 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  163 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  163 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  163 , M_lda =  41  --->  Accuracy = 92.31%\n",
      "M_pca =  163 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  163 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  163 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  163 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  163 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  163 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  163 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  163 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  163 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  163 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  164 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  164 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  164 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  164 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  164 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  164 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  164 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  164 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  164 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  164 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  164 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  164 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  164 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  164 , M_lda =  14  --->  Accuracy = 87.50%\n",
      "M_pca =  164 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  164 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  164 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  164 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  164 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  164 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  164 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  164 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  164 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  164 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  164 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  164 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  164 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  164 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  164 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  164 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  164 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  164 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  164 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  43  --->  Accuracy = 93.27%\n",
      "M_pca =  164 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  164 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  164 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  164 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  164 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  164 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  165 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  165 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  165 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  165 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  165 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  165 , M_lda =  7  --->  Accuracy = 75.00%\n",
      "M_pca =  165 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  165 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  165 , M_lda =  10  --->  Accuracy = 80.77%\n",
      "M_pca =  165 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  165 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  165 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  165 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  165 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  165 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  165 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  165 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  165 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  165 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  165 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  165 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  165 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  165 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  165 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  165 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  165 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  165 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  165 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  165 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  165 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  165 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  165 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  165 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  165 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  165 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  165 , M_lda =  40  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  165 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  165 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  165 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  165 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  166 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  166 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  166 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  166 , M_lda =  4  --->  Accuracy = 39.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  166 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  166 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  166 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  166 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  166 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  166 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  166 , M_lda =  11  --->  Accuracy = 84.62%\n",
      "M_pca =  166 , M_lda =  12  --->  Accuracy = 85.58%\n",
      "M_pca =  166 , M_lda =  13  --->  Accuracy = 83.65%\n",
      "M_pca =  166 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  166 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  166 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  166 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  166 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  166 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  166 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  166 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  166 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  166 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  166 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  166 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  28  --->  Accuracy = 90.38%\n",
      "M_pca =  166 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  166 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  166 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  166 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  166 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  166 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  166 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  166 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  166 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  166 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  166 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  166 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  166 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  166 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  166 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  167 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  167 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  167 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  167 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  167 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  167 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  167 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  167 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  167 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  167 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  167 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  167 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  167 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  167 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  167 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  167 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  167 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  167 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  167 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  167 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  167 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  167 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  167 , M_lda =  23  --->  Accuracy = 91.35%\n",
      "M_pca =  167 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  167 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  167 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  167 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  167 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  167 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  167 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  167 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  167 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  34  --->  Accuracy = 92.31%\n",
      "M_pca =  167 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  167 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  167 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  167 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  167 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  167 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  167 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  167 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  167 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  167 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  167 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  167 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  167 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  167 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  168 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  168 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  168 , M_lda =  4  --->  Accuracy = 46.15%\n",
      "M_pca =  168 , M_lda =  5  --->  Accuracy = 54.81%\n",
      "M_pca =  168 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  168 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  168 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  168 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  168 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  168 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  168 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  168 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  168 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  168 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  168 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  168 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  168 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  168 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  168 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  168 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  168 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  168 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  168 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  168 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  168 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  168 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  168 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  168 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  168 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  168 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  168 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  168 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  168 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  168 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  168 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  168 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  168 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  168 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  168 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  168 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  169 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  169 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  169 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  169 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  169 , M_lda =  5  --->  Accuracy = 56.73%\n",
      "M_pca =  169 , M_lda =  6  --->  Accuracy = 67.31%\n",
      "M_pca =  169 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  169 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  169 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  169 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  169 , M_lda =  11  --->  Accuracy = 79.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  169 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  169 , M_lda =  13  --->  Accuracy = 88.46%\n",
      "M_pca =  169 , M_lda =  14  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  169 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  169 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  169 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  169 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  169 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  169 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  169 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  169 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  169 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  169 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  169 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  169 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  169 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  169 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  169 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  169 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  169 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  169 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  169 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  169 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  170 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  170 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  170 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  170 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  170 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  170 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  170 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  170 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  170 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  170 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  170 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  170 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  170 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  170 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  170 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  170 , M_lda =  17  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  170 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  170 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  170 , M_lda =  21  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  170 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  170 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  170 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  170 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  170 , M_lda =  30  --->  Accuracy = 91.35%\n",
      "M_pca =  170 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  170 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  170 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  170 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  170 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  170 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  170 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  170 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  170 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  170 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  171 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  171 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  171 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  171 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  171 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  171 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  171 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  171 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  171 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  171 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  171 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  171 , M_lda =  12  --->  Accuracy = 82.69%\n",
      "M_pca =  171 , M_lda =  13  --->  Accuracy = 85.58%\n",
      "M_pca =  171 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  171 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  171 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  171 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  171 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  171 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  171 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  171 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  171 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  171 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  171 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  171 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  171 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  171 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  171 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  171 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  171 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  171 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  171 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  171 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  171 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  171 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  171 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  171 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  171 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  171 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  171 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  171 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  171 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  171 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  171 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  171 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  171 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  171 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  171 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  171 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  171 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  171 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  172 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  172 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  172 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  172 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  172 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  172 , M_lda =  7  --->  Accuracy = 72.12%\n",
      "M_pca =  172 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  172 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  172 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  172 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  172 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  172 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  172 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  172 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  172 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  172 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  172 , M_lda =  18  --->  Accuracy = 85.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  172 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  172 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  172 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  172 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  172 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  172 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  172 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  172 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  172 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  172 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  172 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  172 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  172 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  172 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  172 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  172 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  172 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  172 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  172 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  172 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  172 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  172 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  172 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  172 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  173 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  173 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  173 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  173 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  173 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  173 , M_lda =  7  --->  Accuracy = 69.23%\n",
      "M_pca =  173 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  173 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  173 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  173 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  173 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  173 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  173 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  173 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  173 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  173 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  173 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  173 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  173 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  173 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  173 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  173 , M_lda =  25  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  173 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  173 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  173 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  173 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  173 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  173 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  173 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  173 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  173 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  173 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  173 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  173 , M_lda =  43  --->  Accuracy = 93.27%\n",
      "M_pca =  173 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  173 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  173 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  173 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  173 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  174 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  174 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  174 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  174 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  174 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  174 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  174 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  174 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  174 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  174 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  174 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  174 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  174 , M_lda =  13  --->  Accuracy = 84.62%\n",
      "M_pca =  174 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  174 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  174 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  174 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  174 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  174 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  174 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  174 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  174 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  174 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  25  --->  Accuracy = 90.38%\n",
      "M_pca =  174 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  174 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  174 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  174 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  37  --->  Accuracy = 90.38%\n",
      "M_pca =  174 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  39  --->  Accuracy = 90.38%\n",
      "M_pca =  174 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  174 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  174 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  174 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  174 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  174 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  174 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  174 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  174 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  175 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  175 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  175 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  175 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  175 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  175 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  175 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  175 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  175 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  175 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  175 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  175 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  175 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  175 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  175 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  175 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  175 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  175 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  175 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  175 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  175 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  175 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  175 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  175 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  25  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  175 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  175 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  175 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  175 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  175 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  175 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  175 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  175 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  175 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  45  --->  Accuracy = 94.23%\n",
      "M_pca =  175 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  175 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  175 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  175 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  175 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  176 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  176 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  176 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  176 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  176 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  176 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  176 , M_lda =  7  --->  Accuracy = 70.19%\n",
      "M_pca =  176 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  176 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  176 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  176 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  176 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  176 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  176 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  176 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  176 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  176 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  176 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  176 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  176 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  176 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  176 , M_lda =  22  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  176 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  176 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  176 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  176 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  176 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  176 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  176 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  176 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  176 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  176 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  176 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  176 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  176 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  176 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  176 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  176 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  176 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  176 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  176 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  176 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  176 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  176 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  177 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  177 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  177 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  177 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  177 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  177 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  177 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  177 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  177 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  177 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  177 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  177 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  177 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  177 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  177 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  177 , M_lda =  16  --->  Accuracy = 86.54%\n",
      "M_pca =  177 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  177 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  177 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  177 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  177 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  177 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  177 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  177 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  177 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  177 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  177 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  177 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  177 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  177 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  177 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  177 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  177 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  177 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  177 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  177 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  177 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  177 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  177 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  177 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  178 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  178 , M_lda =  2  --->  Accuracy = 25.96%\n",
      "M_pca =  178 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  178 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  178 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  178 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  178 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  178 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  178 , M_lda =  9  --->  Accuracy = 77.88%\n",
      "M_pca =  178 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  178 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  178 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  178 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  178 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  178 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  178 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  178 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  178 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  178 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  178 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  178 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  178 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  32  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  178 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  178 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  178 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  178 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  178 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  178 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  178 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  178 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  178 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  178 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  179 , M_lda =  1  --->  Accuracy = 13.46%\n",
      "M_pca =  179 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  179 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  179 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  179 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  179 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  179 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  179 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  179 , M_lda =  9  --->  Accuracy = 78.85%\n",
      "M_pca =  179 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  179 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  179 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  179 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  179 , M_lda =  14  --->  Accuracy = 84.62%\n",
      "M_pca =  179 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  179 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  179 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  179 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  179 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  179 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  179 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  179 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  179 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  179 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  179 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  179 , M_lda =  26  --->  Accuracy = 90.38%\n",
      "M_pca =  179 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  179 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  179 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  179 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  179 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  179 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  179 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  179 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  179 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  179 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  179 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  179 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  179 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  179 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  179 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  179 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  179 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  179 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  179 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  179 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  179 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  179 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  179 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  179 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  179 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  180 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  180 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  180 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  180 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  180 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  180 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  180 , M_lda =  8  --->  Accuracy = 75.96%\n",
      "M_pca =  180 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  180 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  180 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  180 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  180 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  180 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  180 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  180 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  180 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  180 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  180 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  180 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  180 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  180 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  180 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  180 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  180 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  180 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  180 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  180 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  180 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  180 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  180 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  180 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  180 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  180 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  180 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  180 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  180 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  180 , M_lda =  48  --->  Accuracy = 93.27%\n",
      "M_pca =  180 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  180 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  180 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  181 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  181 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  181 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  181 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  181 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  181 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  181 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  181 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  181 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  181 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  181 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  181 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  181 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  181 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  181 , M_lda =  15  --->  Accuracy = 84.62%\n",
      "M_pca =  181 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  181 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  181 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  181 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  181 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  181 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  181 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  181 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  181 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  181 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  181 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  181 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  181 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  181 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  181 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  181 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  181 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  181 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  181 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  181 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  181 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  181 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  181 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  181 , M_lda =  39  --->  Accuracy = 89.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  181 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  181 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  181 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  181 , M_lda =  43  --->  Accuracy = 93.27%\n",
      "M_pca =  181 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  181 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  181 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  181 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  181 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  181 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  181 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  181 , M_lda =  51  --->  Accuracy = 93.27%\n",
      "M_pca =  182 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  182 , M_lda =  2  --->  Accuracy = 24.04%\n",
      "M_pca =  182 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  182 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  182 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  182 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  182 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  182 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  182 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  182 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  182 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  182 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  182 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  182 , M_lda =  14  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  182 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  182 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  182 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  182 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  182 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  182 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  182 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  182 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  182 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  182 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  182 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  182 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  182 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  182 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  182 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  182 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  182 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  182 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  182 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  182 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  183 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  183 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  183 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  183 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  183 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  183 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  183 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  183 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  183 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  183 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  183 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  183 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  183 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  183 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  183 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  183 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  183 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  183 , M_lda =  18  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  183 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  183 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  183 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  183 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  183 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  183 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  183 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  183 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  183 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  183 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  183 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  183 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  183 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  183 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  183 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  183 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  183 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  184 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  184 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  184 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  184 , M_lda =  4  --->  Accuracy = 53.85%\n",
      "M_pca =  184 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  184 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  184 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  184 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  184 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  184 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  184 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  184 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  184 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  184 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  184 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  184 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  184 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  184 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  184 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  184 , M_lda =  20  --->  Accuracy = 88.46%\n",
      "M_pca =  184 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  184 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  184 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  184 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  184 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  184 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  184 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  184 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  184 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  184 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  184 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  184 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  184 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  184 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  184 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  184 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  184 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  184 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  184 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  184 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  184 , M_lda =  46  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  184 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  184 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  184 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  184 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  184 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  185 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  185 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  185 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  185 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  185 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  185 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  185 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  185 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  185 , M_lda =  9  --->  Accuracy = 76.92%\n",
      "M_pca =  185 , M_lda =  10  --->  Accuracy = 81.73%\n",
      "M_pca =  185 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  185 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  185 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  185 , M_lda =  14  --->  Accuracy = 83.65%\n",
      "M_pca =  185 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  185 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  185 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  19  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  185 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  185 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  185 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  185 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  185 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  185 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  185 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  185 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  185 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  185 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  185 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  185 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  185 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  185 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  185 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  186 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  186 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  186 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  186 , M_lda =  4  --->  Accuracy = 49.04%\n",
      "M_pca =  186 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  186 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  186 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  186 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  186 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  186 , M_lda =  10  --->  Accuracy = 78.85%\n",
      "M_pca =  186 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  186 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  186 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  186 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  186 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  186 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  186 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  186 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  186 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  186 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  186 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  186 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  186 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  186 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  186 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  186 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  186 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  186 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  186 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  186 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  186 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  186 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  186 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  186 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  186 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  187 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  187 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  187 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  187 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  187 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  187 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  187 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  187 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  187 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  187 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  187 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  187 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  187 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  187 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  187 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  187 , M_lda =  16  --->  Accuracy = 85.58%\n",
      "M_pca =  187 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  187 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  187 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  187 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  187 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  187 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  187 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  187 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  187 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  187 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  187 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  187 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  187 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  187 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  187 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  187 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  187 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  187 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  187 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  187 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  187 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  187 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  187 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  188 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  188 , M_lda =  2  --->  Accuracy = 15.38%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  188 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  188 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  188 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  188 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  188 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  188 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  188 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  188 , M_lda =  10  --->  Accuracy = 79.81%\n",
      "M_pca =  188 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  188 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  188 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  188 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  188 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  188 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  188 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  188 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  188 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  188 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  188 , M_lda =  21  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  188 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  188 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  188 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  188 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  188 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  188 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  188 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  188 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  188 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  188 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  188 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  188 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  188 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  188 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  189 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  189 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  189 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  189 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  189 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  189 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  189 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  189 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  189 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  189 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  189 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  189 , M_lda =  12  --->  Accuracy = 80.77%\n",
      "M_pca =  189 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  189 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  189 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  189 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  189 , M_lda =  17  --->  Accuracy = 85.58%\n",
      "M_pca =  189 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  189 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  189 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  189 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  189 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  189 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  189 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  189 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  189 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  189 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  189 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  189 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  189 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  189 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  189 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  189 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  189 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  189 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  189 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  189 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  189 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  189 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  189 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  189 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  190 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  190 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  190 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  190 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  190 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  190 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  190 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  190 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  190 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  190 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  190 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  190 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  190 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  190 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  190 , M_lda =  15  --->  Accuracy = 83.65%\n",
      "M_pca =  190 , M_lda =  16  --->  Accuracy = 84.62%\n",
      "M_pca =  190 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  190 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  190 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  190 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  190 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  190 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  190 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  190 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  190 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  190 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  190 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  190 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  190 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  190 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  190 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  190 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  190 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  190 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  190 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  191 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  191 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  191 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  191 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  191 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  191 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  191 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  191 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  191 , M_lda =  9  --->  Accuracy = 73.08%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  191 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  191 , M_lda =  11  --->  Accuracy = 80.77%\n",
      "M_pca =  191 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  191 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  191 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  191 , M_lda =  15  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  191 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  191 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  191 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  191 , M_lda =  20  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  191 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  191 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  191 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  30  --->  Accuracy = 90.38%\n",
      "M_pca =  191 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  191 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  191 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  191 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  191 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  191 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  191 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  191 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  191 , M_lda =  44  --->  Accuracy = 92.31%\n",
      "M_pca =  191 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  191 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  191 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  191 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  191 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  191 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  191 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  192 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  192 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  192 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  192 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  192 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  192 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  192 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  192 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  192 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  192 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  192 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  192 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  192 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  192 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  192 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  192 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  192 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  192 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  192 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  192 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  192 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  192 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  192 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  192 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  192 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  192 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  192 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  192 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  192 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  192 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  192 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  192 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  192 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  192 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  192 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  192 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  192 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  192 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  192 , M_lda =  44  --->  Accuracy = 93.27%\n",
      "M_pca =  192 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  192 , M_lda =  46  --->  Accuracy = 92.31%\n",
      "M_pca =  192 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  192 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  192 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  192 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  192 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  193 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  193 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  193 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  193 , M_lda =  5  --->  Accuracy = 52.88%\n",
      "M_pca =  193 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  193 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  193 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  193 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  193 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  193 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  193 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  193 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  193 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  193 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  193 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  193 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  193 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  193 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  193 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  193 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  193 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  193 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  193 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  193 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  193 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  193 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  193 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  193 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  193 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  193 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  193 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  193 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  193 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  193 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  193 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  193 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  193 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  193 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  193 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  194 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  194 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  194 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  194 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  194 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  194 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  194 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  194 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  194 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  194 , M_lda =  10  --->  Accuracy = 76.92%\n",
      "M_pca =  194 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  194 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  194 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  194 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  194 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  194 , M_lda =  16  --->  Accuracy = 84.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  194 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  194 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  194 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  194 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  194 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  194 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  194 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  194 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  194 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  194 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  194 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  194 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  194 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  194 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  194 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  194 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  194 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  194 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  194 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  194 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  194 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  194 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  194 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  194 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  194 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  194 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  194 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  194 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  194 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  195 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  195 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  195 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  195 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  195 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  195 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  195 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  195 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  195 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  195 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  195 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  195 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  195 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  195 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  195 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  195 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  195 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  195 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  195 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  195 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  195 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  195 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  195 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  195 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  195 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  195 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  195 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  195 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  195 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  195 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  195 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  195 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  195 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  195 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  195 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  195 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  196 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  196 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  196 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  196 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  196 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  196 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  196 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  196 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  196 , M_lda =  9  --->  Accuracy = 75.96%\n",
      "M_pca =  196 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  196 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  196 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  196 , M_lda =  13  --->  Accuracy = 81.73%\n",
      "M_pca =  196 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  196 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  196 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  196 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  196 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  196 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  196 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  196 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  196 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  196 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  196 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  196 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  196 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  196 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  196 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  196 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  196 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  196 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  196 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  196 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  196 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  197 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  197 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  197 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  197 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  197 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  197 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  197 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  197 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  197 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  197 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  197 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  197 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  197 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  197 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  197 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  197 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  197 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  197 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  197 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  21  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  23  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  197 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  197 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  197 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  197 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  197 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  41  --->  Accuracy = 92.31%\n",
      "M_pca =  197 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  197 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  197 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  197 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  197 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  197 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  197 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  197 , M_lda =  49  --->  Accuracy = 92.31%\n",
      "M_pca =  197 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  197 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  198 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  198 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  198 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  198 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  198 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  198 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  198 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  198 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  198 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  198 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  198 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  198 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  198 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  198 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  198 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  198 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  198 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  198 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  198 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  198 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  198 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  198 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  198 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  198 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  198 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  198 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  198 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  198 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  198 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  198 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  198 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  198 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  198 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  198 , M_lda =  37  --->  Accuracy = 91.35%\n",
      "M_pca =  198 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  198 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  198 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  198 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  198 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  198 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  198 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  198 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  198 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  198 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  198 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  199 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  199 , M_lda =  2  --->  Accuracy = 25.00%\n",
      "M_pca =  199 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  199 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  199 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  199 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  199 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  199 , M_lda =  8  --->  Accuracy = 72.12%\n",
      "M_pca =  199 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  199 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  199 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  199 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  199 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  199 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  199 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  199 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  199 , M_lda =  17  --->  Accuracy = 84.62%\n",
      "M_pca =  199 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  199 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  199 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  199 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  199 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  199 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  199 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  199 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  199 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  199 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  199 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  199 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  199 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  199 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  199 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  199 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  199 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  199 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  199 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  199 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  199 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  199 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  199 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  199 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  199 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  199 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  200 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  200 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  200 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  200 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  200 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  200 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  200 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  200 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  200 , M_lda =  9  --->  Accuracy = 75.00%\n",
      "M_pca =  200 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  200 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  200 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  200 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  200 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  200 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  200 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  200 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  200 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  200 , M_lda =  19  --->  Accuracy = 87.50%\n",
      "M_pca =  200 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  200 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  200 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  200 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  200 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  200 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  200 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  200 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  200 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  29  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  200 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  200 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  200 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  200 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  200 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  200 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  200 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  200 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  200 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  200 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  200 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  200 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  200 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  200 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  200 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  200 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  200 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  200 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  201 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  201 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  201 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  201 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  201 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  201 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  201 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  201 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  201 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  201 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  201 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  201 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  201 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  201 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  201 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  201 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  201 , M_lda =  17  --->  Accuracy = 87.50%\n",
      "M_pca =  201 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  201 , M_lda =  19  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  201 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  201 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  201 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  201 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  201 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  201 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  201 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  201 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  201 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  201 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  201 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  201 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  201 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  201 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  201 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  201 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  201 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  201 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  201 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  201 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  201 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  201 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  202 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  202 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  202 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  202 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  202 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  202 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  202 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  202 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  202 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  202 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  202 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  202 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  202 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  202 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  202 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  202 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  202 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  202 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  202 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  202 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  202 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  202 , M_lda =  23  --->  Accuracy = 89.42%\n",
      "M_pca =  202 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  202 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  202 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  202 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  202 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  202 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  202 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  202 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  202 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  202 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  202 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  202 , M_lda =  47  --->  Accuracy = 92.31%\n",
      "M_pca =  202 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  202 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  202 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  202 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  203 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  203 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  203 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  203 , M_lda =  4  --->  Accuracy = 30.77%\n",
      "M_pca =  203 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  203 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  203 , M_lda =  7  --->  Accuracy = 68.27%\n",
      "M_pca =  203 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  203 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  203 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  203 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  203 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  203 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  203 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  203 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  203 , M_lda =  16  --->  Accuracy = 83.65%\n",
      "M_pca =  203 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  203 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  203 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  203 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  203 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  203 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  203 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  203 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  203 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  35  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  203 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  203 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  203 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  203 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  203 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  203 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  203 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  203 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  203 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  203 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  203 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  204 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  204 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  204 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  204 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  204 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  204 , M_lda =  6  --->  Accuracy = 62.50%\n",
      "M_pca =  204 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  204 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  204 , M_lda =  9  --->  Accuracy = 74.04%\n",
      "M_pca =  204 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  204 , M_lda =  11  --->  Accuracy = 78.85%\n",
      "M_pca =  204 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  204 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  204 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  204 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  204 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  204 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  204 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  204 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  204 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  204 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  204 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  204 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  204 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  204 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  204 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  204 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  204 , M_lda =  47  --->  Accuracy = 93.27%\n",
      "M_pca =  204 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  204 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  204 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  205 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  205 , M_lda =  2  --->  Accuracy = 24.04%\n",
      "M_pca =  205 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  205 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  205 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  205 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  205 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  205 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  205 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  205 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  205 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  205 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  205 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  205 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  205 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  205 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  205 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  205 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  205 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  205 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  205 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  205 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  205 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  205 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  205 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  205 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  205 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  205 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  205 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  205 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  205 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  205 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  205 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  205 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  205 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  205 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  205 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  205 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  205 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  206 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  206 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  206 , M_lda =  4  --->  Accuracy = 44.23%\n",
      "M_pca =  206 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  206 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  206 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  206 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  206 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  206 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  206 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  206 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  206 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  206 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  206 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  206 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  206 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  206 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  206 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  206 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  206 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  206 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  206 , M_lda =  23  --->  Accuracy = 90.38%\n",
      "M_pca =  206 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  206 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  206 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  206 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  206 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  206 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  206 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  206 , M_lda =  42  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  206 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  206 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  206 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  206 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  206 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  206 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  207 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  207 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  207 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  207 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  207 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  207 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  207 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  207 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  207 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  207 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  207 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  207 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  207 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  207 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  207 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  207 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  207 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  207 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  207 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  207 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  207 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  207 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  207 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  207 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  207 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  207 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  207 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  207 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  207 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  207 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  207 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  207 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  207 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  207 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  207 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  207 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  207 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  208 , M_lda =  1  --->  Accuracy = 14.42%\n",
      "M_pca =  208 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  208 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  208 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  208 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  208 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  208 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  208 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  208 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  208 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  208 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  208 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  208 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  208 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  208 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  208 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  208 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  208 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  208 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  208 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  208 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  208 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  208 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  208 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  208 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  208 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  208 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  208 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  208 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  208 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  208 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  208 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  208 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  208 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  208 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  208 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  208 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  208 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  209 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  209 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  209 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  209 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  209 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  209 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  209 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  209 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  209 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  209 , M_lda =  10  --->  Accuracy = 75.96%\n",
      "M_pca =  209 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  209 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  209 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  209 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  209 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  209 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  209 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  209 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  209 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  209 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  209 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  209 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  209 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  209 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  209 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  209 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  209 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  209 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  209 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  209 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  209 , M_lda =  33  --->  Accuracy = 89.42%\n",
      "M_pca =  209 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  209 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  209 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  209 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  209 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  209 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  209 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  209 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  209 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  209 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  209 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  209 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  209 , M_lda =  49  --->  Accuracy = 89.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  209 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  209 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  210 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  210 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  210 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  210 , M_lda =  4  --->  Accuracy = 45.19%\n",
      "M_pca =  210 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  210 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  210 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  210 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  210 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  210 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  210 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  210 , M_lda =  12  --->  Accuracy = 79.81%\n",
      "M_pca =  210 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  210 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  210 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  210 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  210 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  210 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  210 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  210 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  210 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  210 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  210 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  210 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  210 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  210 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  210 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  210 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  210 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  210 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  210 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  210 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  210 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  210 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  210 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  210 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  210 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  210 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  210 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  210 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  211 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  211 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  211 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  211 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  211 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  211 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  211 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  211 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  211 , M_lda =  10  --->  Accuracy = 74.04%\n",
      "M_pca =  211 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  211 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  211 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  211 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  211 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  211 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  211 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  211 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  211 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  211 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  211 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  211 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  211 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  211 , M_lda =  24  --->  Accuracy = 89.42%\n",
      "M_pca =  211 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  211 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  211 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  211 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  211 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  211 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  211 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  211 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  211 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  211 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  211 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  211 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  211 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  211 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  211 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  211 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  45  --->  Accuracy = 91.35%\n",
      "M_pca =  211 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  211 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  211 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  211 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  211 , M_lda =  51  --->  Accuracy = 92.31%\n",
      "M_pca =  212 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  212 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  212 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  212 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  212 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  212 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  212 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  212 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  212 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  212 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  212 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  212 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  212 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  212 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  212 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  212 , M_lda =  16  --->  Accuracy = 82.69%\n",
      "M_pca =  212 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  212 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  212 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  212 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  212 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  212 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  212 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  212 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  212 , M_lda =  32  --->  Accuracy = 92.31%\n",
      "M_pca =  212 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  212 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  212 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  212 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  212 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  212 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  212 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  212 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  212 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  212 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  212 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  212 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  212 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  212 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  212 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  213 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  213 , M_lda =  3  --->  Accuracy = 37.50%\n",
      "M_pca =  213 , M_lda =  4  --->  Accuracy = 29.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  213 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  213 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  213 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  213 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  213 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  213 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  213 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  213 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  213 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  213 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  213 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  213 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  213 , M_lda =  17  --->  Accuracy = 83.65%\n",
      "M_pca =  213 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  213 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  213 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  213 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  213 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  213 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  213 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  213 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  213 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  213 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  213 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  213 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  213 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  213 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  213 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  213 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  213 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  213 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  213 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  213 , M_lda =  47  --->  Accuracy = 91.35%\n",
      "M_pca =  213 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  213 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  213 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  213 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  214 , M_lda =  2  --->  Accuracy = 24.04%\n",
      "M_pca =  214 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  214 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  214 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  214 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  214 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  214 , M_lda =  8  --->  Accuracy = 71.15%\n",
      "M_pca =  214 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  214 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  214 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  214 , M_lda =  12  --->  Accuracy = 81.73%\n",
      "M_pca =  214 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  214 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  214 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  214 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  214 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  214 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  214 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  214 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  214 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  214 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  214 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  214 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  214 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  214 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  214 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  214 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  214 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  214 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  214 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  214 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  214 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  214 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  214 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  215 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  215 , M_lda =  2  --->  Accuracy = 24.04%\n",
      "M_pca =  215 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  215 , M_lda =  4  --->  Accuracy = 50.00%\n",
      "M_pca =  215 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  215 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  215 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  215 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  215 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  215 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  215 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  215 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  215 , M_lda =  13  --->  Accuracy = 78.85%\n",
      "M_pca =  215 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  215 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  215 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  215 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  215 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  215 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  215 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  215 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  215 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  215 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  215 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  215 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  215 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  215 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  215 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  215 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  215 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  215 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  215 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  215 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  215 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  215 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  215 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  215 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  215 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  215 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  215 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  215 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  215 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  216 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  216 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  216 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  216 , M_lda =  4  --->  Accuracy = 47.12%\n",
      "M_pca =  216 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  216 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  216 , M_lda =  7  --->  Accuracy = 66.35%\n",
      "M_pca =  216 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  216 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  216 , M_lda =  10  --->  Accuracy = 72.12%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  216 , M_lda =  11  --->  Accuracy = 77.88%\n",
      "M_pca =  216 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  216 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  216 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  216 , M_lda =  15  --->  Accuracy = 82.69%\n",
      "M_pca =  216 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  216 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  216 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  216 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  216 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  216 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  216 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  216 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  216 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  216 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  216 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  216 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  216 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  39  --->  Accuracy = 91.35%\n",
      "M_pca =  216 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  216 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  216 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  216 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  216 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  216 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  216 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  216 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  216 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  216 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  216 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  217 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  217 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  217 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  217 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  217 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  217 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  217 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  217 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  217 , M_lda =  10  --->  Accuracy = 77.88%\n",
      "M_pca =  217 , M_lda =  11  --->  Accuracy = 79.81%\n",
      "M_pca =  217 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  217 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  217 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  217 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  217 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  217 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  217 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  217 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  217 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  217 , M_lda =  21  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  217 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  217 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  217 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  217 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  217 , M_lda =  27  --->  Accuracy = 92.31%\n",
      "M_pca =  217 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  217 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  217 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  217 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  217 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  217 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  217 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  217 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  217 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  217 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  217 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  217 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  217 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  217 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  217 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  218 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  218 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  218 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  218 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  218 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  218 , M_lda =  7  --->  Accuracy = 56.73%\n",
      "M_pca =  218 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  218 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  218 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  218 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  218 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  218 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  218 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  218 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  218 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  218 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  218 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  218 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  218 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  218 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  218 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  25  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  218 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  218 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  218 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  218 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  218 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  218 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  37  --->  Accuracy = 92.31%\n",
      "M_pca =  218 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  218 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  218 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  218 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  218 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  218 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  218 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  49  --->  Accuracy = 93.27%\n",
      "M_pca =  218 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  218 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  219 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  219 , M_lda =  2  --->  Accuracy = 26.92%\n",
      "M_pca =  219 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  219 , M_lda =  4  --->  Accuracy = 48.08%\n",
      "M_pca =  219 , M_lda =  5  --->  Accuracy = 51.92%\n",
      "M_pca =  219 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  219 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  219 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  219 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  219 , M_lda =  10  --->  Accuracy = 75.00%\n",
      "M_pca =  219 , M_lda =  11  --->  Accuracy = 81.73%\n",
      "M_pca =  219 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  219 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  219 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  219 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  219 , M_lda =  16  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  219 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  219 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  219 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  219 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  219 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  219 , M_lda =  22  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  219 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  219 , M_lda =  26  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  219 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  219 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  219 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  219 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  219 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  219 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  219 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  219 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  219 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  219 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  219 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  220 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  220 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  220 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  220 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  220 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  220 , M_lda =  6  --->  Accuracy = 45.19%\n",
      "M_pca =  220 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  220 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  220 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  220 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  220 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  220 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  220 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  220 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  220 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  220 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  220 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  220 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  220 , M_lda =  19  --->  Accuracy = 85.58%\n",
      "M_pca =  220 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  220 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  220 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  220 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  220 , M_lda =  24  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  220 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  220 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  220 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  220 , M_lda =  36  --->  Accuracy = 90.38%\n",
      "M_pca =  220 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  220 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  220 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  220 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  220 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  220 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  220 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  220 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  221 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  221 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  221 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  221 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  221 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  221 , M_lda =  7  --->  Accuracy = 56.73%\n",
      "M_pca =  221 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  221 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  221 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  221 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  221 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  221 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  221 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  221 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  221 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  221 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  221 , M_lda =  18  --->  Accuracy = 85.58%\n",
      "M_pca =  221 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  221 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  221 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  221 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  221 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  221 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  221 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  221 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  221 , M_lda =  37  --->  Accuracy = 92.31%\n",
      "M_pca =  221 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  221 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  221 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  221 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  221 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  221 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  221 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  221 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  221 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  221 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  221 , M_lda =  50  --->  Accuracy = 91.35%\n",
      "M_pca =  221 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  222 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  222 , M_lda =  2  --->  Accuracy = 25.00%\n",
      "M_pca =  222 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  222 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  222 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  222 , M_lda =  6  --->  Accuracy = 61.54%\n",
      "M_pca =  222 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  222 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  222 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  222 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  222 , M_lda =  11  --->  Accuracy = 76.92%\n",
      "M_pca =  222 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  222 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  222 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  222 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  222 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  222 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  222 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  222 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  222 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  222 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  222 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  222 , M_lda =  23  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  222 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  222 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  222 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  222 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  222 , M_lda =  31  --->  Accuracy = 90.38%\n",
      "M_pca =  222 , M_lda =  32  --->  Accuracy = 91.35%\n",
      "M_pca =  222 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  222 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  222 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  222 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  222 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  222 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  222 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  222 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  222 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  223 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  223 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  223 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  223 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  223 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  223 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  223 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  223 , M_lda =  8  --->  Accuracy = 70.19%\n",
      "M_pca =  223 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  223 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  223 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  223 , M_lda =  12  --->  Accuracy = 77.88%\n",
      "M_pca =  223 , M_lda =  13  --->  Accuracy = 82.69%\n",
      "M_pca =  223 , M_lda =  14  --->  Accuracy = 82.69%\n",
      "M_pca =  223 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  223 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  223 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  223 , M_lda =  18  --->  Accuracy = 84.62%\n",
      "M_pca =  223 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  223 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  223 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  223 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  223 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  223 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  223 , M_lda =  26  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  27  --->  Accuracy = 85.58%\n",
      "M_pca =  223 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  223 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  223 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  223 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  223 , M_lda =  34  --->  Accuracy = 90.38%\n",
      "M_pca =  223 , M_lda =  35  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  223 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  223 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  223 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  223 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  223 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  224 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  224 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  224 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  224 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  224 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  224 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  224 , M_lda =  8  --->  Accuracy = 73.08%\n",
      "M_pca =  224 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  224 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  224 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  224 , M_lda =  12  --->  Accuracy = 78.85%\n",
      "M_pca =  224 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  224 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  224 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  224 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  224 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  224 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  224 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  224 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  224 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  224 , M_lda =  22  --->  Accuracy = 87.50%\n",
      "M_pca =  224 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  224 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  224 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  224 , M_lda =  26  --->  Accuracy = 86.54%\n",
      "M_pca =  224 , M_lda =  27  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  224 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  224 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  224 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  224 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  224 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  224 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  224 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  224 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  224 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  224 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  224 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  224 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  224 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  224 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  224 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  224 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  224 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  225 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  225 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  225 , M_lda =  4  --->  Accuracy = 39.42%\n",
      "M_pca =  225 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  225 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  225 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  225 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  225 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  225 , M_lda =  10  --->  Accuracy = 72.12%\n",
      "M_pca =  225 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  225 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  225 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  225 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  225 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  225 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  225 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  225 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  225 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  225 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  225 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  225 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  225 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  225 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  225 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  225 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  225 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  225 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  225 , M_lda =  29  --->  Accuracy = 88.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  225 , M_lda =  30  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  225 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  225 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  225 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  225 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  225 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  225 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  225 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  225 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  225 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  225 , M_lda =  42  --->  Accuracy = 92.31%\n",
      "M_pca =  225 , M_lda =  43  --->  Accuracy = 92.31%\n",
      "M_pca =  225 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  225 , M_lda =  46  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  225 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  225 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  225 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  226 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  226 , M_lda =  2  --->  Accuracy = 26.92%\n",
      "M_pca =  226 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  226 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  226 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  226 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  226 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  226 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  226 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  226 , M_lda =  10  --->  Accuracy = 73.08%\n",
      "M_pca =  226 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  226 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  226 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  226 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  226 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  226 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  226 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  226 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  226 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  226 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  226 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  226 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  226 , M_lda =  23  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  226 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  226 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  226 , M_lda =  28  --->  Accuracy = 89.42%\n",
      "M_pca =  226 , M_lda =  29  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  226 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  226 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  226 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  226 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  226 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  226 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  226 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  226 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  226 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  226 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  226 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  226 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  226 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  226 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  226 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  226 , M_lda =  48  --->  Accuracy = 91.35%\n",
      "M_pca =  226 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  226 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  226 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  227 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  227 , M_lda =  2  --->  Accuracy = 26.92%\n",
      "M_pca =  227 , M_lda =  3  --->  Accuracy = 38.46%\n",
      "M_pca =  227 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  227 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  227 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  227 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  227 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  227 , M_lda =  9  --->  Accuracy = 72.12%\n",
      "M_pca =  227 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  227 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  227 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  227 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  227 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  227 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  227 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  227 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  227 , M_lda =  18  --->  Accuracy = 86.54%\n",
      "M_pca =  227 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  227 , M_lda =  20  --->  Accuracy = 85.58%\n",
      "M_pca =  227 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  227 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  227 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  227 , M_lda =  24  --->  Accuracy = 86.54%\n",
      "M_pca =  227 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  227 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  28  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  227 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  227 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  227 , M_lda =  37  --->  Accuracy = 91.35%\n",
      "M_pca =  227 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  227 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  227 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  227 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  227 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  227 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  227 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  227 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  227 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  228 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  228 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  228 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  228 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  228 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  228 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  228 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  228 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  228 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  228 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  228 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  228 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  228 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  228 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  228 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  228 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  228 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  228 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  228 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  228 , M_lda =  20  --->  Accuracy = 86.54%\n",
      "M_pca =  228 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  228 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  228 , M_lda =  23  --->  Accuracy = 87.50%\n",
      "M_pca =  228 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  228 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  228 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  228 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  228 , M_lda =  28  --->  Accuracy = 87.50%\n",
      "M_pca =  228 , M_lda =  29  --->  Accuracy = 87.50%\n",
      "M_pca =  228 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  228 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  228 , M_lda =  32  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  228 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  228 , M_lda =  35  --->  Accuracy = 89.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  228 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  228 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  228 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  228 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  228 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  228 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  42  --->  Accuracy = 91.35%\n",
      "M_pca =  228 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  228 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  228 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  228 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  228 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  228 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  229 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  229 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  229 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  229 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  229 , M_lda =  6  --->  Accuracy = 58.65%\n",
      "M_pca =  229 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  229 , M_lda =  8  --->  Accuracy = 67.31%\n",
      "M_pca =  229 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  229 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  229 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  229 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  229 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  229 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  229 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  229 , M_lda =  16  --->  Accuracy = 81.73%\n",
      "M_pca =  229 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  229 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  229 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  229 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  229 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  229 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  229 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  229 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  229 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  26  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  229 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  229 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  229 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  229 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  229 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  229 , M_lda =  34  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  229 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  229 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  229 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  229 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  229 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  229 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  229 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  229 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  229 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  229 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  230 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  230 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  230 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  230 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  230 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  230 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  230 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  230 , M_lda =  8  --->  Accuracy = 66.35%\n",
      "M_pca =  230 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  230 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  230 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  230 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  230 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  230 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  230 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  230 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  230 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  230 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  230 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  230 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  230 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  230 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  230 , M_lda =  23  --->  Accuracy = 86.54%\n",
      "M_pca =  230 , M_lda =  24  --->  Accuracy = 87.50%\n",
      "M_pca =  230 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  230 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  230 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  230 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  230 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  230 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  230 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  230 , M_lda =  32  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  230 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  230 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  230 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  230 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  230 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  41  --->  Accuracy = 90.38%\n",
      "M_pca =  230 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  43  --->  Accuracy = 91.35%\n",
      "M_pca =  230 , M_lda =  44  --->  Accuracy = 90.38%\n",
      "M_pca =  230 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  230 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  230 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  230 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  230 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  230 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  231 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  231 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  231 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  231 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  231 , M_lda =  6  --->  Accuracy = 60.58%\n",
      "M_pca =  231 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  231 , M_lda =  8  --->  Accuracy = 69.23%\n",
      "M_pca =  231 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  231 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  231 , M_lda =  11  --->  Accuracy = 75.96%\n",
      "M_pca =  231 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  231 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  231 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  231 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  231 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  231 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  231 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  231 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  231 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  231 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  231 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  231 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  231 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  231 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  231 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  231 , M_lda =  27  --->  Accuracy = 88.46%\n",
      "M_pca =  231 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  231 , M_lda =  29  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  231 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  231 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  231 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  231 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  231 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  231 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  231 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  41  --->  Accuracy = 87.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  231 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  231 , M_lda =  43  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  231 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  231 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  231 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  231 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  231 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  232 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  232 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  232 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  232 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  232 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  232 , M_lda =  7  --->  Accuracy = 64.42%\n",
      "M_pca =  232 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  232 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  232 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  232 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  232 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  232 , M_lda =  13  --->  Accuracy = 79.81%\n",
      "M_pca =  232 , M_lda =  14  --->  Accuracy = 80.77%\n",
      "M_pca =  232 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  232 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  232 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  232 , M_lda =  18  --->  Accuracy = 83.65%\n",
      "M_pca =  232 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  232 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  232 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  232 , M_lda =  22  --->  Accuracy = 84.62%\n",
      "M_pca =  232 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  232 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  232 , M_lda =  25  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  232 , M_lda =  27  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  232 , M_lda =  29  --->  Accuracy = 90.38%\n",
      "M_pca =  232 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  232 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  32  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  232 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  232 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  232 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  232 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  38  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  40  --->  Accuracy = 90.38%\n",
      "M_pca =  232 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  232 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  232 , M_lda =  44  --->  Accuracy = 91.35%\n",
      "M_pca =  232 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  232 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  232 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  232 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  232 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  233 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  233 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  233 , M_lda =  3  --->  Accuracy = 19.23%\n",
      "M_pca =  233 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  233 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  233 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  233 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  233 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  233 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  233 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  233 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  233 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  233 , M_lda =  13  --->  Accuracy = 80.77%\n",
      "M_pca =  233 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  233 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  233 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  233 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  233 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  233 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  233 , M_lda =  20  --->  Accuracy = 84.62%\n",
      "M_pca =  233 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  233 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  233 , M_lda =  23  --->  Accuracy = 85.58%\n",
      "M_pca =  233 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  233 , M_lda =  25  --->  Accuracy = 88.46%\n",
      "M_pca =  233 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  233 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  233 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  233 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  233 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  233 , M_lda =  31  --->  Accuracy = 89.42%\n",
      "M_pca =  233 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  233 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  233 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  233 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  233 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  233 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  233 , M_lda =  38  --->  Accuracy = 90.38%\n",
      "M_pca =  233 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  233 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  233 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  233 , M_lda =  42  --->  Accuracy = 90.38%\n",
      "M_pca =  233 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  233 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  233 , M_lda =  45  --->  Accuracy = 92.31%\n",
      "M_pca =  233 , M_lda =  46  --->  Accuracy = 91.35%\n",
      "M_pca =  233 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  233 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  233 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  233 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  233 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  234 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  234 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  234 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  234 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  234 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  234 , M_lda =  7  --->  Accuracy = 60.58%\n",
      "M_pca =  234 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  234 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  234 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  234 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  234 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  234 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  234 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  234 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  234 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  234 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  234 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  234 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  234 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  234 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  234 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  234 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  234 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  234 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  234 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  234 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  234 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  234 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  234 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  234 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  234 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  36  --->  Accuracy = 89.42%\n",
      "M_pca =  234 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  234 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  234 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  234 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  234 , M_lda =  47  --->  Accuracy = 89.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  234 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  234 , M_lda =  49  --->  Accuracy = 91.35%\n",
      "M_pca =  234 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  234 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  235 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  235 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  235 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  235 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  235 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  235 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  235 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  235 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  235 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  235 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  235 , M_lda =  11  --->  Accuracy = 75.00%\n",
      "M_pca =  235 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  235 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  235 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  235 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  235 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  235 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  235 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  235 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  235 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  235 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  235 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  235 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  235 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  235 , M_lda =  25  --->  Accuracy = 86.54%\n",
      "M_pca =  235 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  235 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  235 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  235 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  235 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  235 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  235 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  235 , M_lda =  33  --->  Accuracy = 90.38%\n",
      "M_pca =  235 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  235 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  235 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  235 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  235 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  235 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  235 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  235 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  235 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  235 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  235 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  235 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  235 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  235 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  235 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  235 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  235 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  235 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  236 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  236 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  236 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  236 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  236 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  236 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  236 , M_lda =  7  --->  Accuracy = 67.31%\n",
      "M_pca =  236 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  236 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  236 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  236 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  236 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  236 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  236 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  236 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  236 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  236 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  236 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  236 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  236 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  236 , M_lda =  21  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  236 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  236 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  236 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  236 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  236 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  236 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  236 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  236 , M_lda =  33  --->  Accuracy = 88.46%\n",
      "M_pca =  236 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  236 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  236 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  236 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  236 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  236 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  236 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  236 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  236 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  236 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  236 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  236 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  236 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  236 , M_lda =  51  --->  Accuracy = 91.35%\n",
      "M_pca =  237 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  237 , M_lda =  2  --->  Accuracy = 28.85%\n",
      "M_pca =  237 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  237 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  237 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  237 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  237 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  237 , M_lda =  8  --->  Accuracy = 65.38%\n",
      "M_pca =  237 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  237 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  237 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  237 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  237 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  237 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  237 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  237 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  237 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  237 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  237 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  237 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  237 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  237 , M_lda =  22  --->  Accuracy = 83.65%\n",
      "M_pca =  237 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  237 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  237 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  237 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  237 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  28  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  31  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  237 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  237 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  237 , M_lda =  36  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  237 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  237 , M_lda =  39  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  237 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  237 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  237 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  237 , M_lda =  46  --->  Accuracy = 90.38%\n",
      "M_pca =  237 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  237 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  237 , M_lda =  51  --->  Accuracy = 90.38%\n",
      "M_pca =  238 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  238 , M_lda =  2  --->  Accuracy = 17.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  238 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  238 , M_lda =  4  --->  Accuracy = 43.27%\n",
      "M_pca =  238 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  238 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  238 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  238 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  238 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  238 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  238 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  238 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  238 , M_lda =  13  --->  Accuracy = 76.92%\n",
      "M_pca =  238 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  238 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  238 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  238 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  238 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  238 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  238 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  238 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  238 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  238 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  238 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  238 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  238 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  238 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  238 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  238 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  238 , M_lda =  30  --->  Accuracy = 88.46%\n",
      "M_pca =  238 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  238 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  238 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  238 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  238 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  238 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  238 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  238 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  238 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  238 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  238 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  238 , M_lda =  42  --->  Accuracy = 89.42%\n",
      "M_pca =  238 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  238 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  238 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  238 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  238 , M_lda =  47  --->  Accuracy = 90.38%\n",
      "M_pca =  238 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  238 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  238 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  238 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  239 , M_lda =  1  --->  Accuracy = 11.54%\n",
      "M_pca =  239 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  239 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  239 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  239 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  239 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  239 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  239 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  239 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  239 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  239 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  239 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  239 , M_lda =  13  --->  Accuracy = 77.88%\n",
      "M_pca =  239 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  239 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  239 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  239 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  239 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  239 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  239 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  239 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  239 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  239 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  239 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  239 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  239 , M_lda =  26  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  239 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  239 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  239 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  239 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  239 , M_lda =  35  --->  Accuracy = 91.35%\n",
      "M_pca =  239 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  239 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  239 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  239 , M_lda =  39  --->  Accuracy = 89.42%\n",
      "M_pca =  239 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  41  --->  Accuracy = 91.35%\n",
      "M_pca =  239 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  239 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  239 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  239 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  239 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  239 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  239 , M_lda =  48  --->  Accuracy = 92.31%\n",
      "M_pca =  239 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  239 , M_lda =  50  --->  Accuracy = 90.38%\n",
      "M_pca =  239 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  240 , M_lda =  1  --->  Accuracy = 13.46%\n",
      "M_pca =  240 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  240 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  240 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  240 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  240 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  240 , M_lda =  7  --->  Accuracy = 65.38%\n",
      "M_pca =  240 , M_lda =  8  --->  Accuracy = 68.27%\n",
      "M_pca =  240 , M_lda =  9  --->  Accuracy = 73.08%\n",
      "M_pca =  240 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  240 , M_lda =  11  --->  Accuracy = 73.08%\n",
      "M_pca =  240 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  240 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  240 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  240 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  240 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  240 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  240 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  240 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  240 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  240 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  240 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  240 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  240 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  240 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  240 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  240 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  240 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  240 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  240 , M_lda =  30  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  240 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  240 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  240 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  240 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  240 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  240 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  240 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  240 , M_lda =  43  --->  Accuracy = 90.38%\n",
      "M_pca =  240 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  240 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  240 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  240 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  240 , M_lda =  48  --->  Accuracy = 90.38%\n",
      "M_pca =  240 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  240 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  240 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  241 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  241 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  241 , M_lda =  4  --->  Accuracy = 28.85%\n",
      "M_pca =  241 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  241 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  241 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  241 , M_lda =  8  --->  Accuracy = 64.42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  241 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  241 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  241 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  241 , M_lda =  12  --->  Accuracy = 76.92%\n",
      "M_pca =  241 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  241 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  241 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  241 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  241 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  241 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  241 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  241 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  241 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  241 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  241 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  241 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  241 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  241 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  241 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  241 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  241 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  241 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  241 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  241 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  241 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  241 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  241 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  241 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  241 , M_lda =  37  --->  Accuracy = 89.42%\n",
      "M_pca =  241 , M_lda =  38  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  241 , M_lda =  40  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  241 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  241 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  241 , M_lda =  44  --->  Accuracy = 89.42%\n",
      "M_pca =  241 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  241 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  241 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  241 , M_lda =  50  --->  Accuracy = 92.31%\n",
      "M_pca =  241 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  242 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  242 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  242 , M_lda =  3  --->  Accuracy = 36.54%\n",
      "M_pca =  242 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  242 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  242 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  242 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  242 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  242 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  242 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  242 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  242 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  242 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  242 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  242 , M_lda =  15  --->  Accuracy = 79.81%\n",
      "M_pca =  242 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  242 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  242 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  242 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  242 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  242 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  242 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  242 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  29  --->  Accuracy = 85.58%\n",
      "M_pca =  242 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  242 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  242 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  242 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  242 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  242 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  242 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  242 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  242 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  242 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  242 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  242 , M_lda =  41  --->  Accuracy = 89.42%\n",
      "M_pca =  242 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  242 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  242 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  242 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  242 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  242 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  242 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  242 , M_lda =  49  --->  Accuracy = 90.38%\n",
      "M_pca =  242 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  242 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  243 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  243 , M_lda =  3  --->  Accuracy = 32.69%\n",
      "M_pca =  243 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  243 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  243 , M_lda =  6  --->  Accuracy = 57.69%\n",
      "M_pca =  243 , M_lda =  7  --->  Accuracy = 61.54%\n",
      "M_pca =  243 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  243 , M_lda =  9  --->  Accuracy = 71.15%\n",
      "M_pca =  243 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  243 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  243 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  243 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  243 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  243 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  243 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  243 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  243 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  243 , M_lda =  19  --->  Accuracy = 84.62%\n",
      "M_pca =  243 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  243 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  243 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  243 , M_lda =  23  --->  Accuracy = 84.62%\n",
      "M_pca =  243 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  243 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  243 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  243 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  243 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  243 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  243 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  243 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  243 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  243 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  243 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  243 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  43  --->  Accuracy = 88.46%\n",
      "M_pca =  243 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  243 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  243 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  243 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  243 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  244 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  244 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  244 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  244 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  244 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  244 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  244 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  244 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  244 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  244 , M_lda =  10  --->  Accuracy = 66.35%\n",
      "M_pca =  244 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  244 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  244 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  244 , M_lda =  14  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  244 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  244 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  244 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  244 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  244 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  244 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  244 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  244 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  244 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  244 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  244 , M_lda =  37  --->  Accuracy = 88.46%\n",
      "M_pca =  244 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  244 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  244 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  244 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  244 , M_lda =  42  --->  Accuracy = 88.46%\n",
      "M_pca =  244 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  244 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  244 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  244 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  244 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  244 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  244 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  244 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  244 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  245 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  245 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  245 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  245 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  245 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  245 , M_lda =  6  --->  Accuracy = 59.62%\n",
      "M_pca =  245 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  245 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  245 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  245 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  245 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  245 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  245 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  245 , M_lda =  14  --->  Accuracy = 79.81%\n",
      "M_pca =  245 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  245 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  245 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  245 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  245 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  245 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  245 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  245 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  245 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  245 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  245 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  245 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  245 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  245 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  245 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  245 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  32  --->  Accuracy = 87.50%\n",
      "M_pca =  245 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  245 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  245 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  245 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  245 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  245 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  245 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  245 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  245 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  245 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  245 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  245 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  245 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  245 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  246 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  246 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  246 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  246 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  246 , M_lda =  5  --->  Accuracy = 50.00%\n",
      "M_pca =  246 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  246 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  246 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  246 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  246 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  246 , M_lda =  11  --->  Accuracy = 70.19%\n",
      "M_pca =  246 , M_lda =  12  --->  Accuracy = 66.35%\n",
      "M_pca =  246 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  246 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  246 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  246 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  246 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  246 , M_lda =  18  --->  Accuracy = 82.69%\n",
      "M_pca =  246 , M_lda =  19  --->  Accuracy = 83.65%\n",
      "M_pca =  246 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  246 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  246 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  246 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  246 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  246 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  246 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  246 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  246 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  246 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  246 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  246 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  246 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  246 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  246 , M_lda =  34  --->  Accuracy = 89.42%\n",
      "M_pca =  246 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  246 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  246 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  246 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  246 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  246 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  246 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  246 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  246 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  246 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  246 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  246 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  246 , M_lda =  47  --->  Accuracy = 89.42%\n",
      "M_pca =  246 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  246 , M_lda =  49  --->  Accuracy = 89.42%\n",
      "M_pca =  246 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  246 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  247 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  247 , M_lda =  2  --->  Accuracy = 21.15%\n",
      "M_pca =  247 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  247 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  247 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  247 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  247 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  247 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  247 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  247 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  247 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  247 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  247 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  247 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  247 , M_lda =  15  --->  Accuracy = 81.73%\n",
      "M_pca =  247 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  247 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  247 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  247 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  247 , M_lda =  20  --->  Accuracy = 78.85%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  247 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  247 , M_lda =  22  --->  Accuracy = 86.54%\n",
      "M_pca =  247 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  247 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  247 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  247 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  247 , M_lda =  27  --->  Accuracy = 86.54%\n",
      "M_pca =  247 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  247 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  247 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  247 , M_lda =  31  --->  Accuracy = 87.50%\n",
      "M_pca =  247 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  247 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  247 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  247 , M_lda =  35  --->  Accuracy = 90.38%\n",
      "M_pca =  247 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  247 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  247 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  247 , M_lda =  39  --->  Accuracy = 87.50%\n",
      "M_pca =  247 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  247 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  247 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  247 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  247 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  247 , M_lda =  45  --->  Accuracy = 90.38%\n",
      "M_pca =  247 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  247 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  247 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  247 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  247 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  247 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  248 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  248 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  248 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  248 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  248 , M_lda =  5  --->  Accuracy = 53.85%\n",
      "M_pca =  248 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  248 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  248 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  248 , M_lda =  9  --->  Accuracy = 65.38%\n",
      "M_pca =  248 , M_lda =  10  --->  Accuracy = 66.35%\n",
      "M_pca =  248 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  248 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  248 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  248 , M_lda =  14  --->  Accuracy = 78.85%\n",
      "M_pca =  248 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  248 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  248 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  248 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  248 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  248 , M_lda =  20  --->  Accuracy = 83.65%\n",
      "M_pca =  248 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  248 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  248 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  248 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  248 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  248 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  248 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  248 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  248 , M_lda =  29  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  248 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  248 , M_lda =  32  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  33  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  248 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  248 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  248 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  248 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  248 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  248 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  248 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  248 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  248 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  248 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  248 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  248 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  248 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  248 , M_lda =  51  --->  Accuracy = 89.42%\n",
      "M_pca =  249 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  249 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  249 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  249 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  249 , M_lda =  5  --->  Accuracy = 48.08%\n",
      "M_pca =  249 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  249 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  249 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  249 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  249 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  249 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  249 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  249 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  249 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  249 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  249 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  249 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  249 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  249 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  249 , M_lda =  20  --->  Accuracy = 82.69%\n",
      "M_pca =  249 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  249 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  249 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  249 , M_lda =  24  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  249 , M_lda =  26  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  249 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  249 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  249 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  249 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  249 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  249 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  249 , M_lda =  45  --->  Accuracy = 89.42%\n",
      "M_pca =  249 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  249 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  249 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  249 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  249 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  249 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  250 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  250 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  250 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  250 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  250 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  250 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  250 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  250 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  250 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  250 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  250 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  250 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  250 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  250 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  250 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  250 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  250 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  250 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  250 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  250 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  250 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  250 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  250 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  250 , M_lda =  26  --->  Accuracy = 79.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  250 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  250 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  250 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  31  --->  Accuracy = 85.58%\n",
      "M_pca =  250 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  250 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  250 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  250 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  250 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  250 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  250 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  250 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  250 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  250 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  250 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  250 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  250 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  250 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  250 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  250 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  250 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  250 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  250 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  251 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  251 , M_lda =  3  --->  Accuracy = 35.58%\n",
      "M_pca =  251 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  251 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  251 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  251 , M_lda =  7  --->  Accuracy = 63.46%\n",
      "M_pca =  251 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  251 , M_lda =  9  --->  Accuracy = 70.19%\n",
      "M_pca =  251 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  251 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  251 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  251 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  251 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  251 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  251 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  251 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  251 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  251 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  251 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  251 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  251 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  251 , M_lda =  23  --->  Accuracy = 83.65%\n",
      "M_pca =  251 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  251 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  251 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  251 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  251 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  251 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  251 , M_lda =  30  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  251 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  251 , M_lda =  33  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  251 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  251 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  251 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  251 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  251 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  251 , M_lda =  41  --->  Accuracy = 88.46%\n",
      "M_pca =  251 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  251 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  251 , M_lda =  45  --->  Accuracy = 88.46%\n",
      "M_pca =  251 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  251 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  251 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  251 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  251 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  251 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  252 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  252 , M_lda =  2  --->  Accuracy = 23.08%\n",
      "M_pca =  252 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  252 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  252 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  252 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  252 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  252 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  252 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  252 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  252 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  252 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  252 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  252 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  252 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  252 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  252 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  252 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  252 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  252 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  252 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  252 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  28  --->  Accuracy = 85.58%\n",
      "M_pca =  252 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  30  --->  Accuracy = 85.58%\n",
      "M_pca =  252 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  252 , M_lda =  32  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  252 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  252 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  252 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  252 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  252 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  252 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  252 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  252 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  252 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  252 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  252 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  252 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  252 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  252 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  253 , M_lda =  1  --->  Accuracy = 12.50%\n",
      "M_pca =  253 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  253 , M_lda =  3  --->  Accuracy = 30.77%\n",
      "M_pca =  253 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  253 , M_lda =  5  --->  Accuracy = 49.04%\n",
      "M_pca =  253 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  253 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  253 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  253 , M_lda =  9  --->  Accuracy = 69.23%\n",
      "M_pca =  253 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  253 , M_lda =  11  --->  Accuracy = 74.04%\n",
      "M_pca =  253 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  253 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  253 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  253 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  253 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  253 , M_lda =  17  --->  Accuracy = 80.77%\n",
      "M_pca =  253 , M_lda =  18  --->  Accuracy = 81.73%\n",
      "M_pca =  253 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  253 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  253 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  253 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  253 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  253 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  253 , M_lda =  25  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  253 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  253 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  253 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  253 , M_lda =  32  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  253 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  253 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  253 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  253 , M_lda =  37  --->  Accuracy = 87.50%\n",
      "M_pca =  253 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  253 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  253 , M_lda =  40  --->  Accuracy = 89.42%\n",
      "M_pca =  253 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  253 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  253 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  253 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  253 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  253 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  253 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  253 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  253 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  253 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  254 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  254 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  254 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  254 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  254 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  254 , M_lda =  6  --->  Accuracy = 50.96%\n",
      "M_pca =  254 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  254 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  254 , M_lda =  9  --->  Accuracy = 65.38%\n",
      "M_pca =  254 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  254 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  254 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  254 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  254 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  254 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  254 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  254 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  254 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  254 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  254 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  254 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  254 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  254 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  254 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  254 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  254 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  254 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  254 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  254 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  254 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  254 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  254 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  254 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  254 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  254 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  254 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  254 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  254 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  254 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  254 , M_lda =  46  --->  Accuracy = 88.46%\n",
      "M_pca =  254 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  254 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  254 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  254 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  254 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  255 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  255 , M_lda =  3  --->  Accuracy = 34.62%\n",
      "M_pca =  255 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  255 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  255 , M_lda =  6  --->  Accuracy = 55.77%\n",
      "M_pca =  255 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  255 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  255 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  255 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  255 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  255 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  255 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  255 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  255 , M_lda =  15  --->  Accuracy = 80.77%\n",
      "M_pca =  255 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  255 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  255 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  255 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  255 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  255 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  255 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  255 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  255 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  255 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  255 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  255 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  255 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  255 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  255 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  255 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  255 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  255 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  255 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  255 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  255 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  255 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  255 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  255 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  255 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  255 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  255 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  255 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  255 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  256 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  256 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  256 , M_lda =  3  --->  Accuracy = 33.65%\n",
      "M_pca =  256 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  256 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  256 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  256 , M_lda =  7  --->  Accuracy = 56.73%\n",
      "M_pca =  256 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  256 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  256 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  256 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  256 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  256 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  256 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  256 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  256 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  256 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  256 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  256 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  256 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  256 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  256 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  256 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  256 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  256 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  256 , M_lda =  31  --->  Accuracy = 86.54%\n",
      "M_pca =  256 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  256 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  256 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  256 , M_lda =  35  --->  Accuracy = 87.50%\n",
      "M_pca =  256 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  256 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  256 , M_lda =  38  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  256 , M_lda =  39  --->  Accuracy = 86.54%\n",
      "M_pca =  256 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  256 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  256 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  256 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  256 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  256 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  256 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  256 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  256 , M_lda =  48  --->  Accuracy = 89.42%\n",
      "M_pca =  256 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  256 , M_lda =  50  --->  Accuracy = 88.46%\n",
      "M_pca =  256 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  257 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  257 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  257 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  257 , M_lda =  4  --->  Accuracy = 42.31%\n",
      "M_pca =  257 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  257 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  257 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  257 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  257 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  257 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  257 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  257 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  257 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  257 , M_lda =  14  --->  Accuracy = 76.92%\n",
      "M_pca =  257 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  257 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  257 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  257 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  257 , M_lda =  19  --->  Accuracy = 81.73%\n",
      "M_pca =  257 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  257 , M_lda =  21  --->  Accuracy = 82.69%\n",
      "M_pca =  257 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  257 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  257 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  257 , M_lda =  25  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  257 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  28  --->  Accuracy = 84.62%\n",
      "M_pca =  257 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  257 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  257 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  32  --->  Accuracy = 85.58%\n",
      "M_pca =  257 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  257 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  35  --->  Accuracy = 88.46%\n",
      "M_pca =  257 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  257 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  257 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  257 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  257 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  257 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  257 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  257 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  257 , M_lda =  47  --->  Accuracy = 87.50%\n",
      "M_pca =  257 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  257 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  257 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  257 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  258 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  258 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  258 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  258 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  258 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  258 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  258 , M_lda =  8  --->  Accuracy = 62.50%\n",
      "M_pca =  258 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  258 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  258 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  258 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  258 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  258 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  258 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  258 , M_lda =  16  --->  Accuracy = 80.77%\n",
      "M_pca =  258 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  258 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  258 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  258 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  258 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  258 , M_lda =  22  --->  Accuracy = 85.58%\n",
      "M_pca =  258 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  258 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  258 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  258 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  258 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  258 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  258 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  258 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  258 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  258 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  258 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  258 , M_lda =  34  --->  Accuracy = 88.46%\n",
      "M_pca =  258 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  258 , M_lda =  36  --->  Accuracy = 87.50%\n",
      "M_pca =  258 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  38  --->  Accuracy = 87.50%\n",
      "M_pca =  258 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  258 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  258 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  258 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  258 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  258 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  258 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  259 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  259 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  259 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  259 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  259 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  259 , M_lda =  6  --->  Accuracy = 56.73%\n",
      "M_pca =  259 , M_lda =  7  --->  Accuracy = 58.65%\n",
      "M_pca =  259 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  259 , M_lda =  9  --->  Accuracy = 66.35%\n",
      "M_pca =  259 , M_lda =  10  --->  Accuracy = 71.15%\n",
      "M_pca =  259 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  259 , M_lda =  12  --->  Accuracy = 75.96%\n",
      "M_pca =  259 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  259 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  259 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  259 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  259 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  259 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  259 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  259 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  259 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  259 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  259 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  259 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  259 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  259 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  259 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  259 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  259 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  259 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  259 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  259 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  259 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  259 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  42  --->  Accuracy = 87.50%\n",
      "M_pca =  259 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  44  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  259 , M_lda =  45  --->  Accuracy = 87.50%\n",
      "M_pca =  259 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  259 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  259 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  259 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  259 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  260 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  260 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  260 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  260 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  260 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  260 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  260 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  260 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  260 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  260 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  260 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  260 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  260 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  260 , M_lda =  14  --->  Accuracy = 81.73%\n",
      "M_pca =  260 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  260 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  260 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  260 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  260 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  260 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  260 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  260 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  260 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  260 , M_lda =  24  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  25  --->  Accuracy = 75.96%\n",
      "M_pca =  260 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  260 , M_lda =  27  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  260 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  260 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  260 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  260 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  260 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  260 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  260 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  260 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  260 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  260 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  260 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  260 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  260 , M_lda =  44  --->  Accuracy = 88.46%\n",
      "M_pca =  260 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  260 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  260 , M_lda =  47  --->  Accuracy = 88.46%\n",
      "M_pca =  260 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  260 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  260 , M_lda =  50  --->  Accuracy = 89.42%\n",
      "M_pca =  260 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  261 , M_lda =  2  --->  Accuracy = 22.12%\n",
      "M_pca =  261 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  261 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  261 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  261 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  261 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  261 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  261 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  261 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  261 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  261 , M_lda =  12  --->  Accuracy = 75.00%\n",
      "M_pca =  261 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  261 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  261 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  261 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  261 , M_lda =  17  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  261 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  261 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  261 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  261 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  261 , M_lda =  23  --->  Accuracy = 82.69%\n",
      "M_pca =  261 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  261 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  261 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  261 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  261 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  261 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  261 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  261 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  261 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  261 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  261 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  261 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  261 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  261 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  262 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  262 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  262 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  262 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  262 , M_lda =  5  --->  Accuracy = 50.96%\n",
      "M_pca =  262 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  262 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  262 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  262 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  262 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  262 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  262 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  262 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  262 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  262 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  262 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  262 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  262 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  262 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  262 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  262 , M_lda =  21  --->  Accuracy = 81.73%\n",
      "M_pca =  262 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  262 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  262 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  262 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  262 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  262 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  262 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  262 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  262 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  262 , M_lda =  36  --->  Accuracy = 86.54%\n",
      "M_pca =  262 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  262 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  262 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  262 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  262 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  262 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  262 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  262 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  262 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  262 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  262 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  262 , M_lda =  50  --->  Accuracy = 86.54%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  262 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  263 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  263 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  263 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  263 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  263 , M_lda =  5  --->  Accuracy = 35.58%\n",
      "M_pca =  263 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  263 , M_lda =  7  --->  Accuracy = 59.62%\n",
      "M_pca =  263 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  263 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  263 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  263 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  263 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  263 , M_lda =  13  --->  Accuracy = 75.96%\n",
      "M_pca =  263 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  263 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  263 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  263 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  263 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  263 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  263 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  263 , M_lda =  21  --->  Accuracy = 85.58%\n",
      "M_pca =  263 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  263 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  263 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  263 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  263 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  263 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  263 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  263 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  263 , M_lda =  31  --->  Accuracy = 84.62%\n",
      "M_pca =  263 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  33  --->  Accuracy = 85.58%\n",
      "M_pca =  263 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  263 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  263 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  263 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  263 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  263 , M_lda =  41  --->  Accuracy = 87.50%\n",
      "M_pca =  263 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  263 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  263 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  263 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  263 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  263 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  263 , M_lda =  50  --->  Accuracy = 87.50%\n",
      "M_pca =  263 , M_lda =  51  --->  Accuracy = 87.50%\n",
      "M_pca =  264 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  264 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  264 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  264 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  264 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  264 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  264 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  264 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  264 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  264 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  264 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  264 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  264 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  264 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  264 , M_lda =  15  --->  Accuracy = 78.85%\n",
      "M_pca =  264 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  264 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  264 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  264 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  264 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  264 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  264 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  264 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  264 , M_lda =  24  --->  Accuracy = 83.65%\n",
      "M_pca =  264 , M_lda =  25  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  264 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  264 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  264 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  264 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  264 , M_lda =  34  --->  Accuracy = 86.54%\n",
      "M_pca =  264 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  264 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  264 , M_lda =  38  --->  Accuracy = 85.58%\n",
      "M_pca =  264 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  264 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  264 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  264 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  264 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  264 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  264 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  264 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  264 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  264 , M_lda =  48  --->  Accuracy = 88.46%\n",
      "M_pca =  264 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  264 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  264 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  265 , M_lda =  1  --->  Accuracy = 10.58%\n",
      "M_pca =  265 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  265 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  265 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  265 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  265 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  265 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  265 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  265 , M_lda =  9  --->  Accuracy = 61.54%\n",
      "M_pca =  265 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  265 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  265 , M_lda =  12  --->  Accuracy = 73.08%\n",
      "M_pca =  265 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  265 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  265 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  265 , M_lda =  16  --->  Accuracy = 78.85%\n",
      "M_pca =  265 , M_lda =  17  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  265 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  265 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  265 , M_lda =  21  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  22  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  265 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  265 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  265 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  265 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  265 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  265 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  265 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  265 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  265 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  265 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  43  --->  Accuracy = 87.50%\n",
      "M_pca =  265 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  265 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  265 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  265 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  265 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  265 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  266 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  266 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  266 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  266 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  266 , M_lda =  5  --->  Accuracy = 45.19%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  266 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  266 , M_lda =  7  --->  Accuracy = 51.92%\n",
      "M_pca =  266 , M_lda =  8  --->  Accuracy = 63.46%\n",
      "M_pca =  266 , M_lda =  9  --->  Accuracy = 60.58%\n",
      "M_pca =  266 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  266 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  266 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  266 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  266 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  266 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  266 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  266 , M_lda =  17  --->  Accuracy = 76.92%\n",
      "M_pca =  266 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  266 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  266 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  266 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  266 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  266 , M_lda =  23  --->  Accuracy = 81.73%\n",
      "M_pca =  266 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  266 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  266 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  266 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  266 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  266 , M_lda =  29  --->  Accuracy = 84.62%\n",
      "M_pca =  266 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  266 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  266 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  266 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  266 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  266 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  266 , M_lda =  36  --->  Accuracy = 85.58%\n",
      "M_pca =  266 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  266 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  266 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  266 , M_lda =  40  --->  Accuracy = 84.62%\n",
      "M_pca =  266 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  266 , M_lda =  42  --->  Accuracy = 86.54%\n",
      "M_pca =  266 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  266 , M_lda =  44  --->  Accuracy = 87.50%\n",
      "M_pca =  266 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  266 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  266 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  266 , M_lda =  48  --->  Accuracy = 87.50%\n",
      "M_pca =  266 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  266 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  266 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  267 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  267 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  267 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  267 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  267 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  267 , M_lda =  6  --->  Accuracy = 54.81%\n",
      "M_pca =  267 , M_lda =  7  --->  Accuracy = 55.77%\n",
      "M_pca =  267 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  267 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  267 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  267 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  267 , M_lda =  12  --->  Accuracy = 66.35%\n",
      "M_pca =  267 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  267 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  267 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  267 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  267 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  267 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  267 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  267 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  267 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  267 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  267 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  267 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  267 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  267 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  267 , M_lda =  27  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  267 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  267 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  267 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  267 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  267 , M_lda =  37  --->  Accuracy = 86.54%\n",
      "M_pca =  267 , M_lda =  38  --->  Accuracy = 84.62%\n",
      "M_pca =  267 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  267 , M_lda =  40  --->  Accuracy = 87.50%\n",
      "M_pca =  267 , M_lda =  41  --->  Accuracy = 84.62%\n",
      "M_pca =  267 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  267 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  267 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  267 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  267 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  267 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  267 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  267 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  267 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  267 , M_lda =  51  --->  Accuracy = 86.54%\n",
      "M_pca =  268 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  268 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  268 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  268 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  268 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  268 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  268 , M_lda =  7  --->  Accuracy = 50.96%\n",
      "M_pca =  268 , M_lda =  8  --->  Accuracy = 64.42%\n",
      "M_pca =  268 , M_lda =  9  --->  Accuracy = 65.38%\n",
      "M_pca =  268 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  268 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  268 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  268 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  268 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  268 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  268 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  268 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  268 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  268 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  268 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  268 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  268 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  268 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  268 , M_lda =  24  --->  Accuracy = 82.69%\n",
      "M_pca =  268 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  268 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  268 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  268 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  268 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  268 , M_lda =  30  --->  Accuracy = 84.62%\n",
      "M_pca =  268 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  268 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  268 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  268 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  268 , M_lda =  35  --->  Accuracy = 86.54%\n",
      "M_pca =  268 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  268 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  268 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  268 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  268 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  268 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  268 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  268 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  268 , M_lda =  44  --->  Accuracy = 86.54%\n",
      "M_pca =  268 , M_lda =  45  --->  Accuracy = 85.58%\n",
      "M_pca =  268 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  268 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  268 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  268 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  268 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  268 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  269 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  269 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  269 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  269 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  269 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  269 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  269 , M_lda =  7  --->  Accuracy = 48.08%\n",
      "M_pca =  269 , M_lda =  8  --->  Accuracy = 61.54%\n",
      "M_pca =  269 , M_lda =  9  --->  Accuracy = 60.58%\n",
      "M_pca =  269 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  269 , M_lda =  11  --->  Accuracy = 67.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  269 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  269 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  269 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  269 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  269 , M_lda =  16  --->  Accuracy = 79.81%\n",
      "M_pca =  269 , M_lda =  17  --->  Accuracy = 79.81%\n",
      "M_pca =  269 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  269 , M_lda =  19  --->  Accuracy = 82.69%\n",
      "M_pca =  269 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  269 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  269 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  269 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  269 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  269 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  269 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  269 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  269 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  269 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  31  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  269 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  269 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  269 , M_lda =  35  --->  Accuracy = 85.58%\n",
      "M_pca =  269 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  269 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  269 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  269 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  269 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  269 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  269 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  269 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  269 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  269 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  269 , M_lda =  49  --->  Accuracy = 88.46%\n",
      "M_pca =  269 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  269 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  270 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  270 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  270 , M_lda =  3  --->  Accuracy = 26.92%\n",
      "M_pca =  270 , M_lda =  4  --->  Accuracy = 40.38%\n",
      "M_pca =  270 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  270 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  270 , M_lda =  7  --->  Accuracy = 51.92%\n",
      "M_pca =  270 , M_lda =  8  --->  Accuracy = 52.88%\n",
      "M_pca =  270 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  270 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  270 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  270 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  270 , M_lda =  13  --->  Accuracy = 75.00%\n",
      "M_pca =  270 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  270 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  270 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  270 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  270 , M_lda =  18  --->  Accuracy = 80.77%\n",
      "M_pca =  270 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  270 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  270 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  270 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  270 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  270 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  270 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  270 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  270 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  270 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  270 , M_lda =  29  --->  Accuracy = 82.69%\n",
      "M_pca =  270 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  270 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  270 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  34  --->  Accuracy = 85.58%\n",
      "M_pca =  270 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  270 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  270 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  40  --->  Accuracy = 86.54%\n",
      "M_pca =  270 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  270 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  270 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  270 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  270 , M_lda =  49  --->  Accuracy = 87.50%\n",
      "M_pca =  270 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  270 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  271 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  271 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  271 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  271 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  271 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  271 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  271 , M_lda =  7  --->  Accuracy = 62.50%\n",
      "M_pca =  271 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  271 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  271 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  271 , M_lda =  11  --->  Accuracy = 72.12%\n",
      "M_pca =  271 , M_lda =  12  --->  Accuracy = 64.42%\n",
      "M_pca =  271 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  271 , M_lda =  14  --->  Accuracy = 74.04%\n",
      "M_pca =  271 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  271 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  271 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  271 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  271 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  271 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  271 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  271 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  271 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  271 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  271 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  271 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  271 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  271 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  271 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  271 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  271 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  271 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  271 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  271 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  271 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  271 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  271 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  271 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  271 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  271 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  271 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  271 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  271 , M_lda =  50  --->  Accuracy = 85.58%\n",
      "M_pca =  271 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  272 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  272 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  272 , M_lda =  3  --->  Accuracy = 29.81%\n",
      "M_pca =  272 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  272 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  272 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  272 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  272 , M_lda =  8  --->  Accuracy = 57.69%\n",
      "M_pca =  272 , M_lda =  9  --->  Accuracy = 62.50%\n",
      "M_pca =  272 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  272 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  272 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  272 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  272 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  272 , M_lda =  15  --->  Accuracy = 77.88%\n",
      "M_pca =  272 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  272 , M_lda =  17  --->  Accuracy = 75.96%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  272 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  272 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  272 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  272 , M_lda =  21  --->  Accuracy = 80.77%\n",
      "M_pca =  272 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  272 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  272 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  272 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  272 , M_lda =  26  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  272 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  272 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  272 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  272 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  272 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  272 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  272 , M_lda =  35  --->  Accuracy = 84.62%\n",
      "M_pca =  272 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  272 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  272 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  272 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  272 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  43  --->  Accuracy = 85.58%\n",
      "M_pca =  272 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  272 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  272 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  272 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  272 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  272 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  272 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  272 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  273 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  273 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  273 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  273 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  273 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  273 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  273 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  273 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  273 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  273 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  273 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  273 , M_lda =  12  --->  Accuracy = 65.38%\n",
      "M_pca =  273 , M_lda =  13  --->  Accuracy = 73.08%\n",
      "M_pca =  273 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  273 , M_lda =  15  --->  Accuracy = 75.96%\n",
      "M_pca =  273 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  273 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  273 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  273 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  273 , M_lda =  20  --->  Accuracy = 79.81%\n",
      "M_pca =  273 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  273 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  273 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  273 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  273 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  273 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  273 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  273 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  273 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  273 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  273 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  273 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  273 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  273 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  36  --->  Accuracy = 84.62%\n",
      "M_pca =  273 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  273 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  273 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  273 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  43  --->  Accuracy = 84.62%\n",
      "M_pca =  273 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  273 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  273 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  273 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  273 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  273 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  273 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  273 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  274 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  274 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  274 , M_lda =  4  --->  Accuracy = 41.35%\n",
      "M_pca =  274 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  274 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  274 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  274 , M_lda =  8  --->  Accuracy = 58.65%\n",
      "M_pca =  274 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  274 , M_lda =  10  --->  Accuracy = 66.35%\n",
      "M_pca =  274 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  274 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  274 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  274 , M_lda =  14  --->  Accuracy = 75.96%\n",
      "M_pca =  274 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  274 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  274 , M_lda =  17  --->  Accuracy = 77.88%\n",
      "M_pca =  274 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  274 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  274 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  274 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  274 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  274 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  274 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  274 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  274 , M_lda =  26  --->  Accuracy = 76.92%\n",
      "M_pca =  274 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  274 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  274 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  274 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  274 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  274 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  274 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  274 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  274 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  274 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  274 , M_lda =  39  --->  Accuracy = 85.58%\n",
      "M_pca =  274 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  274 , M_lda =  41  --->  Accuracy = 86.54%\n",
      "M_pca =  274 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  274 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  274 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  46  --->  Accuracy = 86.54%\n",
      "M_pca =  274 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  274 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  274 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  274 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  274 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  275 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  275 , M_lda =  2  --->  Accuracy = 20.19%\n",
      "M_pca =  275 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  275 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  275 , M_lda =  5  --->  Accuracy = 46.15%\n",
      "M_pca =  275 , M_lda =  6  --->  Accuracy = 46.15%\n",
      "M_pca =  275 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  275 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  275 , M_lda =  9  --->  Accuracy = 68.27%\n",
      "M_pca =  275 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  275 , M_lda =  11  --->  Accuracy = 71.15%\n",
      "M_pca =  275 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  275 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  275 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  275 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  275 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  275 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  275 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  275 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  275 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  275 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  275 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  275 , M_lda =  23  --->  Accuracy = 79.81%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  275 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  275 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  275 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  275 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  275 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  275 , M_lda =  29  --->  Accuracy = 78.85%\n",
      "M_pca =  275 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  275 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  275 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  33  --->  Accuracy = 78.85%\n",
      "M_pca =  275 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  275 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  275 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  39  --->  Accuracy = 84.62%\n",
      "M_pca =  275 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  275 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  275 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  275 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  275 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  275 , M_lda =  46  --->  Accuracy = 85.58%\n",
      "M_pca =  275 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  275 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  275 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  276 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  276 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  276 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  276 , M_lda =  5  --->  Accuracy = 47.12%\n",
      "M_pca =  276 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  276 , M_lda =  7  --->  Accuracy = 56.73%\n",
      "M_pca =  276 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  276 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  276 , M_lda =  10  --->  Accuracy = 67.31%\n",
      "M_pca =  276 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  276 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  276 , M_lda =  13  --->  Accuracy = 74.04%\n",
      "M_pca =  276 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  276 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  276 , M_lda =  16  --->  Accuracy = 76.92%\n",
      "M_pca =  276 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  276 , M_lda =  18  --->  Accuracy = 79.81%\n",
      "M_pca =  276 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  276 , M_lda =  20  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  276 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  276 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  276 , M_lda =  24  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  276 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  276 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  276 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  276 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  276 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  37  --->  Accuracy = 84.62%\n",
      "M_pca =  276 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  276 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  276 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  276 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  276 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  276 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  276 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  276 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  276 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  276 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  276 , M_lda =  51  --->  Accuracy = 88.46%\n",
      "M_pca =  277 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  277 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  277 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  277 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  277 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  277 , M_lda =  6  --->  Accuracy = 53.85%\n",
      "M_pca =  277 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  277 , M_lda =  8  --->  Accuracy = 57.69%\n",
      "M_pca =  277 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  277 , M_lda =  10  --->  Accuracy = 69.23%\n",
      "M_pca =  277 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  277 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  277 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  277 , M_lda =  14  --->  Accuracy = 72.12%\n",
      "M_pca =  277 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  277 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  277 , M_lda =  17  --->  Accuracy = 78.85%\n",
      "M_pca =  277 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  277 , M_lda =  19  --->  Accuracy = 80.77%\n",
      "M_pca =  277 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  277 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  277 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  277 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  277 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  277 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  277 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  277 , M_lda =  27  --->  Accuracy = 82.69%\n",
      "M_pca =  277 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  277 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  277 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  277 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  277 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  277 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  277 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  277 , M_lda =  38  --->  Accuracy = 86.54%\n",
      "M_pca =  277 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  277 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  277 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  277 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  277 , M_lda =  45  --->  Accuracy = 86.54%\n",
      "M_pca =  277 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  277 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  277 , M_lda =  48  --->  Accuracy = 86.54%\n",
      "M_pca =  277 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  277 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  277 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  278 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  278 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  278 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  278 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  278 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  278 , M_lda =  6  --->  Accuracy = 51.92%\n",
      "M_pca =  278 , M_lda =  7  --->  Accuracy = 55.77%\n",
      "M_pca =  278 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  278 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  278 , M_lda =  10  --->  Accuracy = 66.35%\n",
      "M_pca =  278 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  278 , M_lda =  12  --->  Accuracy = 64.42%\n",
      "M_pca =  278 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  278 , M_lda =  14  --->  Accuracy = 67.31%\n",
      "M_pca =  278 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  278 , M_lda =  16  --->  Accuracy = 77.88%\n",
      "M_pca =  278 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  278 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  278 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  278 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  278 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  278 , M_lda =  22  --->  Accuracy = 81.73%\n",
      "M_pca =  278 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  278 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  278 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  278 , M_lda =  29  --->  Accuracy = 80.77%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  278 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  278 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  278 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  278 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  278 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  278 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  278 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  278 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  278 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  278 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  278 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  278 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  278 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  278 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  278 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  278 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  278 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  278 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  278 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  279 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  279 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  279 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  279 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  279 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  279 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  279 , M_lda =  8  --->  Accuracy = 55.77%\n",
      "M_pca =  279 , M_lda =  9  --->  Accuracy = 67.31%\n",
      "M_pca =  279 , M_lda =  10  --->  Accuracy = 65.38%\n",
      "M_pca =  279 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  279 , M_lda =  12  --->  Accuracy = 72.12%\n",
      "M_pca =  279 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  279 , M_lda =  14  --->  Accuracy = 72.12%\n",
      "M_pca =  279 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  279 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  279 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  279 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  279 , M_lda =  19  --->  Accuracy = 79.81%\n",
      "M_pca =  279 , M_lda =  20  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  279 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  279 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  279 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  279 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  279 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  279 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  279 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  279 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  279 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  279 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  279 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  279 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  279 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  279 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  279 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  279 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  280 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  280 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  280 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  280 , M_lda =  4  --->  Accuracy = 38.46%\n",
      "M_pca =  280 , M_lda =  5  --->  Accuracy = 45.19%\n",
      "M_pca =  280 , M_lda =  6  --->  Accuracy = 52.88%\n",
      "M_pca =  280 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  280 , M_lda =  8  --->  Accuracy = 52.88%\n",
      "M_pca =  280 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  280 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  280 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  280 , M_lda =  12  --->  Accuracy = 74.04%\n",
      "M_pca =  280 , M_lda =  13  --->  Accuracy = 68.27%\n",
      "M_pca =  280 , M_lda =  14  --->  Accuracy = 77.88%\n",
      "M_pca =  280 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  280 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  280 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  280 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  280 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  280 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  280 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  280 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  280 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  280 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  280 , M_lda =  25  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  26  --->  Accuracy = 76.92%\n",
      "M_pca =  280 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  280 , M_lda =  28  --->  Accuracy = 83.65%\n",
      "M_pca =  280 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  280 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  280 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  280 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  280 , M_lda =  41  --->  Accuracy = 85.58%\n",
      "M_pca =  280 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  280 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  280 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  280 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  280 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  280 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  280 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  280 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  281 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  281 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  281 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  281 , M_lda =  4  --->  Accuracy = 37.50%\n",
      "M_pca =  281 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  281 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  281 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  281 , M_lda =  8  --->  Accuracy = 50.00%\n",
      "M_pca =  281 , M_lda =  9  --->  Accuracy = 55.77%\n",
      "M_pca =  281 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  281 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  281 , M_lda =  12  --->  Accuracy = 71.15%\n",
      "M_pca =  281 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  281 , M_lda =  14  --->  Accuracy = 73.08%\n",
      "M_pca =  281 , M_lda =  15  --->  Accuracy = 70.19%\n",
      "M_pca =  281 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  281 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  281 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  281 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  281 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  281 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  281 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  281 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  281 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  281 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  281 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  281 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  281 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  281 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  281 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  35  --->  Accuracy = 82.69%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  281 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  281 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  281 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  281 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  281 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  281 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  281 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  281 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  281 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  281 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  281 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  282 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  282 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  282 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  282 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  282 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  282 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  282 , M_lda =  7  --->  Accuracy = 57.69%\n",
      "M_pca =  282 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  282 , M_lda =  9  --->  Accuracy = 61.54%\n",
      "M_pca =  282 , M_lda =  10  --->  Accuracy = 66.35%\n",
      "M_pca =  282 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  282 , M_lda =  12  --->  Accuracy = 66.35%\n",
      "M_pca =  282 , M_lda =  13  --->  Accuracy = 66.35%\n",
      "M_pca =  282 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  282 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  282 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  282 , M_lda =  17  --->  Accuracy = 75.96%\n",
      "M_pca =  282 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  282 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  282 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  282 , M_lda =  21  --->  Accuracy = 78.85%\n",
      "M_pca =  282 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  282 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  282 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  282 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  282 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  282 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  33  --->  Accuracy = 84.62%\n",
      "M_pca =  282 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  282 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  282 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  42  --->  Accuracy = 85.58%\n",
      "M_pca =  282 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  282 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  282 , M_lda =  46  --->  Accuracy = 84.62%\n",
      "M_pca =  282 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  282 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  282 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  282 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  282 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  283 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  283 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  283 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  283 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  283 , M_lda =  5  --->  Accuracy = 44.23%\n",
      "M_pca =  283 , M_lda =  6  --->  Accuracy = 42.31%\n",
      "M_pca =  283 , M_lda =  7  --->  Accuracy = 55.77%\n",
      "M_pca =  283 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  283 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  283 , M_lda =  10  --->  Accuracy = 68.27%\n",
      "M_pca =  283 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  283 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  283 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  283 , M_lda =  14  --->  Accuracy = 72.12%\n",
      "M_pca =  283 , M_lda =  15  --->  Accuracy = 70.19%\n",
      "M_pca =  283 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  283 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  283 , M_lda =  18  --->  Accuracy = 78.85%\n",
      "M_pca =  283 , M_lda =  19  --->  Accuracy = 76.92%\n",
      "M_pca =  283 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  283 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  283 , M_lda =  22  --->  Accuracy = 79.81%\n",
      "M_pca =  283 , M_lda =  23  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  283 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  283 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  283 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  283 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  283 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  283 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  283 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  283 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  283 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  283 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  283 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  283 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  283 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  283 , M_lda =  49  --->  Accuracy = 79.81%\n",
      "M_pca =  283 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  283 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  284 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  284 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  284 , M_lda =  3  --->  Accuracy = 31.73%\n",
      "M_pca =  284 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  284 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  284 , M_lda =  6  --->  Accuracy = 43.27%\n",
      "M_pca =  284 , M_lda =  7  --->  Accuracy = 55.77%\n",
      "M_pca =  284 , M_lda =  8  --->  Accuracy = 51.92%\n",
      "M_pca =  284 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  284 , M_lda =  10  --->  Accuracy = 58.65%\n",
      "M_pca =  284 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  284 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  284 , M_lda =  13  --->  Accuracy = 66.35%\n",
      "M_pca =  284 , M_lda =  14  --->  Accuracy = 70.19%\n",
      "M_pca =  284 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  284 , M_lda =  16  --->  Accuracy = 70.19%\n",
      "M_pca =  284 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  284 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  284 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  284 , M_lda =  20  --->  Accuracy = 78.85%\n",
      "M_pca =  284 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  284 , M_lda =  22  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  284 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  284 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  284 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  284 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  284 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  284 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  284 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  284 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  284 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  284 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  284 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  284 , M_lda =  41  --->  Accuracy = 83.65%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  284 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  284 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  284 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  284 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  284 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  284 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  284 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  284 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  284 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  284 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  285 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  285 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  285 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  285 , M_lda =  4  --->  Accuracy = 25.00%\n",
      "M_pca =  285 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  285 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  285 , M_lda =  7  --->  Accuracy = 54.81%\n",
      "M_pca =  285 , M_lda =  8  --->  Accuracy = 51.92%\n",
      "M_pca =  285 , M_lda =  9  --->  Accuracy = 53.85%\n",
      "M_pca =  285 , M_lda =  10  --->  Accuracy = 70.19%\n",
      "M_pca =  285 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  285 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  285 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  285 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  285 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  285 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  285 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  285 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  285 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  285 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  285 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  285 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  285 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  285 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  285 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  285 , M_lda =  26  --->  Accuracy = 83.65%\n",
      "M_pca =  285 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  285 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  285 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  285 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  285 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  285 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  285 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  285 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  285 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  285 , M_lda =  48  --->  Accuracy = 85.58%\n",
      "M_pca =  285 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  285 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  286 , M_lda =  1  --->  Accuracy = 13.46%\n",
      "M_pca =  286 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  286 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  286 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  286 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  286 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  286 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  286 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  286 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  286 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  286 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  286 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  286 , M_lda =  13  --->  Accuracy = 72.12%\n",
      "M_pca =  286 , M_lda =  14  --->  Accuracy = 75.00%\n",
      "M_pca =  286 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  286 , M_lda =  16  --->  Accuracy = 69.23%\n",
      "M_pca =  286 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  286 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  286 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  286 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  286 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  286 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  286 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  286 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  286 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  286 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  286 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  286 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  286 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  286 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  286 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  286 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  286 , M_lda =  33  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  286 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  286 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  286 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  286 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  286 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  286 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  286 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  286 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  47  --->  Accuracy = 82.69%\n",
      "M_pca =  286 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  286 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  286 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  286 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  287 , M_lda =  1  --->  Accuracy = 1.92%\n",
      "M_pca =  287 , M_lda =  2  --->  Accuracy = 8.65%\n",
      "M_pca =  287 , M_lda =  3  --->  Accuracy = 28.85%\n",
      "M_pca =  287 , M_lda =  4  --->  Accuracy = 30.77%\n",
      "M_pca =  287 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  287 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  287 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  287 , M_lda =  8  --->  Accuracy = 57.69%\n",
      "M_pca =  287 , M_lda =  9  --->  Accuracy = 60.58%\n",
      "M_pca =  287 , M_lda =  10  --->  Accuracy = 59.62%\n",
      "M_pca =  287 , M_lda =  11  --->  Accuracy = 69.23%\n",
      "M_pca =  287 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  287 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  287 , M_lda =  14  --->  Accuracy = 67.31%\n",
      "M_pca =  287 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  287 , M_lda =  16  --->  Accuracy = 70.19%\n",
      "M_pca =  287 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  287 , M_lda =  18  --->  Accuracy = 76.92%\n",
      "M_pca =  287 , M_lda =  19  --->  Accuracy = 74.04%\n",
      "M_pca =  287 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  287 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  287 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  287 , M_lda =  23  --->  Accuracy = 79.81%\n",
      "M_pca =  287 , M_lda =  24  --->  Accuracy = 75.96%\n",
      "M_pca =  287 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  287 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  287 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  287 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  287 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  287 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  287 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  287 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  287 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  287 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  287 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  287 , M_lda =  37  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  40  --->  Accuracy = 85.58%\n",
      "M_pca =  287 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  287 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  287 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  287 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  287 , M_lda =  47  --->  Accuracy = 85.58%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  287 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  287 , M_lda =  49  --->  Accuracy = 85.58%\n",
      "M_pca =  287 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  287 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  288 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  288 , M_lda =  2  --->  Accuracy = 19.23%\n",
      "M_pca =  288 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  288 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  288 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  288 , M_lda =  6  --->  Accuracy = 46.15%\n",
      "M_pca =  288 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  288 , M_lda =  8  --->  Accuracy = 55.77%\n",
      "M_pca =  288 , M_lda =  9  --->  Accuracy = 53.85%\n",
      "M_pca =  288 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  288 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  288 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  288 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  288 , M_lda =  14  --->  Accuracy = 69.23%\n",
      "M_pca =  288 , M_lda =  15  --->  Accuracy = 76.92%\n",
      "M_pca =  288 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  288 , M_lda =  17  --->  Accuracy = 75.00%\n",
      "M_pca =  288 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  288 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  288 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  288 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  288 , M_lda =  22  --->  Accuracy = 75.00%\n",
      "M_pca =  288 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  288 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  288 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  27  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  29  --->  Accuracy = 83.65%\n",
      "M_pca =  288 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  37  --->  Accuracy = 83.65%\n",
      "M_pca =  288 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  288 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  288 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  288 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  288 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  288 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  288 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  288 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  288 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  289 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  289 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  289 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  289 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  289 , M_lda =  6  --->  Accuracy = 41.35%\n",
      "M_pca =  289 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  289 , M_lda =  8  --->  Accuracy = 59.62%\n",
      "M_pca =  289 , M_lda =  9  --->  Accuracy = 58.65%\n",
      "M_pca =  289 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  289 , M_lda =  11  --->  Accuracy = 65.38%\n",
      "M_pca =  289 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  289 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  289 , M_lda =  14  --->  Accuracy = 69.23%\n",
      "M_pca =  289 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  289 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  289 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  289 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  289 , M_lda =  19  --->  Accuracy = 78.85%\n",
      "M_pca =  289 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  289 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  289 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  289 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  289 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  289 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  27  --->  Accuracy = 76.92%\n",
      "M_pca =  289 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  29  --->  Accuracy = 76.92%\n",
      "M_pca =  289 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  289 , M_lda =  31  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  289 , M_lda =  35  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  289 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  289 , M_lda =  38  --->  Accuracy = 83.65%\n",
      "M_pca =  289 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  289 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  289 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  289 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  289 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  289 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  289 , M_lda =  47  --->  Accuracy = 86.54%\n",
      "M_pca =  289 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  289 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  289 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  289 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  290 , M_lda =  1  --->  Accuracy = 5.77%\n",
      "M_pca =  290 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  290 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  290 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  290 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  290 , M_lda =  6  --->  Accuracy = 46.15%\n",
      "M_pca =  290 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  290 , M_lda =  8  --->  Accuracy = 50.96%\n",
      "M_pca =  290 , M_lda =  9  --->  Accuracy = 63.46%\n",
      "M_pca =  290 , M_lda =  10  --->  Accuracy = 62.50%\n",
      "M_pca =  290 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  290 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  290 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  290 , M_lda =  14  --->  Accuracy = 64.42%\n",
      "M_pca =  290 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  290 , M_lda =  16  --->  Accuracy = 69.23%\n",
      "M_pca =  290 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  290 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  290 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  290 , M_lda =  20  --->  Accuracy = 70.19%\n",
      "M_pca =  290 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  290 , M_lda =  22  --->  Accuracy = 74.04%\n",
      "M_pca =  290 , M_lda =  23  --->  Accuracy = 77.88%\n",
      "M_pca =  290 , M_lda =  24  --->  Accuracy = 76.92%\n",
      "M_pca =  290 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  290 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  290 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  290 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  290 , M_lda =  30  --->  Accuracy = 82.69%\n",
      "M_pca =  290 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  290 , M_lda =  32  --->  Accuracy = 82.69%\n",
      "M_pca =  290 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  36  --->  Accuracy = 82.69%\n",
      "M_pca =  290 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  290 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  290 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  290 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  290 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  290 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  290 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  290 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  290 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  290 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  290 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  290 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  290 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  290 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  291 , M_lda =  1  --->  Accuracy = 9.62%\n",
      "M_pca =  291 , M_lda =  2  --->  Accuracy = 13.46%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  291 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  291 , M_lda =  4  --->  Accuracy = 29.81%\n",
      "M_pca =  291 , M_lda =  5  --->  Accuracy = 43.27%\n",
      "M_pca =  291 , M_lda =  6  --->  Accuracy = 37.50%\n",
      "M_pca =  291 , M_lda =  7  --->  Accuracy = 55.77%\n",
      "M_pca =  291 , M_lda =  8  --->  Accuracy = 60.58%\n",
      "M_pca =  291 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  291 , M_lda =  10  --->  Accuracy = 58.65%\n",
      "M_pca =  291 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  291 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  291 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  291 , M_lda =  14  --->  Accuracy = 70.19%\n",
      "M_pca =  291 , M_lda =  15  --->  Accuracy = 75.00%\n",
      "M_pca =  291 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  291 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  291 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  291 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  291 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  291 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  291 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  291 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  291 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  291 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  291 , M_lda =  26  --->  Accuracy = 80.77%\n",
      "M_pca =  291 , M_lda =  27  --->  Accuracy = 76.92%\n",
      "M_pca =  291 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  291 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  291 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  291 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  291 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  291 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  291 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  291 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  291 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  291 , M_lda =  37  --->  Accuracy = 76.92%\n",
      "M_pca =  291 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  291 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  291 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  291 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  291 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  291 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  291 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  291 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  291 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  291 , M_lda =  47  --->  Accuracy = 83.65%\n",
      "M_pca =  291 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  291 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  291 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  291 , M_lda =  51  --->  Accuracy = 85.58%\n",
      "M_pca =  292 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  292 , M_lda =  2  --->  Accuracy = 11.54%\n",
      "M_pca =  292 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  292 , M_lda =  4  --->  Accuracy = 30.77%\n",
      "M_pca =  292 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  292 , M_lda =  6  --->  Accuracy = 41.35%\n",
      "M_pca =  292 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  292 , M_lda =  8  --->  Accuracy = 53.85%\n",
      "M_pca =  292 , M_lda =  9  --->  Accuracy = 64.42%\n",
      "M_pca =  292 , M_lda =  10  --->  Accuracy = 56.73%\n",
      "M_pca =  292 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  292 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  292 , M_lda =  13  --->  Accuracy = 70.19%\n",
      "M_pca =  292 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  292 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  292 , M_lda =  16  --->  Accuracy = 75.96%\n",
      "M_pca =  292 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  292 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  292 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  292 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  292 , M_lda =  21  --->  Accuracy = 79.81%\n",
      "M_pca =  292 , M_lda =  22  --->  Accuracy = 76.92%\n",
      "M_pca =  292 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  292 , M_lda =  24  --->  Accuracy = 76.92%\n",
      "M_pca =  292 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  292 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  292 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  292 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  292 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  35  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  36  --->  Accuracy = 78.85%\n",
      "M_pca =  292 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  292 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  292 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  292 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  292 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  292 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  292 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  292 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  293 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  293 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  293 , M_lda =  3  --->  Accuracy = 27.88%\n",
      "M_pca =  293 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  293 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  293 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  293 , M_lda =  7  --->  Accuracy = 53.85%\n",
      "M_pca =  293 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  293 , M_lda =  9  --->  Accuracy = 52.88%\n",
      "M_pca =  293 , M_lda =  10  --->  Accuracy = 56.73%\n",
      "M_pca =  293 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  293 , M_lda =  12  --->  Accuracy = 70.19%\n",
      "M_pca =  293 , M_lda =  13  --->  Accuracy = 71.15%\n",
      "M_pca =  293 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  293 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  293 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  293 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  293 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  293 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  293 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  293 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  293 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  293 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  293 , M_lda =  24  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  293 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  293 , M_lda =  28  --->  Accuracy = 82.69%\n",
      "M_pca =  293 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  293 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  293 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  33  --->  Accuracy = 83.65%\n",
      "M_pca =  293 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  35  --->  Accuracy = 77.88%\n",
      "M_pca =  293 , M_lda =  36  --->  Accuracy = 83.65%\n",
      "M_pca =  293 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  293 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  293 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  293 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  293 , M_lda =  42  --->  Accuracy = 84.62%\n",
      "M_pca =  293 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  293 , M_lda =  44  --->  Accuracy = 85.58%\n",
      "M_pca =  293 , M_lda =  45  --->  Accuracy = 84.62%\n",
      "M_pca =  293 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  293 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  293 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  293 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  293 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  293 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  294 , M_lda =  1  --->  Accuracy = 8.65%\n",
      "M_pca =  294 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  294 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  294 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  294 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  294 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  294 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  294 , M_lda =  8  --->  Accuracy = 53.85%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  294 , M_lda =  9  --->  Accuracy = 55.77%\n",
      "M_pca =  294 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  294 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  294 , M_lda =  12  --->  Accuracy = 68.27%\n",
      "M_pca =  294 , M_lda =  13  --->  Accuracy = 68.27%\n",
      "M_pca =  294 , M_lda =  14  --->  Accuracy = 71.15%\n",
      "M_pca =  294 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  294 , M_lda =  16  --->  Accuracy = 71.15%\n",
      "M_pca =  294 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  294 , M_lda =  18  --->  Accuracy = 77.88%\n",
      "M_pca =  294 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  294 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  294 , M_lda =  21  --->  Accuracy = 75.00%\n",
      "M_pca =  294 , M_lda =  22  --->  Accuracy = 78.85%\n",
      "M_pca =  294 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  294 , M_lda =  24  --->  Accuracy = 75.96%\n",
      "M_pca =  294 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  294 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  294 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  294 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  294 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  294 , M_lda =  30  --->  Accuracy = 75.96%\n",
      "M_pca =  294 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  294 , M_lda =  32  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  33  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  294 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  294 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  294 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  294 , M_lda =  39  --->  Accuracy = 83.65%\n",
      "M_pca =  294 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  294 , M_lda =  41  --->  Accuracy = 82.69%\n",
      "M_pca =  294 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  294 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  294 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  294 , M_lda =  45  --->  Accuracy = 82.69%\n",
      "M_pca =  294 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  294 , M_lda =  49  --->  Accuracy = 84.62%\n",
      "M_pca =  294 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  294 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  295 , M_lda =  1  --->  Accuracy = 2.88%\n",
      "M_pca =  295 , M_lda =  2  --->  Accuracy = 18.27%\n",
      "M_pca =  295 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  295 , M_lda =  4  --->  Accuracy = 35.58%\n",
      "M_pca =  295 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  295 , M_lda =  6  --->  Accuracy = 47.12%\n",
      "M_pca =  295 , M_lda =  7  --->  Accuracy = 47.12%\n",
      "M_pca =  295 , M_lda =  8  --->  Accuracy = 51.92%\n",
      "M_pca =  295 , M_lda =  9  --->  Accuracy = 54.81%\n",
      "M_pca =  295 , M_lda =  10  --->  Accuracy = 58.65%\n",
      "M_pca =  295 , M_lda =  11  --->  Accuracy = 64.42%\n",
      "M_pca =  295 , M_lda =  12  --->  Accuracy = 67.31%\n",
      "M_pca =  295 , M_lda =  13  --->  Accuracy = 68.27%\n",
      "M_pca =  295 , M_lda =  14  --->  Accuracy = 65.38%\n",
      "M_pca =  295 , M_lda =  15  --->  Accuracy = 69.23%\n",
      "M_pca =  295 , M_lda =  16  --->  Accuracy = 75.00%\n",
      "M_pca =  295 , M_lda =  17  --->  Accuracy = 74.04%\n",
      "M_pca =  295 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  295 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  295 , M_lda =  20  --->  Accuracy = 77.88%\n",
      "M_pca =  295 , M_lda =  21  --->  Accuracy = 77.88%\n",
      "M_pca =  295 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  295 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  295 , M_lda =  24  --->  Accuracy = 74.04%\n",
      "M_pca =  295 , M_lda =  25  --->  Accuracy = 79.81%\n",
      "M_pca =  295 , M_lda =  26  --->  Accuracy = 81.73%\n",
      "M_pca =  295 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  295 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  295 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  295 , M_lda =  31  --->  Accuracy = 77.88%\n",
      "M_pca =  295 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  34  --->  Accuracy = 83.65%\n",
      "M_pca =  295 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  295 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  37  --->  Accuracy = 85.58%\n",
      "M_pca =  295 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  295 , M_lda =  41  --->  Accuracy = 81.73%\n",
      "M_pca =  295 , M_lda =  42  --->  Accuracy = 76.92%\n",
      "M_pca =  295 , M_lda =  43  --->  Accuracy = 86.54%\n",
      "M_pca =  295 , M_lda =  44  --->  Accuracy = 82.69%\n",
      "M_pca =  295 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  295 , M_lda =  46  --->  Accuracy = 81.73%\n",
      "M_pca =  295 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  295 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  295 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  295 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  296 , M_lda =  1  --->  Accuracy = 1.92%\n",
      "M_pca =  296 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  296 , M_lda =  3  --->  Accuracy = 25.96%\n",
      "M_pca =  296 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  296 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  296 , M_lda =  6  --->  Accuracy = 49.04%\n",
      "M_pca =  296 , M_lda =  7  --->  Accuracy = 45.19%\n",
      "M_pca =  296 , M_lda =  8  --->  Accuracy = 43.27%\n",
      "M_pca =  296 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  296 , M_lda =  10  --->  Accuracy = 64.42%\n",
      "M_pca =  296 , M_lda =  11  --->  Accuracy = 67.31%\n",
      "M_pca =  296 , M_lda =  12  --->  Accuracy = 69.23%\n",
      "M_pca =  296 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  296 , M_lda =  14  --->  Accuracy = 69.23%\n",
      "M_pca =  296 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  296 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  296 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  296 , M_lda =  18  --->  Accuracy = 75.96%\n",
      "M_pca =  296 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  296 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  296 , M_lda =  21  --->  Accuracy = 75.96%\n",
      "M_pca =  296 , M_lda =  22  --->  Accuracy = 74.04%\n",
      "M_pca =  296 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  296 , M_lda =  24  --->  Accuracy = 76.92%\n",
      "M_pca =  296 , M_lda =  25  --->  Accuracy = 76.92%\n",
      "M_pca =  296 , M_lda =  26  --->  Accuracy = 76.92%\n",
      "M_pca =  296 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  296 , M_lda =  28  --->  Accuracy = 81.73%\n",
      "M_pca =  296 , M_lda =  29  --->  Accuracy = 81.73%\n",
      "M_pca =  296 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  296 , M_lda =  31  --->  Accuracy = 78.85%\n",
      "M_pca =  296 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  296 , M_lda =  33  --->  Accuracy = 78.85%\n",
      "M_pca =  296 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  296 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  296 , M_lda =  36  --->  Accuracy = 81.73%\n",
      "M_pca =  296 , M_lda =  37  --->  Accuracy = 78.85%\n",
      "M_pca =  296 , M_lda =  38  --->  Accuracy = 79.81%\n",
      "M_pca =  296 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  296 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  296 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  296 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  296 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  296 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  296 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  296 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  296 , M_lda =  47  --->  Accuracy = 85.58%\n",
      "M_pca =  296 , M_lda =  48  --->  Accuracy = 83.65%\n",
      "M_pca =  296 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  296 , M_lda =  50  --->  Accuracy = 86.54%\n",
      "M_pca =  296 , M_lda =  51  --->  Accuracy = 84.62%\n",
      "M_pca =  297 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  297 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  297 , M_lda =  3  --->  Accuracy = 23.08%\n",
      "M_pca =  297 , M_lda =  4  --->  Accuracy = 34.62%\n",
      "M_pca =  297 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  297 , M_lda =  6  --->  Accuracy = 36.54%\n",
      "M_pca =  297 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  297 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  297 , M_lda =  9  --->  Accuracy = 61.54%\n",
      "M_pca =  297 , M_lda =  10  --->  Accuracy = 63.46%\n",
      "M_pca =  297 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  297 , M_lda =  12  --->  Accuracy = 63.46%\n",
      "M_pca =  297 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  297 , M_lda =  14  --->  Accuracy = 68.27%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  297 , M_lda =  15  --->  Accuracy = 74.04%\n",
      "M_pca =  297 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  297 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  297 , M_lda =  18  --->  Accuracy = 75.00%\n",
      "M_pca =  297 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  297 , M_lda =  20  --->  Accuracy = 76.92%\n",
      "M_pca =  297 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  297 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  297 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  297 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  297 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  297 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  297 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  297 , M_lda =  31  --->  Accuracy = 76.92%\n",
      "M_pca =  297 , M_lda =  32  --->  Accuracy = 83.65%\n",
      "M_pca =  297 , M_lda =  33  --->  Accuracy = 77.88%\n",
      "M_pca =  297 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  297 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  37  --->  Accuracy = 80.77%\n",
      "M_pca =  297 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  297 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  297 , M_lda =  40  --->  Accuracy = 83.65%\n",
      "M_pca =  297 , M_lda =  41  --->  Accuracy = 80.77%\n",
      "M_pca =  297 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  297 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  297 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  297 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  297 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  297 , M_lda =  47  --->  Accuracy = 84.62%\n",
      "M_pca =  297 , M_lda =  48  --->  Accuracy = 82.69%\n",
      "M_pca =  297 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  297 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  297 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  298 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  298 , M_lda =  2  --->  Accuracy = 12.50%\n",
      "M_pca =  298 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  298 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  298 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  298 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  298 , M_lda =  7  --->  Accuracy = 45.19%\n",
      "M_pca =  298 , M_lda =  8  --->  Accuracy = 52.88%\n",
      "M_pca =  298 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  298 , M_lda =  10  --->  Accuracy = 56.73%\n",
      "M_pca =  298 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  298 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  298 , M_lda =  13  --->  Accuracy = 64.42%\n",
      "M_pca =  298 , M_lda =  14  --->  Accuracy = 66.35%\n",
      "M_pca =  298 , M_lda =  15  --->  Accuracy = 67.31%\n",
      "M_pca =  298 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  298 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  298 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  298 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  298 , M_lda =  20  --->  Accuracy = 75.96%\n",
      "M_pca =  298 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  298 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  298 , M_lda =  23  --->  Accuracy = 76.92%\n",
      "M_pca =  298 , M_lda =  24  --->  Accuracy = 75.96%\n",
      "M_pca =  298 , M_lda =  25  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  26  --->  Accuracy = 74.04%\n",
      "M_pca =  298 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  298 , M_lda =  29  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  34  --->  Accuracy = 82.69%\n",
      "M_pca =  298 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  38  --->  Accuracy = 76.92%\n",
      "M_pca =  298 , M_lda =  39  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  41  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  298 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  298 , M_lda =  44  --->  Accuracy = 78.85%\n",
      "M_pca =  298 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  298 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  298 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  298 , M_lda =  48  --->  Accuracy = 79.81%\n",
      "M_pca =  298 , M_lda =  49  --->  Accuracy = 81.73%\n",
      "M_pca =  298 , M_lda =  50  --->  Accuracy = 81.73%\n",
      "M_pca =  298 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  299 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  299 , M_lda =  2  --->  Accuracy = 10.58%\n",
      "M_pca =  299 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  299 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  299 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  299 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  299 , M_lda =  7  --->  Accuracy = 46.15%\n",
      "M_pca =  299 , M_lda =  8  --->  Accuracy = 50.00%\n",
      "M_pca =  299 , M_lda =  9  --->  Accuracy = 57.69%\n",
      "M_pca =  299 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  299 , M_lda =  11  --->  Accuracy = 66.35%\n",
      "M_pca =  299 , M_lda =  12  --->  Accuracy = 63.46%\n",
      "M_pca =  299 , M_lda =  13  --->  Accuracy = 69.23%\n",
      "M_pca =  299 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  299 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  299 , M_lda =  16  --->  Accuracy = 72.12%\n",
      "M_pca =  299 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  299 , M_lda =  18  --->  Accuracy = 74.04%\n",
      "M_pca =  299 , M_lda =  19  --->  Accuracy = 69.23%\n",
      "M_pca =  299 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  299 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  299 , M_lda =  22  --->  Accuracy = 73.08%\n",
      "M_pca =  299 , M_lda =  23  --->  Accuracy = 78.85%\n",
      "M_pca =  299 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  299 , M_lda =  25  --->  Accuracy = 75.00%\n",
      "M_pca =  299 , M_lda =  26  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  27  --->  Accuracy = 75.96%\n",
      "M_pca =  299 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  299 , M_lda =  29  --->  Accuracy = 80.77%\n",
      "M_pca =  299 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  31  --->  Accuracy = 81.73%\n",
      "M_pca =  299 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  34  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  299 , M_lda =  37  --->  Accuracy = 76.92%\n",
      "M_pca =  299 , M_lda =  38  --->  Accuracy = 81.73%\n",
      "M_pca =  299 , M_lda =  39  --->  Accuracy = 79.81%\n",
      "M_pca =  299 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  299 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  299 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  299 , M_lda =  43  --->  Accuracy = 81.73%\n",
      "M_pca =  299 , M_lda =  44  --->  Accuracy = 83.65%\n",
      "M_pca =  299 , M_lda =  45  --->  Accuracy = 83.65%\n",
      "M_pca =  299 , M_lda =  46  --->  Accuracy = 87.50%\n",
      "M_pca =  299 , M_lda =  47  --->  Accuracy = 82.69%\n",
      "M_pca =  299 , M_lda =  48  --->  Accuracy = 78.85%\n",
      "M_pca =  299 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  299 , M_lda =  50  --->  Accuracy = 80.77%\n",
      "M_pca =  299 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  300 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  300 , M_lda =  2  --->  Accuracy = 8.65%\n",
      "M_pca =  300 , M_lda =  3  --->  Accuracy = 25.00%\n",
      "M_pca =  300 , M_lda =  4  --->  Accuracy = 32.69%\n",
      "M_pca =  300 , M_lda =  5  --->  Accuracy = 42.31%\n",
      "M_pca =  300 , M_lda =  6  --->  Accuracy = 50.00%\n",
      "M_pca =  300 , M_lda =  7  --->  Accuracy = 44.23%\n",
      "M_pca =  300 , M_lda =  8  --->  Accuracy = 55.77%\n",
      "M_pca =  300 , M_lda =  9  --->  Accuracy = 54.81%\n",
      "M_pca =  300 , M_lda =  10  --->  Accuracy = 53.85%\n",
      "M_pca =  300 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  300 , M_lda =  12  --->  Accuracy = 61.54%\n",
      "M_pca =  300 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  300 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  300 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  300 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  300 , M_lda =  17  --->  Accuracy = 71.15%\n",
      "M_pca =  300 , M_lda =  18  --->  Accuracy = 71.15%\n",
      "M_pca =  300 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  300 , M_lda =  20  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  300 , M_lda =  21  --->  Accuracy = 75.00%\n",
      "M_pca =  300 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  300 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  300 , M_lda =  24  --->  Accuracy = 78.85%\n",
      "M_pca =  300 , M_lda =  25  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  26  --->  Accuracy = 75.00%\n",
      "M_pca =  300 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  300 , M_lda =  28  --->  Accuracy = 78.85%\n",
      "M_pca =  300 , M_lda =  29  --->  Accuracy = 76.92%\n",
      "M_pca =  300 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  300 , M_lda =  31  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  32  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  33  --->  Accuracy = 76.92%\n",
      "M_pca =  300 , M_lda =  34  --->  Accuracy = 84.62%\n",
      "M_pca =  300 , M_lda =  35  --->  Accuracy = 78.85%\n",
      "M_pca =  300 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  300 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  300 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  39  --->  Accuracy = 81.73%\n",
      "M_pca =  300 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  300 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  300 , M_lda =  42  --->  Accuracy = 83.65%\n",
      "M_pca =  300 , M_lda =  43  --->  Accuracy = 82.69%\n",
      "M_pca =  300 , M_lda =  44  --->  Accuracy = 78.85%\n",
      "M_pca =  300 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  300 , M_lda =  47  --->  Accuracy = 82.69%\n",
      "M_pca =  300 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  300 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  300 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  300 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  1  --->  Accuracy = 4.81%\n",
      "M_pca =  301 , M_lda =  2  --->  Accuracy = 17.31%\n",
      "M_pca =  301 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  301 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  301 , M_lda =  5  --->  Accuracy = 41.35%\n",
      "M_pca =  301 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  301 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  301 , M_lda =  8  --->  Accuracy = 52.88%\n",
      "M_pca =  301 , M_lda =  9  --->  Accuracy = 58.65%\n",
      "M_pca =  301 , M_lda =  10  --->  Accuracy = 61.54%\n",
      "M_pca =  301 , M_lda =  11  --->  Accuracy = 61.54%\n",
      "M_pca =  301 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  301 , M_lda =  13  --->  Accuracy = 66.35%\n",
      "M_pca =  301 , M_lda =  14  --->  Accuracy = 69.23%\n",
      "M_pca =  301 , M_lda =  15  --->  Accuracy = 71.15%\n",
      "M_pca =  301 , M_lda =  16  --->  Accuracy = 69.23%\n",
      "M_pca =  301 , M_lda =  17  --->  Accuracy = 69.23%\n",
      "M_pca =  301 , M_lda =  18  --->  Accuracy = 68.27%\n",
      "M_pca =  301 , M_lda =  19  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  20  --->  Accuracy = 74.04%\n",
      "M_pca =  301 , M_lda =  21  --->  Accuracy = 73.08%\n",
      "M_pca =  301 , M_lda =  22  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  23  --->  Accuracy = 73.08%\n",
      "M_pca =  301 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  25  --->  Accuracy = 74.04%\n",
      "M_pca =  301 , M_lda =  26  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  27  --->  Accuracy = 80.77%\n",
      "M_pca =  301 , M_lda =  28  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  30  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  31  --->  Accuracy = 78.85%\n",
      "M_pca =  301 , M_lda =  32  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  301 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  37  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  301 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  40  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  301 , M_lda =  42  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  301 , M_lda =  44  --->  Accuracy = 77.88%\n",
      "M_pca =  301 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  301 , M_lda =  46  --->  Accuracy = 83.65%\n",
      "M_pca =  301 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  301 , M_lda =  49  --->  Accuracy = 82.69%\n",
      "M_pca =  301 , M_lda =  50  --->  Accuracy = 84.62%\n",
      "M_pca =  301 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  302 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  302 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  302 , M_lda =  3  --->  Accuracy = 19.23%\n",
      "M_pca =  302 , M_lda =  4  --->  Accuracy = 30.77%\n",
      "M_pca =  302 , M_lda =  5  --->  Accuracy = 36.54%\n",
      "M_pca =  302 , M_lda =  6  --->  Accuracy = 40.38%\n",
      "M_pca =  302 , M_lda =  7  --->  Accuracy = 49.04%\n",
      "M_pca =  302 , M_lda =  8  --->  Accuracy = 53.85%\n",
      "M_pca =  302 , M_lda =  9  --->  Accuracy = 58.65%\n",
      "M_pca =  302 , M_lda =  10  --->  Accuracy = 56.73%\n",
      "M_pca =  302 , M_lda =  11  --->  Accuracy = 62.50%\n",
      "M_pca =  302 , M_lda =  12  --->  Accuracy = 62.50%\n",
      "M_pca =  302 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  302 , M_lda =  14  --->  Accuracy = 63.46%\n",
      "M_pca =  302 , M_lda =  15  --->  Accuracy = 72.12%\n",
      "M_pca =  302 , M_lda =  16  --->  Accuracy = 68.27%\n",
      "M_pca =  302 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  302 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  302 , M_lda =  19  --->  Accuracy = 75.96%\n",
      "M_pca =  302 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  302 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  302 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  302 , M_lda =  23  --->  Accuracy = 73.08%\n",
      "M_pca =  302 , M_lda =  24  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  302 , M_lda =  26  --->  Accuracy = 78.85%\n",
      "M_pca =  302 , M_lda =  27  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  302 , M_lda =  29  --->  Accuracy = 78.85%\n",
      "M_pca =  302 , M_lda =  30  --->  Accuracy = 77.88%\n",
      "M_pca =  302 , M_lda =  31  --->  Accuracy = 77.88%\n",
      "M_pca =  302 , M_lda =  32  --->  Accuracy = 76.92%\n",
      "M_pca =  302 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  34  --->  Accuracy = 80.77%\n",
      "M_pca =  302 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  38  --->  Accuracy = 82.69%\n",
      "M_pca =  302 , M_lda =  39  --->  Accuracy = 77.88%\n",
      "M_pca =  302 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  302 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  302 , M_lda =  42  --->  Accuracy = 82.69%\n",
      "M_pca =  302 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  302 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  302 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  302 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  302 , M_lda =  47  --->  Accuracy = 82.69%\n",
      "M_pca =  302 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  302 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  302 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  302 , M_lda =  51  --->  Accuracy = 83.65%\n",
      "M_pca =  303 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  303 , M_lda =  2  --->  Accuracy = 14.42%\n",
      "M_pca =  303 , M_lda =  3  --->  Accuracy = 21.15%\n",
      "M_pca =  303 , M_lda =  4  --->  Accuracy = 31.73%\n",
      "M_pca =  303 , M_lda =  5  --->  Accuracy = 38.46%\n",
      "M_pca =  303 , M_lda =  6  --->  Accuracy = 45.19%\n",
      "M_pca =  303 , M_lda =  7  --->  Accuracy = 44.23%\n",
      "M_pca =  303 , M_lda =  8  --->  Accuracy = 54.81%\n",
      "M_pca =  303 , M_lda =  9  --->  Accuracy = 58.65%\n",
      "M_pca =  303 , M_lda =  10  --->  Accuracy = 55.77%\n",
      "M_pca =  303 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  303 , M_lda =  12  --->  Accuracy = 63.46%\n",
      "M_pca =  303 , M_lda =  13  --->  Accuracy = 67.31%\n",
      "M_pca =  303 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  303 , M_lda =  15  --->  Accuracy = 68.27%\n",
      "M_pca =  303 , M_lda =  16  --->  Accuracy = 73.08%\n",
      "M_pca =  303 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  303 , M_lda =  18  --->  Accuracy = 73.08%\n",
      "M_pca =  303 , M_lda =  19  --->  Accuracy = 72.12%\n",
      "M_pca =  303 , M_lda =  20  --->  Accuracy = 72.12%\n",
      "M_pca =  303 , M_lda =  21  --->  Accuracy = 72.12%\n",
      "M_pca =  303 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  303 , M_lda =  23  --->  Accuracy = 74.04%\n",
      "M_pca =  303 , M_lda =  24  --->  Accuracy = 75.00%\n",
      "M_pca =  303 , M_lda =  25  --->  Accuracy = 75.96%\n",
      "M_pca =  303 , M_lda =  26  --->  Accuracy = 75.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  303 , M_lda =  27  --->  Accuracy = 77.88%\n",
      "M_pca =  303 , M_lda =  28  --->  Accuracy = 80.77%\n",
      "M_pca =  303 , M_lda =  29  --->  Accuracy = 79.81%\n",
      "M_pca =  303 , M_lda =  30  --->  Accuracy = 78.85%\n",
      "M_pca =  303 , M_lda =  31  --->  Accuracy = 75.00%\n",
      "M_pca =  303 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  303 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  303 , M_lda =  34  --->  Accuracy = 77.88%\n",
      "M_pca =  303 , M_lda =  35  --->  Accuracy = 79.81%\n",
      "M_pca =  303 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  303 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  303 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  303 , M_lda =  39  --->  Accuracy = 75.96%\n",
      "M_pca =  303 , M_lda =  40  --->  Accuracy = 82.69%\n",
      "M_pca =  303 , M_lda =  41  --->  Accuracy = 77.88%\n",
      "M_pca =  303 , M_lda =  42  --->  Accuracy = 79.81%\n",
      "M_pca =  303 , M_lda =  43  --->  Accuracy = 79.81%\n",
      "M_pca =  303 , M_lda =  44  --->  Accuracy = 84.62%\n",
      "M_pca =  303 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  303 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  303 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  303 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  303 , M_lda =  49  --->  Accuracy = 80.77%\n",
      "M_pca =  303 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  303 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  304 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  304 , M_lda =  3  --->  Accuracy = 17.31%\n",
      "M_pca =  304 , M_lda =  4  --->  Accuracy = 27.88%\n",
      "M_pca =  304 , M_lda =  5  --->  Accuracy = 37.50%\n",
      "M_pca =  304 , M_lda =  6  --->  Accuracy = 44.23%\n",
      "M_pca =  304 , M_lda =  7  --->  Accuracy = 44.23%\n",
      "M_pca =  304 , M_lda =  8  --->  Accuracy = 56.73%\n",
      "M_pca =  304 , M_lda =  9  --->  Accuracy = 59.62%\n",
      "M_pca =  304 , M_lda =  10  --->  Accuracy = 60.58%\n",
      "M_pca =  304 , M_lda =  11  --->  Accuracy = 63.46%\n",
      "M_pca =  304 , M_lda =  12  --->  Accuracy = 56.73%\n",
      "M_pca =  304 , M_lda =  13  --->  Accuracy = 66.35%\n",
      "M_pca =  304 , M_lda =  14  --->  Accuracy = 63.46%\n",
      "M_pca =  304 , M_lda =  15  --->  Accuracy = 65.38%\n",
      "M_pca =  304 , M_lda =  16  --->  Accuracy = 70.19%\n",
      "M_pca =  304 , M_lda =  17  --->  Accuracy = 72.12%\n",
      "M_pca =  304 , M_lda =  18  --->  Accuracy = 69.23%\n",
      "M_pca =  304 , M_lda =  19  --->  Accuracy = 75.00%\n",
      "M_pca =  304 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  304 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  304 , M_lda =  22  --->  Accuracy = 72.12%\n",
      "M_pca =  304 , M_lda =  23  --->  Accuracy = 75.00%\n",
      "M_pca =  304 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  304 , M_lda =  25  --->  Accuracy = 77.88%\n",
      "M_pca =  304 , M_lda =  26  --->  Accuracy = 73.08%\n",
      "M_pca =  304 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  304 , M_lda =  28  --->  Accuracy = 76.92%\n",
      "M_pca =  304 , M_lda =  29  --->  Accuracy = 76.92%\n",
      "M_pca =  304 , M_lda =  30  --->  Accuracy = 83.65%\n",
      "M_pca =  304 , M_lda =  31  --->  Accuracy = 79.81%\n",
      "M_pca =  304 , M_lda =  32  --->  Accuracy = 77.88%\n",
      "M_pca =  304 , M_lda =  33  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  34  --->  Accuracy = 78.85%\n",
      "M_pca =  304 , M_lda =  35  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  36  --->  Accuracy = 79.81%\n",
      "M_pca =  304 , M_lda =  37  --->  Accuracy = 76.92%\n",
      "M_pca =  304 , M_lda =  38  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  39  --->  Accuracy = 82.69%\n",
      "M_pca =  304 , M_lda =  40  --->  Accuracy = 79.81%\n",
      "M_pca =  304 , M_lda =  41  --->  Accuracy = 77.88%\n",
      "M_pca =  304 , M_lda =  42  --->  Accuracy = 78.85%\n",
      "M_pca =  304 , M_lda =  43  --->  Accuracy = 83.65%\n",
      "M_pca =  304 , M_lda =  44  --->  Accuracy = 79.81%\n",
      "M_pca =  304 , M_lda =  45  --->  Accuracy = 81.73%\n",
      "M_pca =  304 , M_lda =  46  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  47  --->  Accuracy = 80.77%\n",
      "M_pca =  304 , M_lda =  48  --->  Accuracy = 84.62%\n",
      "M_pca =  304 , M_lda =  49  --->  Accuracy = 86.54%\n",
      "M_pca =  304 , M_lda =  50  --->  Accuracy = 82.69%\n",
      "M_pca =  304 , M_lda =  51  --->  Accuracy = 82.69%\n",
      "M_pca =  305 , M_lda =  1  --->  Accuracy = 6.73%\n",
      "M_pca =  305 , M_lda =  2  --->  Accuracy = 15.38%\n",
      "M_pca =  305 , M_lda =  3  --->  Accuracy = 19.23%\n",
      "M_pca =  305 , M_lda =  4  --->  Accuracy = 36.54%\n",
      "M_pca =  305 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  305 , M_lda =  6  --->  Accuracy = 40.38%\n",
      "M_pca =  305 , M_lda =  7  --->  Accuracy = 48.08%\n",
      "M_pca =  305 , M_lda =  8  --->  Accuracy = 49.04%\n",
      "M_pca =  305 , M_lda =  9  --->  Accuracy = 56.73%\n",
      "M_pca =  305 , M_lda =  10  --->  Accuracy = 54.81%\n",
      "M_pca =  305 , M_lda =  11  --->  Accuracy = 60.58%\n",
      "M_pca =  305 , M_lda =  12  --->  Accuracy = 65.38%\n",
      "M_pca =  305 , M_lda =  13  --->  Accuracy = 65.38%\n",
      "M_pca =  305 , M_lda =  14  --->  Accuracy = 67.31%\n",
      "M_pca =  305 , M_lda =  15  --->  Accuracy = 69.23%\n",
      "M_pca =  305 , M_lda =  16  --->  Accuracy = 66.35%\n",
      "M_pca =  305 , M_lda =  17  --->  Accuracy = 73.08%\n",
      "M_pca =  305 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  305 , M_lda =  19  --->  Accuracy = 71.15%\n",
      "M_pca =  305 , M_lda =  20  --->  Accuracy = 75.00%\n",
      "M_pca =  305 , M_lda =  21  --->  Accuracy = 76.92%\n",
      "M_pca =  305 , M_lda =  22  --->  Accuracy = 75.00%\n",
      "M_pca =  305 , M_lda =  23  --->  Accuracy = 74.04%\n",
      "M_pca =  305 , M_lda =  24  --->  Accuracy = 77.88%\n",
      "M_pca =  305 , M_lda =  25  --->  Accuracy = 76.92%\n",
      "M_pca =  305 , M_lda =  26  --->  Accuracy = 74.04%\n",
      "M_pca =  305 , M_lda =  27  --->  Accuracy = 78.85%\n",
      "M_pca =  305 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  305 , M_lda =  29  --->  Accuracy = 77.88%\n",
      "M_pca =  305 , M_lda =  30  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  31  --->  Accuracy = 75.96%\n",
      "M_pca =  305 , M_lda =  32  --->  Accuracy = 78.85%\n",
      "M_pca =  305 , M_lda =  33  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  34  --->  Accuracy = 81.73%\n",
      "M_pca =  305 , M_lda =  35  --->  Accuracy = 77.88%\n",
      "M_pca =  305 , M_lda =  36  --->  Accuracy = 80.77%\n",
      "M_pca =  305 , M_lda =  37  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  38  --->  Accuracy = 78.85%\n",
      "M_pca =  305 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  305 , M_lda =  40  --->  Accuracy = 78.85%\n",
      "M_pca =  305 , M_lda =  41  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  42  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  43  --->  Accuracy = 80.77%\n",
      "M_pca =  305 , M_lda =  44  --->  Accuracy = 81.73%\n",
      "M_pca =  305 , M_lda =  45  --->  Accuracy = 79.81%\n",
      "M_pca =  305 , M_lda =  46  --->  Accuracy = 82.69%\n",
      "M_pca =  305 , M_lda =  47  --->  Accuracy = 78.85%\n",
      "M_pca =  305 , M_lda =  48  --->  Accuracy = 81.73%\n",
      "M_pca =  305 , M_lda =  49  --->  Accuracy = 83.65%\n",
      "M_pca =  305 , M_lda =  50  --->  Accuracy = 83.65%\n",
      "M_pca =  305 , M_lda =  51  --->  Accuracy = 81.73%\n",
      "M_pca =  306 , M_lda =  1  --->  Accuracy = 7.69%\n",
      "M_pca =  306 , M_lda =  2  --->  Accuracy = 16.35%\n",
      "M_pca =  306 , M_lda =  3  --->  Accuracy = 24.04%\n",
      "M_pca =  306 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  306 , M_lda =  5  --->  Accuracy = 39.42%\n",
      "M_pca =  306 , M_lda =  6  --->  Accuracy = 42.31%\n",
      "M_pca =  306 , M_lda =  7  --->  Accuracy = 50.00%\n",
      "M_pca =  306 , M_lda =  8  --->  Accuracy = 49.04%\n",
      "M_pca =  306 , M_lda =  9  --->  Accuracy = 54.81%\n",
      "M_pca =  306 , M_lda =  10  --->  Accuracy = 59.62%\n",
      "M_pca =  306 , M_lda =  11  --->  Accuracy = 68.27%\n",
      "M_pca =  306 , M_lda =  12  --->  Accuracy = 64.42%\n",
      "M_pca =  306 , M_lda =  13  --->  Accuracy = 66.35%\n",
      "M_pca =  306 , M_lda =  14  --->  Accuracy = 68.27%\n",
      "M_pca =  306 , M_lda =  15  --->  Accuracy = 73.08%\n",
      "M_pca =  306 , M_lda =  16  --->  Accuracy = 74.04%\n",
      "M_pca =  306 , M_lda =  17  --->  Accuracy = 70.19%\n",
      "M_pca =  306 , M_lda =  18  --->  Accuracy = 72.12%\n",
      "M_pca =  306 , M_lda =  19  --->  Accuracy = 73.08%\n",
      "M_pca =  306 , M_lda =  20  --->  Accuracy = 73.08%\n",
      "M_pca =  306 , M_lda =  21  --->  Accuracy = 74.04%\n",
      "M_pca =  306 , M_lda =  22  --->  Accuracy = 75.96%\n",
      "M_pca =  306 , M_lda =  23  --->  Accuracy = 75.96%\n",
      "M_pca =  306 , M_lda =  24  --->  Accuracy = 75.00%\n",
      "M_pca =  306 , M_lda =  25  --->  Accuracy = 76.92%\n",
      "M_pca =  306 , M_lda =  26  --->  Accuracy = 75.96%\n",
      "M_pca =  306 , M_lda =  27  --->  Accuracy = 76.92%\n",
      "M_pca =  306 , M_lda =  28  --->  Accuracy = 77.88%\n",
      "M_pca =  306 , M_lda =  29  --->  Accuracy = 78.85%\n",
      "M_pca =  306 , M_lda =  30  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  31  --->  Accuracy = 77.88%\n",
      "M_pca =  306 , M_lda =  32  --->  Accuracy = 76.92%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_pca =  306 , M_lda =  33  --->  Accuracy = 78.85%\n",
      "M_pca =  306 , M_lda =  34  --->  Accuracy = 76.92%\n",
      "M_pca =  306 , M_lda =  35  --->  Accuracy = 83.65%\n",
      "M_pca =  306 , M_lda =  36  --->  Accuracy = 77.88%\n",
      "M_pca =  306 , M_lda =  37  --->  Accuracy = 77.88%\n",
      "M_pca =  306 , M_lda =  38  --->  Accuracy = 77.88%\n",
      "M_pca =  306 , M_lda =  39  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  40  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  41  --->  Accuracy = 83.65%\n",
      "M_pca =  306 , M_lda =  42  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  43  --->  Accuracy = 78.85%\n",
      "M_pca =  306 , M_lda =  44  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  45  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  46  --->  Accuracy = 79.81%\n",
      "M_pca =  306 , M_lda =  47  --->  Accuracy = 81.73%\n",
      "M_pca =  306 , M_lda =  48  --->  Accuracy = 80.77%\n",
      "M_pca =  306 , M_lda =  49  --->  Accuracy = 78.85%\n",
      "M_pca =  306 , M_lda =  50  --->  Accuracy = 79.81%\n",
      "M_pca =  306 , M_lda =  51  --->  Accuracy = 80.77%\n",
      "M_pca =  307 , M_lda =  1  --->  Accuracy = 3.85%\n",
      "M_pca =  307 , M_lda =  2  --->  Accuracy = 13.46%\n",
      "M_pca =  307 , M_lda =  3  --->  Accuracy = 22.12%\n",
      "M_pca =  307 , M_lda =  4  --->  Accuracy = 33.65%\n",
      "M_pca =  307 , M_lda =  5  --->  Accuracy = 40.38%\n",
      "M_pca =  307 , M_lda =  6  --->  Accuracy = 48.08%\n",
      "M_pca =  307 , M_lda =  7  --->  Accuracy = 52.88%\n",
      "M_pca =  307 , M_lda =  8  --->  Accuracy = 50.00%\n",
      "M_pca =  307 , M_lda =  9  --->  Accuracy = 52.88%\n",
      "M_pca =  307 , M_lda =  10  --->  Accuracy = 59.62%\n",
      "M_pca =  307 , M_lda =  11  --->  Accuracy = 60.58%\n",
      "M_pca =  307 , M_lda =  12  --->  Accuracy = 64.42%\n",
      "M_pca =  307 , M_lda =  13  --->  Accuracy = 70.19%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d9eeb5e78e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mM_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mW_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msvd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             raise ValueError(\"Unrecognized svd_solver='{0}'\"\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/decomposition/pca.py\u001b[0m in \u001b[0;36m_fit_truncated\u001b[0;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[1;32m    493\u001b[0m                                      \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterated_power\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                                      \u001b[0mflip_sign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m                                      random_state=random_state)\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[0;32m--> 326\u001b[0;31m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'QR'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/scipy/linalg/decomp_lu.py\u001b[0m in \u001b[0;36mlu\u001b[0;34m(a, permute_l, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0moverwrite_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverwrite_a\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_datacopied\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mflu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_flinalg_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpermute_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         raise ValueError('illegal value in %d-th argument of '\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LDA-PCA using scikit learn PCA function\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# KNN Classifer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "_card = 52\n",
    "D, N = X_train.shape\n",
    "        \n",
    "    \n",
    "M_pca = 1\n",
    "M_lda = 1\n",
    "\n",
    "\n",
    "standard = False\n",
    "\n",
    "M__pca_ideal = None\n",
    "M__lda_ideal = None\n",
    "acc_max = 0\n",
    "\n",
    "while M_pca <= (N-_card):\n",
    "    M_lda = 1\n",
    "    while M_lda <= (_card-1):\n",
    "\n",
    "        pca = PCA(n_components=M_pca)\n",
    "        W_train = pca.fit_transform(X_train.T)\n",
    "\n",
    "        lda = LinearDiscriminantAnalysis(n_components=M_lda)\n",
    "        W_train_2 = lda.fit_transform(W_train, y_train.T.ravel())\n",
    "\n",
    "        nn = KNeighborsClassifier(n_neighbors=1)\n",
    "        nn.fit(W_train_2, y_train.T.ravel())\n",
    "\n",
    "        W_test = pca.transform(X_test.T)\n",
    "\n",
    "        W_test_2 = lda.transform(W_test)\n",
    "\n",
    "        acc = nn.score(W_test_2, y_test.T.ravel())\n",
    "\n",
    "        print('M_pca = ', M_pca, ', M_lda = ', M_lda,' --->  Accuracy = %.2f%%' % (acc * 100))\n",
    "        \n",
    "        if (acc > acc_max):\n",
    "            M__pca_ideal = M_pca\n",
    "            M__lda_ideal = M_lda\n",
    "            acc_max = acc\n",
    "\n",
    "        M_lda = M_lda + 1\n",
    "        \n",
    "    M_pca = M_pca + 1\n",
    "    \n",
    "print (\"Accuracy is maximum for M__pca = \", M__pca_ideal, \", M_lda = \", M__lda_ideal, \" with accuracy of %.2f%%\"% (acc_max * 100), \".\")\n",
    "\n",
    "#Accuracy is maximum for M__pca =  150 , M_lda =  47  with accuracy of 94.23% ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M__pca_ideal =  150\n",
      "M__lda_ideal =  47\n",
      "(2576, 416)\n",
      "(1, 416)\n",
      "Accuracy of base estimator with no pre PCA = 90.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of base estimator with pre PCA applied = 91.35%\n",
      "(416, 415)\n",
      "Accuracy of ensemble estimator = 84.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation mean accuracy = 6.51%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=1704556621, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  1  = 0.00%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=908355190, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  2  = 2.88%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=1629840588, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  3  = 0.00%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=1337434398, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  4  = 0.00%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=449113202, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  5  = 1.92%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=195085914, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  6  = 0.96%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=444907332, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  7  = 0.96%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=1664333891, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  8  = 0.96%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=148405549, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  9  = 2.88%\n",
      "Pipeline(memory=None,\n",
      "     steps=[('pca', PCA(copy=True, iterated_power='auto', n_components=150,\n",
      "  random_state=1448271824, svd_solver='auto', tol=0.0, whiten=False)), ('lda', LinearDiscriminantAnalysis(n_components=47, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "           weights='uniform'))])\n",
      "Accuracy of sub model  10  = 2.88%\n",
      "Average accuracy of sub models = 1.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of voting = 91.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "### Draft cell, do not use\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "D, N = X_train.shape\n",
    "\n",
    "standard = False\n",
    "#M__pca_ideal = 147\n",
    "#M__lda_ideal = 46\n",
    "\n",
    "print ('M__pca_ideal = ', M__pca_ideal)\n",
    "print ('M__lda_ideal = ', M__lda_ideal)\n",
    "\n",
    "M_pca_bag = N-1\n",
    "\n",
    "M_pca = 150 #M__pca_ideal\n",
    "M_lda = 47 #M__lda_ideal\n",
    "\n",
    "n_est = 10\n",
    "\n",
    "estimators = [('pca', PCA(n_components=M_pca)), ('lda', LinearDiscriminantAnalysis(n_components=M_lda)), ('knn', KNeighborsClassifier(n_neighbors=1))]\n",
    "\n",
    "base_est = Pipeline (estimators)\n",
    "\n",
    "print (X_train.shape)\n",
    "print (y_train.shape)\n",
    "\n",
    "base_est.fit(X_train.T, y_train.T.ravel())\n",
    "\n",
    "acc = base_est.score(X_test.T, y_test.T.ravel())\n",
    "print ('Accuracy of base estimator with no pre PCA = %.2f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "pca = PCA(n_components=M_pca_bag)\n",
    "W_train = pca.fit_transform(X_train.T)\n",
    "W_test = pca.transform(X_test.T)\n",
    "\n",
    "base_est.fit(W_train, y_train.T.ravel())\n",
    "\n",
    "acc = base_est.score(W_test, y_test.T.ravel())\n",
    "print ('Accuracy of base estimator with pre PCA applied = %.2f%%' % (acc * 100))\n",
    "\n",
    "bagging = BaggingClassifier(base_estimator=base_est,\n",
    "                            max_samples=1.0,\n",
    "                            max_features=1.0,\n",
    "                            bootstrap=True,\n",
    "                            #bootstrap_features=True,\n",
    "                            n_estimators=n_est)\n",
    "\n",
    "print (W_train.shape)\n",
    "\n",
    "bagging = bagging.fit(W_train, y_train.T.ravel())\n",
    "\n",
    "acc = bagging.score(W_test, y_test.T.ravel())\n",
    "print ('Accuracy of ensemble estimator = %.2f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "results = model_selection.cross_val_score(bagging, W_train, y_train.T.ravel(), cv=kfold)\n",
    "print('Cross validation mean accuracy = %.2f%%' % (results.mean() * 100))\n",
    "\n",
    "sub_model_accuracies = []\n",
    "\n",
    "sub_estimators = []\n",
    "\n",
    "for i, estimator in enumerate(bagging.estimators_):\n",
    "    print(estimator)\n",
    "    sub_model_acc = estimator.score(W_test, y_test.T.ravel())\n",
    "    print ('Accuracy of sub model ', i+1, ' = %.2f%%' % (sub_model_acc * 100))\n",
    "    sub_model_accuracies.append(sub_model_acc)\n",
    "    name = 'est'+str(i+1)\n",
    "    sub_estimators.append((name, estimator))\n",
    "    \n",
    "ave_sub_model_acc = sum(sub_model_accuracies)/n_est\n",
    "\n",
    "print ('Average accuracy of sub models = %.2f%%' % (ave_sub_model_acc * 100))\n",
    "\n",
    "\n",
    "voting = VotingClassifier(estimators=sub_estimators, voting='soft')\n",
    "voting = voting.fit(W_train, y_train.T.ravel())\n",
    "acc = voting.score(W_test, y_test.T.ravel())\n",
    "print ('Accuracy of voting = %.2f%%' % (acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Bagging\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "def bagging(n_estimators, max_samples, verbose = False):\n",
    "\n",
    "    D, N = X_train.shape\n",
    "\n",
    "    standard = False\n",
    "    #M__pca_ideal = 147\n",
    "    #M__lda_ideal = 46\n",
    "\n",
    "    if verbose:\n",
    "        print ('M__pca_ideal = ', M__pca_ideal)\n",
    "        print ('M__lda_ideal = ', M__lda_ideal)\n",
    "\n",
    "    M_pca_bag = N-1\n",
    "\n",
    "    M_pca = 150 #M__pca_ideal\n",
    "    M_lda = 47 #M__lda_ideal\n",
    "\n",
    "    estimators = [('pca', PCA(n_components=M_pca)), ('lda', LinearDiscriminantAnalysis(n_components=M_lda)), ('knn', KNeighborsClassifier(n_neighbors=1))]\n",
    "\n",
    "    base_est = Pipeline (estimators)\n",
    "\n",
    "    base_est.fit(X_train.T, y_train.T.ravel())\n",
    "\n",
    "    acc = base_est.score(X_test.T, y_test.T.ravel())\n",
    "    if verbose:\n",
    "        print ('Accuracy of base estimator with no pre PCA = %.2f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "    pca = PCA(n_components=M_pca_bag)\n",
    "    W_train = pca.fit_transform(X_train.T)\n",
    "    W_test = pca.transform(X_test.T)\n",
    "\n",
    "    base_est.fit(W_train, y_train.T.ravel())\n",
    "\n",
    "    acc = base_est.score(W_test, y_test.T.ravel())\n",
    "    if verbose:\n",
    "        print ('Accuracy of base estimator with pre PCA applied = %.2f%%' % (acc * 100))\n",
    "\n",
    "    estimators = []\n",
    "    sub_model_accuracies = []\n",
    "\n",
    "    for i in range (n_estimators):\n",
    "\n",
    "        mask = np.random.choice(np.arange(N), int(max_samples * N), replace=False)\n",
    "\n",
    "        mask = np.array(mask).ravel()\n",
    "\n",
    "        W_bag = W_train[mask, :]\n",
    "        y_bag = y_train[:, mask]\n",
    "    \n",
    "        estimator = clone(base_est)\n",
    "\n",
    "        estimator.fit(W_bag, y_bag.T.ravel())\n",
    "    \n",
    "        name = 'est_'+str(i+1)\n",
    "        estimators.append((name, estimator))\n",
    "    \n",
    "        sub_model_acc = estimator.score(W_test, y_test.T.ravel())\n",
    "        sub_model_accuracies.append(sub_model_acc)\n",
    "        if verbose:\n",
    "            print ('Accuracy of sub model ', i+1, ' = %.2f%%' % (sub_model_acc * 100))\n",
    "    \n",
    "\n",
    "    ave_sub_model_acc = sum(sub_model_accuracies)/n_estimators\n",
    "    if verbose:\n",
    "        print ('Average accuracy of sub models = %.2f%%' % (ave_sub_model_acc * 100))\n",
    "    \n",
    "    y_hat = []\n",
    "\n",
    "    for w in W_test:\n",
    "        prediction_sum = 0\n",
    "        predictions = np.empty(n_estimators, dtype = np.int64)\n",
    "        for i, (name, estimator) in enumerate(estimators):\n",
    "            y = estimator.predict(w.reshape(1, -1))\n",
    "        \n",
    "            prediction_sum = prediction_sum + float(y[0])\n",
    "            predictions[i] = int(y[0])\n",
    "        prediction = round(prediction_sum/n_estimators)\n",
    "        \n",
    "        counts = np.bincount(predictions)\n",
    "        #y_hat.append(prediction)\n",
    "        y_hat.append(np.argmax(counts))\n",
    "    \n",
    "    acc = accuracy_score(y_test.T, y_hat)\n",
    "    if verbose:\n",
    "        print ('Accuracy of ensemble estimator = %.2f%%' % (acc * 100))\n",
    "    \n",
    "    return acc, ave_sub_model_acc\n",
    "    \n",
    "        \n",
    "\n",
    "n_estimators = 50\n",
    "max_samples = 0.8\n",
    "\n",
    "acc, ave_sub_model_acc = bagging(n_estimators, max_samples)\n",
    "\n",
    "n_estimators = 30\n",
    "max_samples = 0.25\n",
    "\n",
    "acc_varying_samples = []\n",
    "acc_varying_samples_ave = []\n",
    "num_samples = []\n",
    "\n",
    "while max_samples <= 1.0:\n",
    "    acc, ave_sub_model_acc = bagging(n_estimators, max_samples)\n",
    "    acc_varying_samples.append(acc*100)\n",
    "    acc_varying_samples_ave.append(ave_sub_model_acc*100)\n",
    "    num_samples.append(max_samples)\n",
    "    max_samples = max_samples + 0.25\n",
    "\n",
    "n_estimators = 1\n",
    "max_samples = 0.8\n",
    "\n",
    "acc_varying_num_est_bag = []\n",
    "num_estimators_list = []\n",
    "n_est_test_range = 10\n",
    "\n",
    "while n_estimators <= n_est_test_range:\n",
    "    acc, ave_sub_model_acc = bagging(n_estimators, max_samples)\n",
    "    acc_varying_num_est_bag.append(acc*100)\n",
    "    num_estimators_list.append(n_estimators)\n",
    "    n_estimators = n_estimators + 1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Accuracy  [%]')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a21b12ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(num_samples, acc_varying_samples)\n",
    "#sns.regplot(x=Ms.reshape(-1, 1), y=np.array(memory))\n",
    "plt.title('Accuracy vs Bagging Ratio for n_est=30\\n')\n",
    "plt.xlabel('Ratio of samples drawn')\n",
    "plt.ylabel('Accuracy  [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Accuracy  [%]')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a21d9c8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(num_estimators_list, acc_varying_num_est_bag)\n",
    "#sns.regplot(x=Ms.reshape(-1, 1), y=np.array(memory))\n",
    "plt.title('Accuracy vs number of estimators\\n')\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.ylabel('Accuracy  [%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M__pca_ideal =  47\n",
      "M__lda_ideal =  15\n",
      "Accuracy of base estimator with no pre PCA = 91.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of base estimator with pre PCA applied = 23.08%\n",
      "Accuracy of sub model  1  = 90.38%\n",
      "Accuracy of sub model  2  = 86.54%\n",
      "Accuracy of sub model  3  = 85.58%\n",
      "Accuracy of sub model  4  = 88.46%\n",
      "Accuracy of sub model  5  = 86.54%\n",
      "Accuracy of sub model  6  = 89.42%\n",
      "Accuracy of sub model  7  = 88.46%\n",
      "Accuracy of sub model  8  = 89.42%\n",
      "Accuracy of sub model  9  = 88.46%\n",
      "Accuracy of sub model  10  = 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  11  = 86.54%\n",
      "Accuracy of sub model  12  = 89.42%\n",
      "Accuracy of sub model  13  = 87.50%\n",
      "Accuracy of sub model  14  = 86.54%\n",
      "Accuracy of sub model  15  = 89.42%\n",
      "Accuracy of sub model  16  = 88.46%\n",
      "Accuracy of sub model  17  = 87.50%\n",
      "Accuracy of sub model  18  = 87.50%\n",
      "Accuracy of sub model  19  = 91.35%\n",
      "Accuracy of sub model  20  = 87.50%\n",
      "Accuracy of sub model  21  = 86.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sub model  22  = 90.38%\n",
      "Accuracy of sub model  23  = 89.42%\n",
      "Accuracy of sub model  24  = 86.54%\n",
      "Accuracy of sub model  25  = 92.31%\n",
      "Accuracy of sub model  26  = 88.46%\n",
      "Accuracy of sub model  27  = 89.42%\n",
      "Accuracy of sub model  28  = 87.50%\n",
      "Accuracy of sub model  29  = 91.35%\n",
      "Accuracy of sub model  30  = 89.42%\n",
      "Average accuracy of sub models = 88.46%\n",
      "Accuracy of ensemble estimator = 94.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFallbackToBackend\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_initialize_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    546\u001b[0m             n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n\u001b[0;32m--> 547\u001b[0;31m                                              **self._backend_args)\n\u001b[0m\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_timeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mconfigure\u001b[0;34m(self, n_jobs, parallel, **backend_args)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;31m# Avoid unnecessary overhead and use sequential backend instead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFallbackToBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequentialBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFallbackToBackend\u001b[0m: <sklearn.externals.joblib._parallel_backends.SequentialBackend object at 0x1a22151da0>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-eb4d4b961d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0macc_varying_subspace_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mM1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mM0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mave_sub_model_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_subspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0macc_varying_subspace_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mnum_M1_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-eb4d4b961d89>\u001b[0m in \u001b[0;36mrandom_subspace\u001b[0;34m(n_estimators, M0, M1, verbose)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mprediction_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_sum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 delayed(self._tree.query, check_pickle=False)(\n\u001b[1;32m    384\u001b[0m                     X[s], n_neighbors, return_distance)\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m             )\n\u001b[1;32m    387\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aborting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_effective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_initialize_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n\u001b[0;32m--> 547\u001b[0;31m                                              **self._backend_args)\n\u001b[0m\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_timeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m                 warnings.warn(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Random subspace\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import clone\n",
    "\n",
    "def random_subspace(n_estimators, M0, M1, verbose = False):\n",
    "\n",
    "    D, N = X_train.shape\n",
    "\n",
    "    standard = False\n",
    "    #M__pca_ideal = 147\n",
    "    #M__lda_ideal = 46\n",
    "\n",
    "    if verbose:\n",
    "        print ('M__pca_ideal = ', M__pca_ideal)\n",
    "        print ('M__lda_ideal = ', M__lda_ideal)\n",
    "\n",
    "    M_pca_bag = N-1\n",
    "\n",
    "    M_pca = 150 #M__pca_ideal\n",
    "    M_lda = 47 #M__lda_ideal\n",
    "\n",
    "    \n",
    "    assert(M1 <= (N-1-M0))\n",
    "    assert(M0+M1 > M_pca)\n",
    "    assert(M_pca > M_lda)\n",
    "\n",
    "    estimators = [('lda', LinearDiscriminantAnalysis(n_components=M_lda)), ('knn', KNeighborsClassifier(n_neighbors=1))]\n",
    "\n",
    "    base_est = Pipeline (estimators)\n",
    "\n",
    "    base_est.fit(X_train.T, y_train.T.ravel())\n",
    "\n",
    "    acc = base_est.score(X_test.T, y_test.T.ravel())\n",
    "    if verbose:\n",
    "        print ('Accuracy of base estimator with no pre PCA = %.2f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "    pca = PCA(n_components=M_pca_bag)\n",
    "    W_train = pca.fit_transform(X_train.T)\n",
    "    W_test = pca.transform(X_test.T)\n",
    "\n",
    "    base_est.fit(W_train, y_train.T.ravel())\n",
    "\n",
    "    acc = base_est.score(W_test, y_test.T.ravel())\n",
    "    if verbose:\n",
    "        print ('Accuracy of base estimator with pre PCA applied = %.2f%%' % (acc * 100))\n",
    "\n",
    "    estimators = []\n",
    "    sub_model_accuracies = []\n",
    "    masks = []\n",
    "\n",
    "    for i in range (n_estimators):\n",
    "\n",
    "        mask0 = np.arange(M0)\n",
    "        mask1 = np.random.choice(np.arange(M0, (N-1)), M1, replace=False)\n",
    "\n",
    "        mask1 = np.array(mask1).ravel()\n",
    "    \n",
    "        mask = np.concatenate((mask0, mask1), axis = None)\n",
    "        masks.append(mask)\n",
    "\n",
    "        W_bag = W_train[:, mask]\n",
    "        y_bag = y_train\n",
    "    \n",
    "        estimator = clone(base_est)\n",
    "\n",
    "        estimator.fit(W_bag, y_bag.T.ravel())\n",
    "    \n",
    "        name = 'est_'+str(i+1)\n",
    "        estimators.append((name, estimator))\n",
    "    \n",
    "        sub_model_acc = estimator.score(W_test[:, mask], y_test.T.ravel())\n",
    "        sub_model_accuracies.append(sub_model_acc)\n",
    "        if verbose:\n",
    "            print ('Accuracy of sub model ', i+1, ' = %.2f%%' % (sub_model_acc * 100))\n",
    "    \n",
    "\n",
    "    ave_sub_model_acc = sum(sub_model_accuracies)/n_estimators\n",
    "    if verbose:\n",
    "        print ('Average accuracy of sub models = %.2f%%' % (ave_sub_model_acc * 100))\n",
    "    \n",
    "    y_hat = []\n",
    "\n",
    "    for w in W_test:\n",
    "        prediction_sum = 0\n",
    "        predictions = np.empty(n_estimators, dtype = np.int64)\n",
    "        for i, (name, estimator) in enumerate(estimators):\n",
    "            y = estimator.predict(w[masks[i]].reshape(1, -1))\n",
    "        \n",
    "            prediction_sum = prediction_sum + float(y[0])\n",
    "            predictions[i] = int(y[0])\n",
    "        prediction = round(prediction_sum/n_estimators)\n",
    "    \n",
    "        counts = np.bincount(predictions)\n",
    "        #y_hat.append(prediction)\n",
    "        y_hat.append(np.argmax(counts))\n",
    "        \n",
    "    acc = accuracy_score(y_test.T, y_hat)\n",
    "    if verbose:\n",
    "        print ('Accuracy of ensemble estimator = %.2f%%' % (acc * 100))\n",
    "        \n",
    "    return acc, ave_sub_model_acc\n",
    "    \n",
    "D, N = X_train.shape      \n",
    "        \n",
    "n_estimators = 30\n",
    "M0 = 120\n",
    "M1 = 60    \n",
    "\n",
    "acc, ave_sub_model_acc = random_subspace(n_estimators, M0, M1, verbose= True)\n",
    "\n",
    "n_estimators = 30\n",
    "M0 = 0\n",
    "M1 = 150-M0+1 \n",
    "\n",
    "acc_varying_subspace = []\n",
    "num_M0 = []\n",
    "num_M1 = []\n",
    "\n",
    "while M0 <= N-1:\n",
    "    M1 = 150-M0+1\n",
    "    num_M1_i = []\n",
    "    acc_varying_subspace_i = []\n",
    "    while M1 <= (N-1-M0):\n",
    "        acc, ave_sub_model_acc = random_subspace(n_estimators, M0, M1)\n",
    "        acc_varying_subspace_i.append((acc*100))\n",
    "        num_M1_i.append(M1)\n",
    "        M1 = M1 + 20\n",
    "    num_M1.append(num_M1_i)\n",
    "    acc_varying_subspace.append(acc_varying_subspace_i)\n",
    "    M0 = M0 + 50\n",
    "\n",
    "n_estimators = 1\n",
    "M0 = 120\n",
    "M1 = 60\n",
    "\n",
    "acc_varying_num_est_ran_subsp = []\n",
    "num_estimators_list = []\n",
    "\n",
    "while n_estimators <= n_est_test_range:\n",
    "    acc, ave_sub_model_acc = random_subspace(n_estimators, M0, M1)\n",
    "    acc_varying_num_est_ran_subsp.append(acc*100)\n",
    "    n_estimators = n_estimators + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
